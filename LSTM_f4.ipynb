{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/LSTM_f4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG54juYZUp9q",
        "outputId": "f335777d-57a3-4aa4-fefb-77cd0d367789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2Oa0WQyVubw",
        "outputId": "276bcd14-b324-43f6-99c0-d7279e5bb8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 掛載 Google 雲端硬碟\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 資料集根目錄\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4\"\n",
        "\n",
        "# 各分類資料夾設定\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/wire_bending\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6OmRck6VSDH"
      },
      "outputs": [],
      "source": [
        "# 設定參數\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_4\"\n",
        "\n",
        "MAX_SEQ_LEN = 10\n",
        "\n",
        "INPUT_DIM = 4\n",
        "HIDDEN_DIM = 16\n",
        "NUM_LAYERS = 4\n",
        "NUM_CLASSES = len(LABEL_DIRS)\n",
        "LEARNING_RATE = 1e-2\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "KFOLD_SPLITS = 5\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ3vGMLCWb3i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "#%% from settings import *\n",
        "\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "# === 資料處理 ===\n",
        "def process_file(file_path, label, sequences, labels, max_seq_len=MAX_SEQ_LEN):\n",
        "    \"\"\"\n",
        "    讀取 CSV 檔案並切分為長度為 max_seq_len 的小段。\n",
        "    :param file_path: CSV檔案路徑\n",
        "    :param label: 資料標籤（例如：0、1、2、3）\n",
        "    :param sequences: 儲存切分後片段的 list\n",
        "    :param labels: 儲存對應標籤的 list\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    # 假設 CSV 中有 'current', 'voltage', 'power' 三個欄位\n",
        "    current = df['current'].values\n",
        "    voltage = df['voltage'].values\n",
        "    power = df['power'].values\n",
        "    temp_C = df['temp_C'].values\n",
        "\n",
        "    sequence = np.column_stack((current, voltage, power, temp_C))  # shape: (N, 4)\n",
        "    seq_len = sequence.shape[0]\n",
        "    num_chunks = seq_len // max_seq_len\n",
        "    for i in range(num_chunks):\n",
        "        start = i * max_seq_len\n",
        "        end = start + max_seq_len\n",
        "        chunk = sequence[start:end]\n",
        "        sequences.append(chunk)\n",
        "        labels.append(label)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for filename in os.listdir(folder):\n",
        "            if filename.lower().endswith(\".csv\"):\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                process_file(file_path, label=label, sequences=sequences, labels=labels, max_seq_len=MAX_SEQ_LEN)\n",
        "    sequences = np.array(sequences, dtype=np.float32)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    print(\"sequences shape:\", sequences.shape)\n",
        "    print(\"labels shape:\", labels.shape)\n",
        "    for i in range(NUM_CLASSES):\n",
        "        print(f\"Number of class {i} samples:\", np.sum(labels == i))\n",
        "    print(\"Unique labels:\", np.unique(labels))\n",
        "    return sequences, labels\n",
        "\n",
        "def split_data(sequences, labels, test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    先切出測試集，再從剩餘資料中切出驗證集。\n",
        "    \"\"\"\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        sequences, labels, test_size=test_size, stratify=labels, random_state=SEED)\n",
        "    val_relative = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_relative, stratify=y_train_val, random_state=SEED)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "# === 自訂 Dataset ===\n",
        "class ChargingDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# === LSTM 模型定義 ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes=NUM_CLASSES, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]   # 使用最後一個時間步的輸出\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# === 評估函數 ===\n",
        "def evaluate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            running_loss += loss.item() * x_batch.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    avg_loss = running_loss / total\n",
        "    avg_acc = correct / total if total > 0 else 0\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "# === K-fold 訓練流程 ===\n",
        "def kfold_training(sequences, labels):\n",
        "    dataset = ChargingDataset(sequences, labels)\n",
        "    kfold = StratifiedKFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=SEED)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    fold_final_metrics = []\n",
        "    all_folds_metrics = []  # 儲存每個 fold 的所有 epoch 指標 DataFrame\n",
        "\n",
        "    fold_idx = 0\n",
        "    for train_indices, test_indices in kfold.split(sequences, labels):\n",
        "        fold_idx += 1\n",
        "        print(f\"\\n=== Fold {fold_idx} / {KFOLD_SPLITS} ===\")\n",
        "        # 切分訓練與測試資料\n",
        "        train_sequences, train_labels = sequences[train_indices], labels[train_indices]\n",
        "        test_sequences, test_labels = sequences[test_indices], labels[test_indices]\n",
        "\n",
        "        # --- 新增資料正規化處理 ---\n",
        "        scaler = StandardScaler()\n",
        "        # 將訓練資料從 3D 轉為 2D: (samples * seq_len, features)\n",
        "        train_reshaped = train_sequences.reshape(-1, train_sequences.shape[-1])\n",
        "        scaler.fit(train_reshaped)\n",
        "        # 將訓練資料轉換回原本的形狀\n",
        "        train_sequences = scaler.transform(train_reshaped).reshape(train_sequences.shape)\n",
        "\n",
        "        # 測試資料僅使用 transform，避免資料洩漏\n",
        "        test_reshaped = test_sequences.reshape(-1, test_sequences.shape[-1])\n",
        "        test_sequences = scaler.transform(test_reshaped).reshape(test_sequences.shape)\n",
        "        # --- 正規化處理結束 ---\n",
        "\n",
        "        train_dataset = ChargingDataset(train_sequences, train_labels)\n",
        "        test_dataset = ChargingDataset(test_sequences, test_labels)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # 建立模型（未訓練狀態）\n",
        "        model = LSTMClassifier(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, NUM_CLASSES).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        # 訓練前：用初始模型在測試集上的表現作為基線（Pre-train Metrics）\n",
        "        pre_test_loss, pre_test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "        print(f\"Pre-training  | Test Loss: {pre_test_loss:.4f}, Test Acc: {pre_test_acc:.4f}\")\n",
        "        print(\"===========================================\")\n",
        "\n",
        "        # 記錄各 epoch 指標\n",
        "        train_loss_list = []\n",
        "        train_acc_list = []\n",
        "        test_loss_list = []\n",
        "        test_acc_list = []\n",
        "        test_precision_list = []\n",
        "        test_recall_list = []\n",
        "        test_f1_list = []\n",
        "\n",
        "        start_time = time.time()\n",
        "        # 開始訓練\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(x_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * x_batch.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == y_batch).sum().item()\n",
        "                total += y_batch.size(0)\n",
        "\n",
        "            epoch_train_loss = running_loss / total\n",
        "            epoch_train_acc = correct / total if total > 0 else 0\n",
        "            epoch_test_loss, epoch_test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "            train_loss_list.append(epoch_train_loss)\n",
        "            train_acc_list.append(epoch_train_acc)\n",
        "            test_loss_list.append(epoch_test_loss)\n",
        "            test_acc_list.append(epoch_test_acc)\n",
        "\n",
        "            # 計算其他指標（以 macro 平均）\n",
        "            model.eval()\n",
        "            preds = []\n",
        "            trues = []\n",
        "            with torch.no_grad():\n",
        "                for x_batch, y_batch in test_loader:\n",
        "                    x_batch = x_batch.to(device)\n",
        "                    y_batch = y_batch.to(device)\n",
        "                    outputs = model(x_batch)\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    preds.extend(predicted.cpu().numpy())\n",
        "                    trues.extend(y_batch.cpu().numpy())\n",
        "            test_precision = precision_score(trues, preds, average='macro', zero_division=0)\n",
        "            test_recall = recall_score(trues, preds, average='macro', zero_division=0)\n",
        "            test_f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
        "\n",
        "            test_precision_list.append(test_precision)\n",
        "            test_recall_list.append(test_recall)\n",
        "            test_f1_list.append(test_f1)\n",
        "\n",
        "            print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | \"\n",
        "                  f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | \"\n",
        "                  f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f} | \"\n",
        "                  f\"Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"⌛訓練時間：{end_time - start_time:.2f} 秒\")\n",
        "\n",
        "        # 訓練結束後，取得訓練後（Post-train）的測試集表現\n",
        "        post_test_loss, post_test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "        print(f\"Post-training | Test Loss: {post_test_loss:.4f}, Test Acc: {post_test_acc:.4f}\")\n",
        "\n",
        "        fold_final_metrics.append({\n",
        "            'Fold': fold_idx,\n",
        "            'Pre-train Loss': pre_test_loss,\n",
        "            'Pre-train Accuracy': pre_test_acc,\n",
        "            'Post-train Loss': post_test_loss,\n",
        "            'Post-train Accuracy': post_test_acc,\n",
        "            'Precision': test_precision_list[-1],\n",
        "            'Recall': test_recall_list[-1],\n",
        "            'F1-Score': test_f1_list[-1]\n",
        "        })\n",
        "\n",
        "        # 混淆矩陣 (使用所有類別)\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        trues = []\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in test_loader:\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                outputs = model(x_batch)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                preds.extend(predicted.cpu().numpy())\n",
        "                trues.extend(y_batch.cpu().numpy())\n",
        "        cm = confusion_matrix(trues, preds, labels=list(range(NUM_CLASSES)))\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(NUM_CLASSES)))\n",
        "        plt.figure(figsize=(4,4))\n",
        "        disp.plot(values_format='d', cmap='Blues')\n",
        "        plt.title(f\"Fold {fold_idx} - Confusion Matrix\")\n",
        "        cm_pdf_path = os.path.join(RESULT_DIR, f\"fold_{fold_idx}_cm.pdf\")\n",
        "        cm_svg_path = os.path.join(RESULT_DIR, f\"fold_{fold_idx}_cm.svg\")\n",
        "        plt.savefig(cm_pdf_path, bbox_inches='tight')\n",
        "        plt.savefig(cm_svg_path, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # 儲存模型\n",
        "        model_save_path = os.path.join(RESULT_DIR, f\"fold_{fold_idx}_model.pth\")\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Model for fold {fold_idx} saved to {model_save_path}\")\n",
        "\n",
        "        # 將該 fold 的所有 epoch 指標記錄成 DataFrame\n",
        "        epochs_range = range(1, NUM_EPOCHS+1)\n",
        "        fold_metrics_df = pd.DataFrame({\n",
        "            'Epoch': epochs_range,\n",
        "            'Train Loss': train_loss_list,\n",
        "            'Test Loss': test_loss_list,\n",
        "            'Train Accuracy': train_acc_list,\n",
        "            'Test Accuracy': test_acc_list,\n",
        "            'Test Precision': test_precision_list,\n",
        "            'Test Recall': test_recall_list,\n",
        "            'Test F1-score': test_f1_list\n",
        "        })\n",
        "        fold_metrics_df['Fold'] = fold_idx\n",
        "        all_folds_metrics.append(fold_metrics_df)\n",
        "\n",
        "    final_metrics_df = pd.DataFrame(fold_final_metrics)\n",
        "    print(\"\\n=== Final Metrics Summary Across All Folds ===\")\n",
        "    print(final_metrics_df.to_string(index=False))\n",
        "    return final_metrics_df, all_folds_metrics\n",
        "\n",
        "\n",
        "def plot_metric_curves(all_folds_metrics):\n",
        "    for fold_df in all_folds_metrics:\n",
        "        fold = fold_df['Fold'].iloc[0]\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "        fig.suptitle(f'Fold {fold} Metrics Curves', fontsize=16)\n",
        "        epochs = fold_df['Epoch']\n",
        "\n",
        "        # Train Loss\n",
        "        axes[0, 0].plot(epochs, fold_df['Train Loss'], label='Train Loss')\n",
        "        axes[0, 0].set_title('Train Loss')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "        # Test Loss\n",
        "        axes[0, 1].plot(epochs, fold_df['Test Loss'], label='Test Loss', color='orange')\n",
        "        axes[0, 1].set_title('Test Loss')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend()\n",
        "\n",
        "        # Train Accuracy\n",
        "        axes[0, 2].plot(epochs, fold_df['Train Accuracy'], label='Train Accuracy', color='green')\n",
        "        axes[0, 2].set_title('Train Accuracy')\n",
        "        axes[0, 2].set_xlabel('Epoch')\n",
        "        axes[0, 2].set_ylabel('Accuracy')\n",
        "        axes[0, 2].legend()\n",
        "\n",
        "        # Test Accuracy\n",
        "        axes[1, 0].plot(epochs, fold_df['Test Accuracy'], label='Test Accuracy', color='red')\n",
        "        axes[1, 0].set_title('Test Accuracy')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Accuracy')\n",
        "        axes[1, 0].legend()\n",
        "\n",
        "        # Test Precision\n",
        "        axes[1, 1].plot(epochs, fold_df['Test Precision'], label='Test Precision', color='purple')\n",
        "        axes[1, 1].set_title('Test Precision')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Precision')\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "        # Test F1-score\n",
        "        axes[1, 2].plot(epochs, fold_df['Test F1-score'], label='Test F1-score', color='brown')\n",
        "        axes[1, 2].set_title('Test F1-score')\n",
        "        axes[1, 2].set_xlabel('Epoch')\n",
        "        axes[1, 2].set_ylabel('F1-score')\n",
        "        axes[1, 2].legend()\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plot_pdf = os.path.join(RESULT_DIR, f\"fold_{fold}_metrics_curves.pdf\")\n",
        "        plot_svg = os.path.join(RESULT_DIR, f\"fold_{fold}_metrics_curves.svg\")\n",
        "        plt.savefig(plot_pdf, bbox_inches='tight')\n",
        "        plt.savefig(plot_svg, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_overlaid_metrics(all_folds_metrics):\n",
        "    # 假設所有 fold 的 epoch 數量相同\n",
        "    epochs = all_folds_metrics[0]['Epoch']\n",
        "\n",
        "    # 定義你想疊加繪製的指標\n",
        "    metrics = {\n",
        "        'Train Accuracy': 'Train Accuracy',\n",
        "        'Test Accuracy': 'Test Accuracy',\n",
        "        'Train Loss': 'Train Loss',\n",
        "        'Test Loss': 'Test Loss',\n",
        "        'Test Precision': 'Test Precision',\n",
        "        'Test Recall': 'Test Recall',\n",
        "        'Test F1-score': 'Test F1-score'\n",
        "    }\n",
        "\n",
        "    for metric_title, col in metrics.items():\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for fold_df in all_folds_metrics:\n",
        "            fold = fold_df['Fold'].iloc[0]\n",
        "            plt.plot(epochs, fold_df[col], label=f'Fold {fold}')\n",
        "        plt.title(f'Combined {metric_title}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric_title)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        pdf_path = os.path.join(RESULT_DIR, f\"combined_{col}.pdf\")\n",
        "        svg_path = os.path.join(RESULT_DIR, f\"combined_{col}.svg\")\n",
        "        plt.savefig(pdf_path, bbox_inches='tight')\n",
        "        plt.savefig(svg_path, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def count_chunks_in_folder(folder_path, max_seq_len=MAX_SEQ_LEN):\n",
        "    \"\"\"\n",
        "    計算指定資料夾內所有 CSV 檔案，依據 max_seq_len 切分後的總片段數量。\n",
        "    \"\"\"\n",
        "    total_chunks = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith(\".csv\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_csv(file_path)\n",
        "            num_rows = len(df)\n",
        "            chunks = num_rows // max_seq_len\n",
        "            total_chunks += chunks\n",
        "    return total_chunks\n",
        "\n",
        "def best_result():\n",
        "    results_csv_path = os.path.join(RESULT_DIR, \"overall_experiment_log.csv\")\n",
        "    if not os.path.exists(results_csv_path):\n",
        "        print(\"未發現網格搜尋結果 CSV，請先執行網格搜尋。\")\n",
        "        return None\n",
        "    df = pd.read_csv(results_csv_path)\n",
        "    # 假設以驗證準確率最高為最佳模型\n",
        "    best_row = df.loc[df['post_train_acc'].idxmax()]\n",
        "    return best_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aRN0k8sDWfFL",
        "outputId": "8f188a10-303a-4014-eb2e-a09ce823965e"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # === ✅ 加這段，保證 feature dim_4 是乾淨的 ===\n",
        "    import shutil\n",
        "    CLEAR_RESULT_DIR_BEFORE_RUN = True\n",
        "\n",
        "    if CLEAR_RESULT_DIR_BEFORE_RUN:\n",
        "        print(f\"[DEBUG] Clearing {RESULT_DIR} before run...\")\n",
        "        shutil.rmtree(RESULT_DIR, ignore_errors=True)\n",
        "        os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "    # 直接設定變數即可\n",
        "    args_check_data = False  # True 只檢查資料\n",
        "    args_best = False        # True 只印最佳結果\n",
        "\n",
        "    if args_best:\n",
        "        best_model = best_result()\n",
        "        if best_model is not None:\n",
        "            print(\"最好的模型指標:\")\n",
        "            print(best_model)\n",
        "    elif args_check_data:\n",
        "        print(f\"Max seq len = {MAX_SEQ_LEN}\")\n",
        "        for label, folder in LABEL_DIRS.items():\n",
        "            count = count_chunks_in_folder(folder, max_seq_len=MAX_SEQ_LEN)\n",
        "            print(f\"Label {label} ({folder}): {count} chunks\")\n",
        "    else:\n",
        "        # 超參數\n",
        "        batch_size_values = [8, 16, 32]\n",
        "        learning_rate_values = [1e-2, 1e-3, 1e-1, 1e-4]\n",
        "        max_seq_len_values = [10, 20, 30, 40]\n",
        "        # batch_size_values = [8]\n",
        "        # learning_rate_values = [1e-3]\n",
        "        # max_seq_len_values = [10]\n",
        "\n",
        "        # 儲存所有實驗結果記錄\n",
        "        overall_experiment_logs = []\n",
        "        # 記錄所有實驗的總結果（此 log 最後存成 CSV 檔）\n",
        "        overall_results = []\n",
        "\n",
        "        # 保存原始的 RESULT_DIR 值，方便還原\n",
        "        original_RESULT_DIR = RESULT_DIR\n",
        "\n",
        "        # 依照超參數組合進行迴圈\n",
        "        for bs in batch_size_values:\n",
        "            for lr in learning_rate_values:\n",
        "                for seq in max_seq_len_values:\n",
        "                    # 更新全域變數（注意：這裡是更新本模組內的變數）\n",
        "                    BATCH_SIZE = bs\n",
        "                    LEARNING_RATE = lr\n",
        "                    MAX_SEQ_LEN = seq\n",
        "\n",
        "                    # 為每組參數建立獨立儲存結果的資料夾\n",
        "                    exp_id = f\"bs_{bs}_lr_{lr}_seq_{seq}\"\n",
        "                    print(f\"\\n==== Running experiment: {exp_id} ====\")\n",
        "                    exp_result_dir = os.path.join(original_RESULT_DIR, exp_id)\n",
        "                    os.makedirs(exp_result_dir, exist_ok=True)\n",
        "\n",
        "                    # 暫時改寫 RESULT_DIR，讓後續的儲存檔案寫入此目錄\n",
        "                    RESULT_DIR = exp_result_dir\n",
        "\n",
        "                    # 載入資料（會根據 MAX_SEQ_LEN 切分資料）\n",
        "                    all_sequences, all_labels = load_data()\n",
        "\n",
        "                    # 執行 K-fold 訓練\n",
        "                    final_metrics_df, all_folds_metrics = kfold_training(all_sequences, all_labels)\n",
        "\n",
        "                    # 繪製指標曲線圖\n",
        "                    plot_metric_curves(all_folds_metrics)\n",
        "                    plot_overlaid_metrics(all_folds_metrics)\n",
        "\n",
        "                    # 儲存本次實驗的最終指標 log\n",
        "                    log_csv_path = os.path.join(RESULT_DIR, \"final_metrics.csv\")\n",
        "                    final_metrics_df.to_csv(log_csv_path, index=False)\n",
        "                    print(f\"Final metrics logged at: {log_csv_path}\")\n",
        "\n",
        "                    # 將本次實驗資訊存入 overall_experiment_logs\n",
        "                    overall_experiment_logs.append({\n",
        "                        'experiment_id': exp_id,\n",
        "                        'batch_size': bs,\n",
        "                        'learning_rate': lr,\n",
        "                        'max_seq_len': seq,\n",
        "                        'final_metrics': final_metrics_df\n",
        "                    })\n",
        "                    # 也可將各折最終的平均值記錄下來（例如：Post-train Accuracy）\n",
        "                    overall_results.append({\n",
        "                        'experiment_id': exp_id,\n",
        "                        'batch_size': bs,\n",
        "                        'learning_rate': lr,\n",
        "                        'max_seq_len': seq,\n",
        "                        'pre_train_loss': final_metrics_df['Pre-train Loss'].mean(),\n",
        "                        'pre_train_acc': final_metrics_df['Pre-train Accuracy'].mean(),\n",
        "                        'post_train_loss': final_metrics_df['Post-train Loss'].mean(),\n",
        "                        'post_train_acc': final_metrics_df['Post-train Accuracy'].mean(),\n",
        "                        'precision': final_metrics_df['Precision'].mean(),\n",
        "                        'recall': final_metrics_df['Recall'].mean(),\n",
        "                        'f1_score': final_metrics_df['F1-Score'].mean()\n",
        "                    })\n",
        "\n",
        "                    RESULT_DIR = original_RESULT_DIR\n",
        "\n",
        "        # 儲存所有實驗的總結果 log\n",
        "        overall_log_path = os.path.join(RESULT_DIR, \"overall_experiment_log.csv\")\n",
        "        overall_df = pd.DataFrame(overall_results)\n",
        "        overall_df.to_csv(overall_log_path, index=False)\n",
        "        print(f\"Overall experiment log saved at: {overall_log_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYAeCWwTUl3t6kHdc9YQo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}