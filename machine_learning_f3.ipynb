{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/machine_learning_f3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdeWQTkaMyj",
        "outputId": "7ea608a9-28ac-457c-ba73-a513a866052a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# ÊéõËºâ Google Èõ≤Á´ØÁ°¨Á¢ü\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ë≥áÊñôÈõÜÊ†πÁõÆÈåÑ\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_3\"\n",
        "\n",
        "# ÂêÑÂàÜÈ°ûË≥áÊñôÂ§æË®≠ÂÆö\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBh_Hm3gvQA1"
      },
      "source": [
        "ÂÆöÁæ©Âø´ÂèñÂáΩÊï∏(‰∏âÁ∂≠)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oay5nopuvOx3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def generate_preprocessed_cache_3d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_3d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_3d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"üì• Â∑≤Â≠òÂú® 3D Âø´ÂèñÔºöseq_len={seq_len}ÔºåÁï•ÈÅé\")\n",
        "            continue\n",
        "\n",
        "        all_seq, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "                    all_seq.extend(chunks)\n",
        "                    all_labels.extend([label] * len(chunks))\n",
        "\n",
        "        seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "        labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "        B, T, F = seq_arr.shape\n",
        "        reshaped = seq_arr.reshape(-1, F)\n",
        "        scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "        np.save(cache_X, scaled)\n",
        "        np.save(cache_y, labels_arr)\n",
        "        print(f\"‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len={seq_len}ÔºàÂÖ± {B} Á≠ÜÂ∫èÂàóÔºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-x-GLAh00T2"
      },
      "source": [
        "ÂÆöÁæ©Âø´ÂèñÂáΩÊï∏(‰∫åÁ∂≠)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naR3mpLp0zSh"
      },
      "outputs": [],
      "source": [
        "def generate_preprocessed_cache_2d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_2d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_2d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"üì• Â∑≤Â≠òÂú® 2D Âø´ÂèñÔºöseq_len={seq_len}ÔºåÁï•ÈÅé\")\n",
        "            continue\n",
        "\n",
        "        all_features, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "\n",
        "                    for chunk in chunks:\n",
        "                        features = []\n",
        "                        features.extend(np.mean(chunk, axis=0))  # 3\n",
        "                        features.extend(np.std(chunk, axis=0))   # 3\n",
        "                        features.extend(np.max(chunk, axis=0))   # 3\n",
        "                        features.extend(np.min(chunk, axis=0))   # 3\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(label)\n",
        "\n",
        "        X = np.array(all_features, dtype=np.float32)\n",
        "        y = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        np.save(cache_X, X_scaled)\n",
        "        np.save(cache_y, y)\n",
        "        print(f\"‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len={seq_len}ÔºàÂÖ± {len(X_scaled)} Á≠ÜÔºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBeY_OhIvQ6o"
      },
      "source": [
        "ÂØ¶ÈöõÂü∑Ë°åÂø´Âèñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wad3ku5VvRNo",
        "outputId": "016c9895-8413-4c5e-d569-a9ad0cb07e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=4ÔºàÂÖ± 10116 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=5ÔºàÂÖ± 8101 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=8ÔºàÂÖ± 5047 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=10ÔºàÂÖ± 4047 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=4ÔºàÂÖ± 10116 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=5ÔºàÂÖ± 8101 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=8ÔºàÂÖ± 5047 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=10ÔºàÂÖ± 4047 Á≠ÜÔºâ\n"
          ]
        }
      ],
      "source": [
        "seq_lens = [4, 5, 8, 10]\n",
        "generate_preprocessed_cache_3d(seq_lens, LABEL_DIRS)      # Áî¢Áîü 3D\n",
        "generate_preprocessed_cache_2d(seq_lens, LABEL_DIRS)   # Áî¢Áîü 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0wQZBWz5Uht"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qepGDca0Yfpm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/LSTM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === LSTM Ê®°Âûã ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === ‰∏ªË®ìÁ∑¥ËàáË∂ÖÂèÉÊï∏ÊêúÂ∞ãÂáΩÊï∏Ôºà‰ΩøÁî®Âø´ÂèñÔºâ ===\n",
        "def train_and_search_lstm(batch_sizes, learning_rates, seq_lens,\n",
        "                          num_epochs=100, hidden_dim=64, num_layers=1,\n",
        "                          num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = LSTMClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                           num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, num_epochs, bs, lr, seq_len):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # Á¥îÁ∑öÊ¢ùÔºåÊ≤íÊúâ marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", num_epochs, bs, lr, seq_len)\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"lstm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWK53YBRYxJY",
        "outputId": "73c820f7-2da6-4105-ad94-982ecc62f23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö1ÔºåÊØèÁµÑË®ìÁ∑¥ 50 epochs\n",
            "\n",
            "üß™ BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9753 | Loss: 0.0885\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9852 | Loss: 0.0474\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9938 | Loss: 0.0330\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9901 | Loss: 0.0400\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9951 | Loss: 0.0378\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9815 | Loss: 0.0581\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9864 | Loss: 0.0446\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9901 | Loss: 0.0277\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9741 | Loss: 0.0652\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9926 | Loss: 0.0209\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9778 | Loss: 0.0881\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9827 | Loss: 0.0592\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9827 | Loss: 0.0755\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9839 | Loss: 0.0712\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9889 | Loss: 0.0619\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9802 | Loss: 0.0551\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9876 | Loss: 0.0579\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9876 | Loss: 0.0349\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9938 | Loss: 0.0236\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9926 | Loss: 0.0245\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9889 | Loss: 0.0482\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9938 | Loss: 0.0331\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9938 | Loss: 0.0253\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9951 | Loss: 0.0192\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9938 | Loss: 0.0193\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=16, LR=0.001, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9938\n",
            "  üîπ F1-score       : 0.9899\n",
            "  ‚è±Ô∏è  Training Time  : 254.95 Áßí\n",
            "\n",
            "üìÑ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/LSTM/lstm_experiment_results.csv\n",
            "\n",
            "üèÜ Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9938\n",
            "\n",
            "üéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9938\n",
            "Final Loss     = 0.0193\n",
            "Precision      = 0.9904\n",
            "Recall         = 0.9896\n",
            "F1-score       = 0.9899\n",
            "Training Time  = 254.95 Áßí\n"
          ]
        }
      ],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")#\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q3inndRoyYY"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMxndCGUhHqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/MLP\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === MLP Ê®°Âûã ===\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_mlp(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=128,\n",
        "                         num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "                input_dim = X.shape[1]\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = MLPClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, num_epochs, bs, lr, seq_len):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # Á¥îÁ∑öÊ¢ù\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", num_epochs, bs, lr, seq_len)\n",
        "\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"mlp_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA1ydN20jMiw",
        "outputId": "8b463447-bab2-4c32-88bd-11c450222e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö1ÔºåÊØèÁµÑË®ìÁ∑¥ 50 epochs\n",
            "\n",
            "üß™ BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9728 | Loss: 0.1296\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9790 | Loss: 0.0771\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9802 | Loss: 0.0726\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9815 | Loss: 0.0558\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9852 | Loss: 0.0618\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9765 | Loss: 0.1051\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9840 | Loss: 0.0677\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9815 | Loss: 0.0660\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9815 | Loss: 0.0666\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9852 | Loss: 0.0650\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9790 | Loss: 0.0998\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9827 | Loss: 0.0683\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9839 | Loss: 0.0605\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9864 | Loss: 0.0573\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9839 | Loss: 0.0661\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9740 | Loss: 0.0996\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9852 | Loss: 0.0568\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9827 | Loss: 0.0463\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9901 | Loss: 0.0337\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9889 | Loss: 0.0323\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9802 | Loss: 0.0781\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9913 | Loss: 0.0356\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9913 | Loss: 0.0252\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9913 | Loss: 0.0207\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9926 | Loss: 0.0255\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=16, LR=0.001, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9926\n",
            "  üîπ F1-score       : 0.9820\n",
            "  ‚è±Ô∏è  Training Time  : 92.37 Áßí\n",
            "\n",
            "üìÑ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/MLP/mlp_experiment_results.csv\n",
            "\n",
            "üèÜ Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9926\n",
            "\n",
            "üéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9926\n",
            "Final Loss     = 0.0255\n",
            "Precision      = 0.9816\n",
            "Recall         = 0.9824\n",
            "F1-score       = 0.9820\n",
            "Training Time  = 92.37 Áßí\n"
          ]
        }
      ],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmAFC98o2iM"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxnqos1Eo4UC"
      },
      "outputs": [],
      "source": [
        "# === SVM Ë®ìÁ∑¥Á®ãÂºèÔºàÊîπÂØ´ÁÇ∫Ëàá LSTM Êû∂Êßã‰∏ÄËá¥Ôºâ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/SVM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === ÊîπÂØ´ÂæåÁöÑ SVM Ë®ìÁ∑¥ÊµÅÁ®ã ===\n",
        "def train_svm_search(kernels, Cs, seq_lens, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "    results = []\n",
        "    best = None\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for C_val in Cs:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüîé Kernel={kernel} | C={C_val} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_list, f1_list, all_y_true, all_y_pred = [], [], [], []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "                    clf = SVC(kernel=kernel, C=C_val)\n",
        "                    clf.fit(X_train, y_train)\n",
        "                    y_pred = clf.predict(X_val)\n",
        "                    acc = accuracy_score(y_val, y_pred)\n",
        "                    f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "                    acc_list.append(acc)\n",
        "                    f1_list.append(f1)\n",
        "                    all_y_true.extend(y_val)\n",
        "                    all_y_pred.extend(y_pred)\n",
        "                t1 = time.time()\n",
        "\n",
        "                final_acc = np.mean(acc_list)\n",
        "                final_f1 = np.mean(f1_list)\n",
        "                if final_acc >= 1.0:\n",
        "                    print(f\"‚ö†Ô∏è Skipped Kernel={kernel} C={C_val} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                precision = precision_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                # Êñ∞Â¢û‰∏âÁ®ÆÂúñ\n",
        "                # 1. K-fold validation accuracy Êõ≤Á∑ö\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds+1), acc_list, label='Validation Accuracy')\n",
        "                plt.title(f\"K-fold Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"accuracy_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "                # 2. K-fold F1-score Êõ≤Á∑ö\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds+1), f1_list, label='Validation F1-score', color='orange')\n",
        "                plt.title(f\"K-fold F1-score\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"F1-score\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"f1_score_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "                # 3. Ë®ìÁ∑¥ÈõÜÊ∫ñÁ¢∫ÁéáÔºàÊï¥È´îÔºâ\n",
        "                clf_full = SVC(kernel=kernel, C=C_val)\n",
        "                clf_full.fit(X, y)\n",
        "                train_pred = clf_full.predict(X)\n",
        "                train_acc = accuracy_score(y, train_pred)\n",
        "                plt.figure()\n",
        "                plt.bar(['train'], [train_acc])\n",
        "                plt.title(f\"Train Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"train_accuracy\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                result = {\n",
        "                    'kernel': kernel, 'C': C_val, 'seq_len': seq_len,\n",
        "                    'final_acc': final_acc, 'precision': precision,\n",
        "                    'recall': recall, 'f1_score': final_f1,\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(result)\n",
        "                if best is None or result['final_acc'] > best['final_acc']:\n",
        "                    best = result\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (Kernel={kernel}, C={C_val}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_acc:.4f}\")\n",
        "                print(f\"  üîπ Precision      : {precision:.4f}\")\n",
        "                print(f\"  üîπ Recall         : {recall:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_f1:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {round(t1 - t0, 2)} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"svm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ Results saved to {csv_path}\")\n",
        "    return results, best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB0a2PFxo8TZ",
        "outputId": "0ba76816-0681-4819-9a4b-0e87499bd1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Kernel=rbf | C=1.0 | SEQ=4\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=rbf, C=1.0, SEQ=4):\n",
            "  üîπ Final Accuracy : 0.9807\n",
            "  üîπ Precision      : 0.9729\n",
            "  üîπ Recall         : 0.9808\n",
            "  üîπ F1-score       : 0.9768\n",
            "  ‚è±Ô∏è  Training Time  : 2.0 Áßí\n",
            "\n",
            "üîé Kernel=rbf | C=1.0 | SEQ=8\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=rbf, C=1.0, SEQ=8):\n",
            "  üîπ Final Accuracy : 0.9816\n",
            "  üîπ Precision      : 0.9717\n",
            "  üîπ Recall         : 0.9820\n",
            "  üîπ F1-score       : 0.9767\n",
            "  ‚è±Ô∏è  Training Time  : 0.59 Áßí\n",
            "\n",
            "üîé Kernel=rbf | C=1.0 | SEQ=10\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=rbf, C=1.0, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9815\n",
            "  üîπ Precision      : 0.9723\n",
            "  üîπ Recall         : 0.9799\n",
            "  üîπ F1-score       : 0.9761\n",
            "  ‚è±Ô∏è  Training Time  : 0.41 Áßí\n",
            "\n",
            "üîé Kernel=rbf | C=10.0 | SEQ=4\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=rbf, C=10.0, SEQ=4):\n",
            "  üîπ Final Accuracy : 0.9870\n",
            "  üîπ Precision      : 0.9811\n",
            "  üîπ Recall         : 0.9851\n",
            "  üîπ F1-score       : 0.9831\n",
            "  ‚è±Ô∏è  Training Time  : 1.55 Áßí\n",
            "\n",
            "üîé Kernel=rbf | C=10.0 | SEQ=8\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=rbf, C=10.0, SEQ=8):\n",
            "  üîπ Final Accuracy : 0.9885\n",
            "  üîπ Precision      : 0.9826\n",
            "  üîπ Recall         : 0.9865\n",
            "  üîπ F1-score       : 0.9845\n",
            "  ‚è±Ô∏è  Training Time  : 0.76 Áßí\n",
            "\n",
            "üîé Kernel=rbf | C=10.0 | SEQ=10\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=rbf, C=10.0, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9874\n",
            "  üîπ Precision      : 0.9814\n",
            "  üîπ Recall         : 0.9850\n",
            "  üîπ F1-score       : 0.9832\n",
            "  ‚è±Ô∏è  Training Time  : 0.3 Áßí\n",
            "\n",
            "üîé Kernel=linear | C=1.0 | SEQ=4\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=linear, C=1.0, SEQ=4):\n",
            "  üîπ Final Accuracy : 0.9354\n",
            "  üîπ Precision      : 0.9124\n",
            "  üîπ Recall         : 0.9024\n",
            "  üîπ F1-score       : 0.9040\n",
            "  ‚è±Ô∏è  Training Time  : 6.4 Áßí\n",
            "\n",
            "üîé Kernel=linear | C=1.0 | SEQ=8\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=linear, C=1.0, SEQ=8):\n",
            "  üîπ Final Accuracy : 0.9408\n",
            "  üîπ Precision      : 0.9200\n",
            "  üîπ Recall         : 0.9097\n",
            "  üîπ F1-score       : 0.9107\n",
            "  ‚è±Ô∏è  Training Time  : 2.75 Áßí\n",
            "\n",
            "üîé Kernel=linear | C=1.0 | SEQ=10\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=linear, C=1.0, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9377\n",
            "  üîπ Precision      : 0.9136\n",
            "  üîπ Recall         : 0.9032\n",
            "  üîπ F1-score       : 0.9042\n",
            "  ‚è±Ô∏è  Training Time  : 1.02 Áßí\n",
            "\n",
            "üîé Kernel=linear | C=10.0 | SEQ=4\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=linear, C=10.0, SEQ=4):\n",
            "  üîπ Final Accuracy : 0.9666\n",
            "  üîπ Precision      : 0.9463\n",
            "  üîπ Recall         : 0.9510\n",
            "  üîπ F1-score       : 0.9482\n",
            "  ‚è±Ô∏è  Training Time  : 18.42 Áßí\n",
            "\n",
            "üîé Kernel=linear | C=10.0 | SEQ=8\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=linear, C=10.0, SEQ=8):\n",
            "  üîπ Final Accuracy : 0.9772\n",
            "  üîπ Precision      : 0.9578\n",
            "  üîπ Recall         : 0.9722\n",
            "  üîπ F1-score       : 0.9646\n",
            "  ‚è±Ô∏è  Training Time  : 3.07 Áßí\n",
            "\n",
            "üîé Kernel=linear | C=10.0 | SEQ=10\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (Kernel=linear, C=10.0, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9748\n",
            "  üîπ Precision      : 0.9535\n",
            "  üîπ Recall         : 0.9680\n",
            "  üîπ F1-score       : 0.9603\n",
            "  ‚è±Ô∏è  Training Time  : 2.09 Áßí\n",
            "\n",
            "üìÑ Results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/SVM/svm_experiment_results.csv\n",
            "\n",
            "üéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\n",
            "Sequence Length = 8\n",
            "C = 10.0\n",
            "Kernel = rbf\n",
            "Accuracy = 0.9885\n",
            "Precision = 0.9826\n",
            "Recall = 0.9865\n",
            "F1-score = 0.9845\n",
            "Training Time = 0.76 Áßí\n"
          ]
        }
      ],
      "source": [
        "seq_lens = [4, 8, 10]\n",
        "C_list = [1.0, 10.0]\n",
        "kernel_list = [\"rbf\", \"linear\"]\n",
        "\n",
        "results, best = train_svm_search(kernel_list, C_list, seq_lens)\n",
        "\n",
        "print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "print(f\"Sequence Length = {best['seq_len']}\")\n",
        "print(f\"C = {best['C']}\")\n",
        "print(f\"Kernel = {best['kernel']}\")\n",
        "print(f\"Accuracy = {best['final_acc']:.4f}\")\n",
        "print(f\"Precision = {best['precision']:.4f}\")\n",
        "print(f\"Recall = {best['recall']:.4f}\")\n",
        "print(f\"F1-score = {best['f1_score']:.4f}\")\n",
        "print(f\"Training Time = {best['training_time_s']} Áßí\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVLlUCDrD_2"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVsy2pBvrFfh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/GRU\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === GRU Ê®°Âûã ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_gru(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X = np.load(os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\"))\n",
        "                y = np.load(os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\"))\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = GRUClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        correct_train, total_train, total_loss_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train_list.append(correct_train / total_train)\n",
        "                        loss_train_list.append(total_loss_train / total_train)\n",
        "\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc_list.append(correct / total)\n",
        "                        loss_list.append(total_loss / total)\n",
        "                        f1_list.append(f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0))\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc_list[-1]:.4f} | Loss: {loss_list[-1]:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # Á¥îÁ∑öÊ¢ùÔºåÁÑ° marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"gru_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVWAGumMrPmo",
        "outputId": "6d7e11c5-c416-4f6d-8b49-d88bd8754dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö1ÔºåÊØèÁµÑË®ìÁ∑¥ 50 epochs\n",
            "\n",
            "üß™ BS=16 | LR=0.0001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9457 | Loss: 0.2989\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9444 | Loss: 0.1641\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9716 | Loss: 0.1028\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9741 | Loss: 0.0860\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9741 | Loss: 0.0800\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9432 | Loss: 0.2125\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9667 | Loss: 0.1231\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9778 | Loss: 0.0901\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9778 | Loss: 0.0795\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9827 | Loss: 0.0682\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9159 | Loss: 0.3364\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9518 | Loss: 0.1527\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9765 | Loss: 0.0945\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9790 | Loss: 0.0834\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9778 | Loss: 0.0783\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9333 | Loss: 0.2528\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9654 | Loss: 0.1483\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9778 | Loss: 0.0944\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9778 | Loss: 0.0720\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9815 | Loss: 0.0622\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9506 | Loss: 0.2293\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9716 | Loss: 0.1186\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9765 | Loss: 0.0800\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9790 | Loss: 0.0637\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9778 | Loss: 0.0605\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=16, LR=0.0001, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9778\n",
            "  üîπ F1-score       : 0.9703\n",
            "  ‚è±Ô∏è  Training Time  : 350.37 Áßí\n",
            "\n",
            "üìÑ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/GRU/gru_experiment_results.csv\n",
            "\n",
            "üèÜ Best: BS=16.0 | LR=0.0001 | SEQ=10.0 | ACC=0.9778\n",
            "\n",
            "üéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.0001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9778\n",
            "Final Loss     = 0.0605\n",
            "Precision      = 0.9665\n",
            "Recall         = 0.9744\n",
            "F1-score       = 0.9703\n",
            "Training Time  = 350.37 Áßí\n"
          ]
        }
      ],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKn_cKbgu0O"
      },
      "source": [
        "1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWrgapHBgxcw"
      },
      "outputs": [],
      "source": [
        "# === CNN Ê®°ÂûãÂÆåÊï¥Ë®ìÁ∑¥Á®ãÂºè ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/CNN\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === CNN Ê®°Âûã ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === ‰∏ªË®ìÁ∑¥ËàáË∂ÖÂèÉÊï∏ÊêúÂ∞ãÂáΩÊï∏Ôºà‰ΩøÁî®Âø´ÂèñÔºâ ===\n",
        "def train_and_search_cnn(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = CNNClassifier(num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # ÁÑ° marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"cnn_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2P8TmkIijrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3753f6-789e-4e34-cfe3-67263a6a73f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö1ÔºåÊØèÁµÑË®ìÁ∑¥ 50 epochs\n",
            "\n",
            "üß™ BS=16 | LR=0.0001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.8605 | Loss: 0.4802\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9617 | Loss: 0.2878\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9642 | Loss: 0.2157\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9667 | Loss: 0.1697\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9741 | Loss: 0.1339\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.8728 | Loss: 0.4318\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9605 | Loss: 0.2157\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9704 | Loss: 0.1497\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9765 | Loss: 0.1192\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9778 | Loss: 0.1023\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.8133 | Loss: 0.5794\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9333 | Loss: 0.3493\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9629 | Loss: 0.2296\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9691 | Loss: 0.1655\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9753 | Loss: 0.1307\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.8195 | Loss: 0.5850\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9295 | Loss: 0.3169\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9642 | Loss: 0.1765\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9716 | Loss: 0.1226\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9815 | Loss: 0.0963\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.8566 | Loss: 0.4442\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9679 | Loss: 0.2178\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9728 | Loss: 0.1297\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9790 | Loss: 0.0930\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9827 | Loss: 0.0763\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=16, LR=0.0001, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9827\n",
            "  üîπ F1-score       : 0.9679\n",
            "  ‚è±Ô∏è  Training Time  : 194.21 Áßí\n",
            "\n",
            "üìÑ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/CNN/cnn_experiment_results.csv\n",
            "\n",
            "üèÜ Best: BS=16.0 | LR=0.0001 | SEQ=10.0 | ACC=0.9827\n",
            "\n",
            "üéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.0001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9827\n",
            "Final Loss     = 0.0763\n",
            "Precision      = 0.9618\n",
            "Recall         = 0.9743\n",
            "F1-score       = 0.9679\n",
            "Training Time  = 194.21 Áßí\n"
          ]
        }
      ],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TimesNet"
      ],
      "metadata": {
        "id": "MIiGTNGfpQn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "# === TimesNet Ê®°Âûã ===\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        # ÊÆòÂ∑Æ skip connection ËôïÁêÜ\n",
        "        if input_dim != hidden_dim:\n",
        "            self.skip_conv = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=1)\n",
        "        else:\n",
        "            self.skip_conv = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x.transpose(1, 2)  # (batch, input_dim, seq_len)\n",
        "\n",
        "        x = residual\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.skip_conv is not None:\n",
        "            residual = self.skip_conv(residual)\n",
        "\n",
        "        x = x + residual  # ÊÆòÂ∑ÆÂä†Ê≥ï\n",
        "        x = x.transpose(1, 2)  # ÂõûÂà∞ (batch, seq_len, hidden_dim)\n",
        "        return x\n",
        "\n",
        "class TimesNetClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([TimesBlock(input_dim if i==0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)  # ‰∏çË¶ÅÂÜçÂä†ÊÆòÂ∑Æ‰∫ÜÔºåblock ÂÖßÈÉ®Â∑≤ËôïÁêÜÊÆòÂ∑Æ\n",
        "        x = x.transpose(1, 2)  # (batch, hidden_dim, seq_len)\n",
        "        x = self.pool(x).squeeze(-1)  # (batch, hidden_dim)\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "# === ‰∏ªË®ìÁ∑¥ËàáË∂ÖÂèÉÊï∏ÊêúÂ∞ãÂáΩÊï∏ ===\n",
        "def train_and_search_timesnet(batch_sizes, learning_rates, seq_lens,\n",
        "                              num_epochs=100, hidden_dim=64, num_layers=2,\n",
        "                              num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    RESULTS_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet\"\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    SUB_DIRS = [\n",
        "        \"accuracy_curves\", \"loss_curves\", \"f1_score_curves\",\n",
        "        \"confusion_matrices\", \"train_accuracy\", \"train_loss\"\n",
        "    ]\n",
        "    for sub in SUB_DIRS:\n",
        "        os.makedirs(os.path.join(RESULTS_DIR, sub), exist_ok=True)\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = TimesNetClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                               num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULTS_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULTS_DIR, \"timesnet_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "QXdg-w6HpQTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "#batch_sizes = [4, 8, 16]\n",
        "#learning_rates = [1e-3, 1e-4]\n",
        "#seq_lens = [4, 5, 8, 10]\n",
        "#num_epochs = 100\n",
        "\n",
        "batch_sizes = [16]\n",
        "learning_rates = [1e-4]\n",
        "seq_lens = [10]\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_timesnet(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO5scvlppmiL",
        "outputId": "4d7f1bba-fa59-4b66-8a5e-43b8580dcba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö1ÔºåÊØèÁµÑË®ìÁ∑¥ 50 epochs\n",
            "\n",
            "üß™ BS=16 | LR=0.0001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9741 | Loss: 0.0921\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9790 | Loss: 0.0628\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9864 | Loss: 0.0504\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9877 | Loss: 0.0401\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9889 | Loss: 0.0398\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9778 | Loss: 0.0792\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9852 | Loss: 0.0487\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9815 | Loss: 0.0528\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9877 | Loss: 0.0451\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9877 | Loss: 0.0399\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9753 | Loss: 0.0865\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9815 | Loss: 0.0708\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9864 | Loss: 0.0581\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9889 | Loss: 0.0526\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9889 | Loss: 0.0577\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9778 | Loss: 0.0660\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9864 | Loss: 0.0432\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9913 | Loss: 0.0335\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9901 | Loss: 0.0310\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9913 | Loss: 0.0265\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9765 | Loss: 0.0549\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9926 | Loss: 0.0322\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9938 | Loss: 0.0230\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9938 | Loss: 0.0164\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9938 | Loss: 0.0190\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=16, LR=0.0001, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9938\n",
            "  üîπ F1-score       : 0.9865\n",
            "  ‚è±Ô∏è  Training Time  : 413.39 Áßí\n",
            "\n",
            "üìÑ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet/timesnet_experiment_results.csv\n",
            "\n",
            "üèÜ Best: BS=16.0 | LR=0.0001 | SEQ=10.0 | ACC=0.9938\n",
            "\n",
            "üéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.0001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9938\n",
            "Final Loss     = 0.0190\n",
            "Precision      = 0.9846\n",
            "Recall         = 0.9884\n",
            "F1-score       = 0.9865\n",
            "Training Time  = 413.39 Áßí\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB688HHI49c8tg8Lvv607G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}