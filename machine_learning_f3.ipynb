{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/machine_learning_f3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdeWQTkaMyj",
        "outputId": "7ea608a9-28ac-457c-ba73-a513a866052a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# æ›è¼‰ Google é›²ç«¯ç¡¬ç¢Ÿ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# è³‡æ–™é›†æ ¹ç›®éŒ„\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_3\"\n",
        "\n",
        "# å„åˆ†é¡è³‡æ–™å¤¾è¨­å®š\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBh_Hm3gvQA1"
      },
      "source": [
        "å®šç¾©å¿«å–å‡½æ•¸(ä¸‰ç¶­)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oay5nopuvOx3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def generate_preprocessed_cache_3d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_3d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_3d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"ğŸ“¥ å·²å­˜åœ¨ 3D å¿«å–ï¼šseq_len={seq_len}ï¼Œç•¥é\")\n",
        "            continue\n",
        "\n",
        "        all_seq, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "                    all_seq.extend(chunks)\n",
        "                    all_labels.extend([label] * len(chunks))\n",
        "\n",
        "        seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "        labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "        B, T, F = seq_arr.shape\n",
        "        reshaped = seq_arr.reshape(-1, F)\n",
        "        scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "        np.save(cache_X, scaled)\n",
        "        np.save(cache_y, labels_arr)\n",
        "        print(f\"âœ… å®Œæˆ 3D å¿«å–ï¼šseq_len={seq_len}ï¼ˆå…± {B} ç­†åºåˆ—ï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-x-GLAh00T2"
      },
      "source": [
        "å®šç¾©å¿«å–å‡½æ•¸(äºŒç¶­)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "naR3mpLp0zSh"
      },
      "outputs": [],
      "source": [
        "def generate_preprocessed_cache_2d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_2d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_2d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"ğŸ“¥ å·²å­˜åœ¨ 2D å¿«å–ï¼šseq_len={seq_len}ï¼Œç•¥é\")\n",
        "            continue\n",
        "\n",
        "        all_features, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "\n",
        "                    for chunk in chunks:\n",
        "                        features = []\n",
        "                        features.extend(np.mean(chunk, axis=0))  # 3\n",
        "                        features.extend(np.std(chunk, axis=0))   # 3\n",
        "                        features.extend(np.max(chunk, axis=0))   # 3\n",
        "                        features.extend(np.min(chunk, axis=0))   # 3\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(label)\n",
        "\n",
        "        X = np.array(all_features, dtype=np.float32)\n",
        "        y = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        np.save(cache_X, X_scaled)\n",
        "        np.save(cache_y, y)\n",
        "        print(f\"âœ… å®Œæˆ 2D å¿«å–ï¼šseq_len={seq_len}ï¼ˆå…± {len(X_scaled)} ç­†ï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBeY_OhIvQ6o"
      },
      "source": [
        "å¯¦éš›åŸ·è¡Œå¿«å–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wad3ku5VvRNo",
        "outputId": "016c9895-8413-4c5e-d569-a9ad0cb07e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… å®Œæˆ 3D å¿«å–ï¼šseq_len=4ï¼ˆå…± 10116 ç­†åºåˆ—ï¼‰\n",
            "âœ… å®Œæˆ 3D å¿«å–ï¼šseq_len=5ï¼ˆå…± 8101 ç­†åºåˆ—ï¼‰\n",
            "âœ… å®Œæˆ 3D å¿«å–ï¼šseq_len=8ï¼ˆå…± 5047 ç­†åºåˆ—ï¼‰\n",
            "âœ… å®Œæˆ 3D å¿«å–ï¼šseq_len=10ï¼ˆå…± 4047 ç­†åºåˆ—ï¼‰\n",
            "âœ… å®Œæˆ 2D å¿«å–ï¼šseq_len=4ï¼ˆå…± 10116 ç­†ï¼‰\n",
            "âœ… å®Œæˆ 2D å¿«å–ï¼šseq_len=5ï¼ˆå…± 8101 ç­†ï¼‰\n",
            "âœ… å®Œæˆ 2D å¿«å–ï¼šseq_len=8ï¼ˆå…± 5047 ç­†ï¼‰\n",
            "âœ… å®Œæˆ 2D å¿«å–ï¼šseq_len=10ï¼ˆå…± 4047 ç­†ï¼‰\n"
          ]
        }
      ],
      "source": [
        "seq_lens = [4, 5, 8, 10]\n",
        "generate_preprocessed_cache_3d(seq_lens, LABEL_DIRS)      # ç”¢ç”Ÿ 3D\n",
        "generate_preprocessed_cache_2d(seq_lens, LABEL_DIRS)   # ç”¢ç”Ÿ 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0wQZBWz5Uht"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qepGDca0Yfpm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/LSTM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === LSTM æ¨¡å‹ ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === ä¸»è¨“ç·´èˆ‡è¶…åƒæ•¸æœå°‹å‡½æ•¸ï¼ˆä½¿ç”¨å¿«å–ï¼‰ ===\n",
        "def train_and_search_lstm(batch_sizes, learning_rates, seq_lens,\n",
        "                          num_epochs=100, hidden_dim=64, num_layers=1,\n",
        "                          num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = LSTMClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                           num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                # æ··æ·†çŸ©é™£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"lstm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWK53YBRYxJY",
        "outputId": "73c820f7-2da6-4105-ad94-982ecc62f23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 50 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9753 | Loss: 0.0885\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9852 | Loss: 0.0474\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9938 | Loss: 0.0330\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9901 | Loss: 0.0400\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9951 | Loss: 0.0378\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9815 | Loss: 0.0581\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9864 | Loss: 0.0446\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9901 | Loss: 0.0277\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9741 | Loss: 0.0652\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9926 | Loss: 0.0209\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9778 | Loss: 0.0881\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9827 | Loss: 0.0592\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9827 | Loss: 0.0755\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9839 | Loss: 0.0712\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9889 | Loss: 0.0619\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9802 | Loss: 0.0551\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9876 | Loss: 0.0579\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9876 | Loss: 0.0349\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9938 | Loss: 0.0236\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9926 | Loss: 0.0245\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9889 | Loss: 0.0482\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9938 | Loss: 0.0331\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9938 | Loss: 0.0253\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9951 | Loss: 0.0192\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9938 | Loss: 0.0193\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9938\n",
            "  ğŸ”¹ F1-score       : 0.9899\n",
            "  â±ï¸  Training Time  : 254.95 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/LSTM/lstm_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9938\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9938\n",
            "Final Loss     = 0.0193\n",
            "Precision      = 0.9904\n",
            "Recall         = 0.9896\n",
            "F1-score       = 0.9899\n",
            "Training Time  = 254.95 ç§’\n"
          ]
        }
      ],
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")#\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q3inndRoyYY"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SMxndCGUhHqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/MLP\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === MLP æ¨¡å‹ ===\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === è¨“ç·´èˆ‡å„²å­˜ ===\n",
        "def train_and_search_mlp(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=128,\n",
        "                         num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "                input_dim = X.shape[1]\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = MLPClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                # æ··æ·†çŸ©é™£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"mlp_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA1ydN20jMiw",
        "outputId": "8b463447-bab2-4c32-88bd-11c450222e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 50 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9728 | Loss: 0.1296\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9790 | Loss: 0.0771\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9802 | Loss: 0.0726\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9815 | Loss: 0.0558\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9852 | Loss: 0.0618\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9765 | Loss: 0.1051\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9840 | Loss: 0.0677\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9815 | Loss: 0.0660\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9815 | Loss: 0.0666\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9852 | Loss: 0.0650\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9790 | Loss: 0.0998\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9827 | Loss: 0.0683\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9839 | Loss: 0.0605\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9864 | Loss: 0.0573\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9839 | Loss: 0.0661\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9740 | Loss: 0.0996\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9852 | Loss: 0.0568\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9827 | Loss: 0.0463\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9901 | Loss: 0.0337\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9889 | Loss: 0.0323\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9802 | Loss: 0.0781\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9913 | Loss: 0.0356\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9913 | Loss: 0.0252\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9913 | Loss: 0.0207\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9926 | Loss: 0.0255\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9926\n",
            "  ğŸ”¹ F1-score       : 0.9820\n",
            "  â±ï¸  Training Time  : 92.37 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/MLP/mlp_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9926\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9926\n",
            "Final Loss     = 0.0255\n",
            "Precision      = 0.9816\n",
            "Recall         = 0.9824\n",
            "F1-score       = 0.9820\n",
            "Training Time  = 92.37 ç§’\n"
          ]
        }
      ],
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmAFC98o2iM"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gxnqos1Eo4UC"
      },
      "outputs": [],
      "source": [
        "# === SVM è¨“ç·´ç¨‹å¼ï¼ˆæ”¹å¯«ç‚ºèˆ‡ LSTM æ¶æ§‹ä¸€è‡´ï¼‰===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/SVM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === æ”¹å¯«å¾Œçš„ SVM è¨“ç·´æµç¨‹ ===\n",
        "def train_svm_search(kernels, Cs, seq_lens, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "    results = []\n",
        "    best = None\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for C_val in Cs:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ” Kernel={kernel} | C={C_val} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_list, f1_list, all_y_true, all_y_pred = [], [], [], []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "                    clf = SVC(kernel=kernel, C=C_val)\n",
        "                    clf.fit(X_train, y_train)\n",
        "                    y_pred = clf.predict(X_val)\n",
        "                    acc = accuracy_score(y_val, y_pred)\n",
        "                    f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "                    acc_list.append(acc)\n",
        "                    f1_list.append(f1)\n",
        "                    all_y_true.extend(y_val)\n",
        "                    all_y_pred.extend(y_pred)\n",
        "                t1 = time.time()\n",
        "\n",
        "                final_acc = np.mean(acc_list)\n",
        "                final_f1 = np.mean(f1_list)\n",
        "                if final_acc >= 1.0:\n",
        "                    print(f\"âš ï¸ Skipped Kernel={kernel} C={C_val} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                precision = precision_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                # æ–°å¢ä¸‰ç¨®åœ–\n",
        "                # 1. K-fold validation accuracy æ›²ç·š\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds+1), acc_list, marker='o', label='Validation Accuracy')\n",
        "                plt.title(f\"K-fold Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"accuracy_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "                # 2. K-fold F1-score æ›²ç·š\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds+1), f1_list, marker='o', label='Validation F1-score', color='orange')\n",
        "                plt.title(f\"K-fold F1-score\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"F1-score\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"f1_score_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "                # 3. è¨“ç·´é›†æº–ç¢ºç‡ï¼ˆæ•´é«”ï¼‰\n",
        "                clf_full = SVC(kernel=kernel, C=C_val)\n",
        "                clf_full.fit(X, y)\n",
        "                train_pred = clf_full.predict(X)\n",
        "                train_acc = accuracy_score(y, train_pred)\n",
        "                plt.figure()\n",
        "                plt.bar(['train'], [train_acc])\n",
        "                plt.title(f\"Train Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"train_accuracy\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # æ··æ·†çŸ©é™£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                result = {\n",
        "                    'kernel': kernel, 'C': C_val, 'seq_len': seq_len,\n",
        "                    'final_acc': final_acc, 'precision': precision,\n",
        "                    'recall': recall, 'f1_score': final_f1,\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(result)\n",
        "                if best is None or result['final_acc'] > best['final_acc']:\n",
        "                    best = result\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (Kernel={kernel}, C={C_val}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_acc:.4f}\")\n",
        "                print(f\"  ğŸ”¹ Precision      : {precision:.4f}\")\n",
        "                print(f\"  ğŸ”¹ Recall         : {recall:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_f1:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {round(t1 - t0, 2)} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"svm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ Results saved to {csv_path}\")\n",
        "    return results, best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB0a2PFxo8TZ",
        "outputId": "0ba76816-0681-4819-9a4b-0e87499bd1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Kernel=rbf | C=1.0 | SEQ=4\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=1.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9807\n",
            "  ğŸ”¹ Precision      : 0.9729\n",
            "  ğŸ”¹ Recall         : 0.9808\n",
            "  ğŸ”¹ F1-score       : 0.9768\n",
            "  â±ï¸  Training Time  : 2.0 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=1.0 | SEQ=8\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=1.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9816\n",
            "  ğŸ”¹ Precision      : 0.9717\n",
            "  ğŸ”¹ Recall         : 0.9820\n",
            "  ğŸ”¹ F1-score       : 0.9767\n",
            "  â±ï¸  Training Time  : 0.59 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=1.0 | SEQ=10\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=1.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9815\n",
            "  ğŸ”¹ Precision      : 0.9723\n",
            "  ğŸ”¹ Recall         : 0.9799\n",
            "  ğŸ”¹ F1-score       : 0.9761\n",
            "  â±ï¸  Training Time  : 0.41 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=10.0 | SEQ=4\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=10.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9870\n",
            "  ğŸ”¹ Precision      : 0.9811\n",
            "  ğŸ”¹ Recall         : 0.9851\n",
            "  ğŸ”¹ F1-score       : 0.9831\n",
            "  â±ï¸  Training Time  : 1.55 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=10.0 | SEQ=8\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=10.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9885\n",
            "  ğŸ”¹ Precision      : 0.9826\n",
            "  ğŸ”¹ Recall         : 0.9865\n",
            "  ğŸ”¹ F1-score       : 0.9845\n",
            "  â±ï¸  Training Time  : 0.76 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=10.0 | SEQ=10\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=10.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9874\n",
            "  ğŸ”¹ Precision      : 0.9814\n",
            "  ğŸ”¹ Recall         : 0.9850\n",
            "  ğŸ”¹ F1-score       : 0.9832\n",
            "  â±ï¸  Training Time  : 0.3 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=1.0 | SEQ=4\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=1.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9354\n",
            "  ğŸ”¹ Precision      : 0.9124\n",
            "  ğŸ”¹ Recall         : 0.9024\n",
            "  ğŸ”¹ F1-score       : 0.9040\n",
            "  â±ï¸  Training Time  : 6.4 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=1.0 | SEQ=8\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=1.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9408\n",
            "  ğŸ”¹ Precision      : 0.9200\n",
            "  ğŸ”¹ Recall         : 0.9097\n",
            "  ğŸ”¹ F1-score       : 0.9107\n",
            "  â±ï¸  Training Time  : 2.75 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=1.0 | SEQ=10\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=1.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9377\n",
            "  ğŸ”¹ Precision      : 0.9136\n",
            "  ğŸ”¹ Recall         : 0.9032\n",
            "  ğŸ”¹ F1-score       : 0.9042\n",
            "  â±ï¸  Training Time  : 1.02 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=10.0 | SEQ=4\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=10.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9666\n",
            "  ğŸ”¹ Precision      : 0.9463\n",
            "  ğŸ”¹ Recall         : 0.9510\n",
            "  ğŸ”¹ F1-score       : 0.9482\n",
            "  â±ï¸  Training Time  : 18.42 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=10.0 | SEQ=8\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=10.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9772\n",
            "  ğŸ”¹ Precision      : 0.9578\n",
            "  ğŸ”¹ Recall         : 0.9722\n",
            "  ğŸ”¹ F1-score       : 0.9646\n",
            "  â±ï¸  Training Time  : 3.07 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=10.0 | SEQ=10\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=10.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9748\n",
            "  ğŸ”¹ Precision      : 0.9535\n",
            "  ğŸ”¹ Recall         : 0.9680\n",
            "  ğŸ”¹ F1-score       : 0.9603\n",
            "  â±ï¸  Training Time  : 2.09 ç§’\n",
            "\n",
            "ğŸ“„ Results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/SVM/svm_experiment_results.csv\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Sequence Length = 8\n",
            "C = 10.0\n",
            "Kernel = rbf\n",
            "Accuracy = 0.9885\n",
            "Precision = 0.9826\n",
            "Recall = 0.9865\n",
            "F1-score = 0.9845\n",
            "Training Time = 0.76 ç§’\n"
          ]
        }
      ],
      "source": [
        "seq_lens = [4, 8, 10]\n",
        "C_list = [1.0, 10.0]\n",
        "kernel_list = [\"rbf\", \"linear\"]\n",
        "\n",
        "results, best = train_svm_search(kernel_list, C_list, seq_lens)\n",
        "\n",
        "print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "print(f\"Sequence Length = {best['seq_len']}\")\n",
        "print(f\"C = {best['C']}\")\n",
        "print(f\"Kernel = {best['kernel']}\")\n",
        "print(f\"Accuracy = {best['final_acc']:.4f}\")\n",
        "print(f\"Precision = {best['precision']:.4f}\")\n",
        "print(f\"Recall = {best['recall']:.4f}\")\n",
        "print(f\"F1-score = {best['f1_score']:.4f}\")\n",
        "print(f\"Training Time = {best['training_time_s']} ç§’\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVLlUCDrD_2"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZVsy2pBvrFfh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/GRU\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === GRU æ¨¡å‹ ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === è¨“ç·´èˆ‡å„²å­˜ ===\n",
        "def train_and_search_gru(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X = np.load(os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\"))\n",
        "                y = np.load(os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\"))\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = GRUClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        correct_train, total_train, total_loss_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train_list.append(correct_train / total_train)\n",
        "                        loss_train_list.append(total_loss_train / total_train)\n",
        "\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc_list.append(correct / total)\n",
        "                        loss_list.append(total_loss / total)\n",
        "                        f1_list.append(f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0))\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc_list[-1]:.4f} | Loss: {loss_list[-1]:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder, bs, lr, seq_len, num_epochs):\n",
        "                  plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                  plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                  plt.xlabel(\"Epoch\")\n",
        "                  plt.ylabel(metric_name.capitalize())\n",
        "                  plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                  plt.grid(True)\n",
        "                  path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                  plt.savefig(path, bbox_inches='tight')\n",
        "                  plt.close()\n",
        "\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"gru_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVWAGumMrPmo",
        "outputId": "6d7e11c5-c416-4f6d-8b49-d88bd8754dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 50 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.0001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.9457 | Loss: 0.2989\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9444 | Loss: 0.1641\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9716 | Loss: 0.1028\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9741 | Loss: 0.0860\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9741 | Loss: 0.0800\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.9432 | Loss: 0.2125\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9667 | Loss: 0.1231\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9778 | Loss: 0.0901\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9778 | Loss: 0.0795\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9827 | Loss: 0.0682\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.9159 | Loss: 0.3364\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9518 | Loss: 0.1527\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9765 | Loss: 0.0945\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9790 | Loss: 0.0834\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9778 | Loss: 0.0783\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.9333 | Loss: 0.2528\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9654 | Loss: 0.1483\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9778 | Loss: 0.0944\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9778 | Loss: 0.0720\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9815 | Loss: 0.0622\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.9506 | Loss: 0.2293\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9716 | Loss: 0.1186\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9765 | Loss: 0.0800\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9790 | Loss: 0.0637\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9778 | Loss: 0.0605\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.0001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9778\n",
            "  ğŸ”¹ F1-score       : 0.9703\n",
            "  â±ï¸  Training Time  : 350.37 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/GRU/gru_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.0001 | SEQ=10.0 | ACC=0.9778\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.0001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9778\n",
            "Final Loss     = 0.0605\n",
            "Precision      = 0.9665\n",
            "Recall         = 0.9744\n",
            "F1-score       = 0.9703\n",
            "Training Time  = 350.37 ç§’\n"
          ]
        }
      ],
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 5, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKn_cKbgu0O"
      },
      "source": [
        "1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tWrgapHBgxcw"
      },
      "outputs": [],
      "source": [
        "# === CNN æ¨¡å‹å®Œæ•´è¨“ç·´ç¨‹å¼ ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/CNN\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === CNN æ¨¡å‹ ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === ä¸»è¨“ç·´èˆ‡è¶…åƒæ•¸æœå°‹å‡½æ•¸ï¼ˆä½¿ç”¨å¿«å–ï¼‰ ===\n",
        "def train_and_search_cnn(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = CNNClassifier(num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"cnn_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "D2P8TmkIijrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3753f6-789e-4e34-cfe3-67263a6a73f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 50 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.0001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/50 | Acc: 0.8605 | Loss: 0.4802\n",
            "    [Fold 1] Epoch 20/50 | Acc: 0.9617 | Loss: 0.2878\n",
            "    [Fold 1] Epoch 30/50 | Acc: 0.9642 | Loss: 0.2157\n",
            "    [Fold 1] Epoch 40/50 | Acc: 0.9667 | Loss: 0.1697\n",
            "    [Fold 1] Epoch 50/50 | Acc: 0.9741 | Loss: 0.1339\n",
            "    [Fold 2] Epoch 10/50 | Acc: 0.8728 | Loss: 0.4318\n",
            "    [Fold 2] Epoch 20/50 | Acc: 0.9605 | Loss: 0.2157\n",
            "    [Fold 2] Epoch 30/50 | Acc: 0.9704 | Loss: 0.1497\n",
            "    [Fold 2] Epoch 40/50 | Acc: 0.9765 | Loss: 0.1192\n",
            "    [Fold 2] Epoch 50/50 | Acc: 0.9778 | Loss: 0.1023\n",
            "    [Fold 3] Epoch 10/50 | Acc: 0.8133 | Loss: 0.5794\n",
            "    [Fold 3] Epoch 20/50 | Acc: 0.9333 | Loss: 0.3493\n",
            "    [Fold 3] Epoch 30/50 | Acc: 0.9629 | Loss: 0.2296\n",
            "    [Fold 3] Epoch 40/50 | Acc: 0.9691 | Loss: 0.1655\n",
            "    [Fold 3] Epoch 50/50 | Acc: 0.9753 | Loss: 0.1307\n",
            "    [Fold 4] Epoch 10/50 | Acc: 0.8195 | Loss: 0.5850\n",
            "    [Fold 4] Epoch 20/50 | Acc: 0.9295 | Loss: 0.3169\n",
            "    [Fold 4] Epoch 30/50 | Acc: 0.9642 | Loss: 0.1765\n",
            "    [Fold 4] Epoch 40/50 | Acc: 0.9716 | Loss: 0.1226\n",
            "    [Fold 4] Epoch 50/50 | Acc: 0.9815 | Loss: 0.0963\n",
            "    [Fold 5] Epoch 10/50 | Acc: 0.8566 | Loss: 0.4442\n",
            "    [Fold 5] Epoch 20/50 | Acc: 0.9679 | Loss: 0.2178\n",
            "    [Fold 5] Epoch 30/50 | Acc: 0.9728 | Loss: 0.1297\n",
            "    [Fold 5] Epoch 40/50 | Acc: 0.9790 | Loss: 0.0930\n",
            "    [Fold 5] Epoch 50/50 | Acc: 0.9827 | Loss: 0.0763\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.0001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9827\n",
            "  ğŸ”¹ F1-score       : 0.9679\n",
            "  â±ï¸  Training Time  : 194.21 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_3/CNN/cnn_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.0001 | SEQ=10.0 | ACC=0.9827\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.0001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9827\n",
            "Final Loss     = 0.0763\n",
            "Precision      = 0.9618\n",
            "Recall         = 0.9743\n",
            "F1-score       = 0.9679\n",
            "Training Time  = 194.21 ç§’\n"
          ]
        }
      ],
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "#batch_sizes = [4, 8, 16]\n",
        "#learning_rates = [1e-3, 1e-4]\n",
        "#seq_lens = [4, 5, 8, 10]\n",
        "#num_epochs = 100\n",
        "\n",
        "batch_sizes = [16]\n",
        "learning_rates = [1e-4]\n",
        "seq_lens = [10]\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGn2JGVMcnqP7n5A+Xt/QU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}