{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHHDZ74kfhF+JoKyVEbL2O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# æ›è¼‰ Google é›²ç«¯ç¡¬ç¢Ÿ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# è³‡æ–™é›†æ ¹ç›®éŒ„\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "\n",
        "# å„åˆ†é¡è³‡æ–™å¤¾è¨­å®š\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdeWQTkaMyj",
        "outputId": "23a24583-c5c2-4d6c-ab45-5fac07dae507"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å®šç¾©å¿«å–å‡½æ•¸(ä¸‰ç¶­)"
      ],
      "metadata": {
        "id": "GBh_Hm3gvQA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def generate_preprocessed_cache_3d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_3d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_3d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"ğŸ“¥ å·²å­˜åœ¨ 3D å¿«å–ï¼šseq_len={seq_len}ï¼Œç•¥é\")\n",
        "            continue\n",
        "\n",
        "        all_seq, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "                    all_seq.extend(chunks)\n",
        "                    all_labels.extend([label] * len(chunks))\n",
        "\n",
        "        seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "        labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "        B, T, F = seq_arr.shape\n",
        "        reshaped = seq_arr.reshape(-1, F)\n",
        "        scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "        np.save(cache_X, scaled)\n",
        "        np.save(cache_y, labels_arr)\n",
        "        print(f\"âœ… å®Œæˆ 3D å¿«å–ï¼šseq_len={seq_len}ï¼ˆå…± {B} ç­†åºåˆ—ï¼‰\")"
      ],
      "metadata": {
        "id": "oay5nopuvOx3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å®šç¾©å¿«å–å‡½æ•¸(äºŒç¶­)"
      ],
      "metadata": {
        "id": "a-x-GLAh00T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_preprocessed_cache_2d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_2d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_2d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"ğŸ“¥ å·²å­˜åœ¨ 2D å¿«å–ï¼šseq_len={seq_len}ï¼Œç•¥é\")\n",
        "            continue\n",
        "\n",
        "        all_features, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "\n",
        "                    for chunk in chunks:\n",
        "                        features = []\n",
        "                        features.extend(np.mean(chunk, axis=0))  # 3\n",
        "                        features.extend(np.std(chunk, axis=0))   # 3\n",
        "                        features.extend(np.max(chunk, axis=0))   # 3\n",
        "                        features.extend(np.min(chunk, axis=0))   # 3\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(label)\n",
        "\n",
        "        X = np.array(all_features, dtype=np.float32)\n",
        "        y = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        np.save(cache_X, X_scaled)\n",
        "        np.save(cache_y, y)\n",
        "        print(f\"âœ… å®Œæˆ 2D å¿«å–ï¼šseq_len={seq_len}ï¼ˆå…± {len(X_scaled)} ç­†ï¼‰\")"
      ],
      "metadata": {
        "id": "naR3mpLp0zSh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å¯¦éš›åŸ·è¡Œå¿«å–"
      ],
      "metadata": {
        "id": "CBeY_OhIvQ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_lens = [4, 5, 8, 10]\n",
        "generate_preprocessed_cache_3d(seq_lens, LABEL_DIRS)      # ç”¢ç”Ÿ 3D\n",
        "generate_preprocessed_cache_2d(seq_lens, LABEL_DIRS)   # ç”¢ç”Ÿ 2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wad3ku5VvRNo",
        "outputId": "9433f25a-008b-41d4-b76d-f9da0dd71396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ å·²å­˜åœ¨ 3D å¿«å–ï¼šseq_len=4ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 3D å¿«å–ï¼šseq_len=5ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 3D å¿«å–ï¼šseq_len=8ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 3D å¿«å–ï¼šseq_len=10ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 2D å¿«å–ï¼šseq_len=4ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 2D å¿«å–ï¼šseq_len=5ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 2D å¿«å–ï¼šseq_len=8ï¼Œç•¥é\n",
            "ğŸ“¥ å·²å­˜åœ¨ 2D å¿«å–ï¼šseq_len=10ï¼Œç•¥é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "t0wQZBWz5Uht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"./models/LSTM/result\"\n",
        "for sub in [\n",
        "    \"accuracy_curves\",         # âœ… æ¯æ¬¡ epoch çš„å¹³å‡æº–ç¢ºç‡\n",
        "    \"loss_curves\",             # âœ… æ¯æ¬¡ epoch çš„å¹³å‡ loss\n",
        "    \"f1_score_curves\",         # âœ… æ¯æ¬¡ epoch çš„ F1-score\n",
        "    \"confusion_matrices\",      # âœ… æ··æ·†çŸ©é™£åœ–\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === LSTM æ¨¡å‹ ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === ä¸»è¨“ç·´èˆ‡è¶…åƒæ•¸æœå°‹å‡½æ•¸ï¼ˆä½¿ç”¨å¿«å–ï¼‰ ===\n",
        "def train_and_search_lstm(batch_sizes, learning_rates, seq_lens,\n",
        "                          num_epochs=100, hidden_dim=64, num_layers=1,\n",
        "                          num_classes=4, k_folds=5):\n",
        "\n",
        "    RESULT_DIR = \"./models/LSTM/result\"\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = LSTMClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                           num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                # æ··æ·†çŸ©é™£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"lstm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "qepGDca0Yfpm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "#batch_sizes = [4, 8, 16]\n",
        "#learning_rates = [1e-3, 1e-4]\n",
        "#seq_lens = [4, 5, 8, 10]\n",
        "#num_epochs = 100\n",
        "\n",
        "batch_sizes = [16]\n",
        "learning_rates = [1e-3]\n",
        "seq_lens = [10]\n",
        "num_epochs = 10\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")#\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWK53YBRYxJY",
        "outputId": "8865188b-2bf3-4b8e-e74f-6691f7ee67d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 10 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/10 | Acc: 0.9790 | Loss: 0.0751\n",
            "    [Fold 2] Epoch 10/10 | Acc: 0.9765 | Loss: 0.0698\n",
            "    [Fold 3] Epoch 10/10 | Acc: 0.9778 | Loss: 0.0862\n",
            "    [Fold 4] Epoch 10/10 | Acc: 0.9815 | Loss: 0.0590\n",
            "    [Fold 5] Epoch 10/10 | Acc: 0.9839 | Loss: 0.0607\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9839\n",
            "  ğŸ”¹ F1-score       : 0.9735\n",
            "  â±ï¸  Training Time  : 57.5 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to ./models/LSTM/result/lstm_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9839\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9839\n",
            "Final Loss     = 0.0607\n",
            "Precision      = 0.9680\n",
            "Recall         = 0.9797\n",
            "F1-score       = 0.9735\n",
            "Training Time  = 57.5 ç§’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "_q3inndRoyYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SMxndCGUhHqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"./models/MLP/result\"\n",
        "for sub in [\n",
        "    \"accuracy_curves\",         # âœ… æ¯æ¬¡ epoch çš„å¹³å‡æº–ç¢ºç‡\n",
        "    \"loss_curves\",             # âœ… æ¯æ¬¡ epoch çš„å¹³å‡ loss\n",
        "    \"f1_score_curves\",         # âœ… æ¯æ¬¡ epoch çš„ F1-score\n",
        "    \"confusion_matrices\",      # âœ… æ··æ·†çŸ©é™£åœ–\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === MLP æ¨¡å‹ ===\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === è¨“ç·´èˆ‡å„²å­˜ ===\n",
        "def train_and_search_mlp(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=128,\n",
        "                         num_classes=4, k_folds=5):\n",
        "\n",
        "    RESULT_DIR = \"./models/MLP/result\"\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "                input_dim = X.shape[1]\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = MLPClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                # æ··æ·†çŸ©é™£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"mlp_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "#batch_sizes = [4, 8, 16]\n",
        "#learning_rates = [1e-3, 1e-4]\n",
        "#seq_lens = [4, 5, 8, 10]\n",
        "#num_epochs = 100\n",
        "\n",
        "batch_sizes = [16]\n",
        "learning_rates = [1e-3]\n",
        "seq_lens = [10]\n",
        "num_epochs = 10\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA1ydN20jMiw",
        "outputId": "300a5fde-fd7e-477d-f7e7-9becbfb0de44"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 10 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/10 | Acc: 0.9716 | Loss: 0.1446\n",
            "    [Fold 2] Epoch 10/10 | Acc: 0.9778 | Loss: 0.1212\n",
            "    [Fold 3] Epoch 10/10 | Acc: 0.9790 | Loss: 0.0999\n",
            "    [Fold 4] Epoch 10/10 | Acc: 0.9765 | Loss: 0.1008\n",
            "    [Fold 5] Epoch 10/10 | Acc: 0.9815 | Loss: 0.0669\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9815\n",
            "  ğŸ”¹ F1-score       : 0.9665\n",
            "  â±ï¸  Training Time  : 17.61 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to ./models/MLP/result/mlp_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9815\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9815\n",
            "Final Loss     = 0.0669\n",
            "Precision      = 0.9616\n",
            "Recall         = 0.9717\n",
            "F1-score       = 0.9665\n",
            "Training Time  = 17.61 ç§’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "8cmAFC98o2iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SVM è¨“ç·´ç¨‹å¼ï¼ˆæ”¹å¯«ç‚ºèˆ‡ LSTM æ¶æ§‹ä¸€è‡´ï¼‰===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"./models/SVM/result\"\n",
        "os.makedirs(os.path.join(RESULT_DIR, \"confusion_matrices\"), exist_ok=True)\n",
        "\n",
        "# === è³‡æ–™è™•ç† ===\n",
        "def process_file(file_path, label, max_seq_len):\n",
        "    df = pd.read_csv(file_path)\n",
        "    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "    num_chunks = len(data) // max_seq_len\n",
        "    chunks = [data[i * max_seq_len : (i + 1) * max_seq_len] for i in range(num_chunks)]\n",
        "    return chunks, [label] * len(chunks)\n",
        "\n",
        "def load_all_sequences(max_seq_len):\n",
        "    all_seq, all_labels = [], []\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".csv\"):\n",
        "                path = os.path.join(folder, fname)\n",
        "                seqs, labels = process_file(path, label, max_seq_len)\n",
        "                all_seq.extend(seqs)\n",
        "                all_labels.extend(labels)\n",
        "\n",
        "    seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "    num_samples = seq_arr.shape[0]\n",
        "    seq_reshaped = seq_arr.reshape(num_samples, -1)\n",
        "    scaled = StandardScaler().fit_transform(seq_reshaped)\n",
        "\n",
        "    print(f\"ğŸ“Š Total sequences loaded: {num_samples}\")\n",
        "    return scaled, np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "# === æ”¹å¯«å¾Œçš„ SVM è¨“ç·´æµç¨‹ ===\n",
        "def train_svm_search(kernels, Cs, seq_lens, k_folds=5):\n",
        "    results = []\n",
        "    best = None\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for C_val in Cs:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ” Kernel={kernel} | C={C_val} | SEQ={seq_len}\")\n",
        "                X, y = load_all_sequences(seq_len)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_list, f1_list, all_y_true, all_y_pred = [], [], [], []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    clf = SVC(kernel=kernel, C=C_val)\n",
        "                    clf.fit(X_train, y_train)\n",
        "\n",
        "                    y_pred = clf.predict(X_val)\n",
        "                    acc = accuracy_score(y_val, y_pred)\n",
        "                    f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                    acc_list.append(acc)\n",
        "                    f1_list.append(f1)\n",
        "                    all_y_true.extend(y_val)\n",
        "                    all_y_pred.extend(y_pred)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                final_acc = np.mean(acc_list)\n",
        "                final_f1 = np.mean(f1_list)\n",
        "\n",
        "                if final_acc >= 1.0:\n",
        "                    print(f\"âš ï¸ Skipped Kernel={kernel} C={C_val} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                precision = precision_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                result = {\n",
        "                    'kernel': kernel, 'C': C_val, 'seq_len': seq_len,\n",
        "                    'final_acc': final_acc, 'precision': precision,\n",
        "                    'recall': recall, 'f1_score': final_f1,\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "                if best is None or result['final_acc'] > best['final_acc']:\n",
        "                    best = result\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (Kernel={kernel}, C={C_val}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_acc:.4f}\")\n",
        "                print(f\"  ğŸ”¹ Precision      : {precision:.4f}\")\n",
        "                print(f\"  ğŸ”¹ Recall         : {recall:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_f1:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {round(t1 - t0, 2)} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"svm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ Results saved to {csv_path}\")\n",
        "\n",
        "    return results, best"
      ],
      "metadata": {
        "id": "Gxnqos1Eo4UC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_lens = [4, 8, 10]\n",
        "C_list = [1.0, 10.0]\n",
        "kernel_list = [\"rbf\", \"linear\"]\n",
        "\n",
        "results, best = train_svm_search(kernel_list, C_list, seq_lens)\n",
        "\n",
        "print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "print(f\"Sequence Length = {best['seq_len']}\")\n",
        "print(f\"C = {best['C']}\")\n",
        "print(f\"Kernel = {best['kernel']}\")\n",
        "print(f\"Accuracy = {best['final_acc']:.4f}\")\n",
        "print(f\"Precision = {best['precision']:.4f}\")\n",
        "print(f\"Recall = {best['recall']:.4f}\")\n",
        "print(f\"F1-score = {best['f1_score']:.4f}\")\n",
        "print(f\"Training Time = {best['training_time_s']} ç§’\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB0a2PFxo8TZ",
        "outputId": "471c64ef-7310-41b5-e409-1a3ee3a6611f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Kernel=rbf | C=1.0 | SEQ=4\n",
            "ğŸ“Š Total sequences loaded: 10116\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=1.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9797\n",
            "  ğŸ”¹ Precision      : 0.9721\n",
            "  ğŸ”¹ Recall         : 0.9799\n",
            "  ğŸ”¹ F1-score       : 0.9759\n",
            "  â±ï¸  Training Time  : 4.07 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=1.0 | SEQ=8\n",
            "ğŸ“Š Total sequences loaded: 5047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=1.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9812\n",
            "  ğŸ”¹ Precision      : 0.9688\n",
            "  ğŸ”¹ Recall         : 0.9822\n",
            "  ğŸ”¹ F1-score       : 0.9753\n",
            "  â±ï¸  Training Time  : 0.62 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=1.0 | SEQ=10\n",
            "ğŸ“Š Total sequences loaded: 4047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=1.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9805\n",
            "  ğŸ”¹ Precision      : 0.9680\n",
            "  ğŸ”¹ Recall         : 0.9808\n",
            "  ğŸ”¹ F1-score       : 0.9743\n",
            "  â±ï¸  Training Time  : 0.52 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=10.0 | SEQ=4\n",
            "ğŸ“Š Total sequences loaded: 10116\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=10.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9866\n",
            "  ğŸ”¹ Precision      : 0.9810\n",
            "  ğŸ”¹ Recall         : 0.9853\n",
            "  ğŸ”¹ F1-score       : 0.9831\n",
            "  â±ï¸  Training Time  : 1.26 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=10.0 | SEQ=8\n",
            "ğŸ“Š Total sequences loaded: 5047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=10.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9889\n",
            "  ğŸ”¹ Precision      : 0.9837\n",
            "  ğŸ”¹ Recall         : 0.9867\n",
            "  ğŸ”¹ F1-score       : 0.9852\n",
            "  â±ï¸  Training Time  : 0.4 ç§’\n",
            "\n",
            "ğŸ” Kernel=rbf | C=10.0 | SEQ=10\n",
            "ğŸ“Š Total sequences loaded: 4047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=rbf, C=10.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9876\n",
            "  ğŸ”¹ Precision      : 0.9813\n",
            "  ğŸ”¹ Recall         : 0.9858\n",
            "  ğŸ”¹ F1-score       : 0.9835\n",
            "  â±ï¸  Training Time  : 0.33 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=1.0 | SEQ=4\n",
            "ğŸ“Š Total sequences loaded: 10116\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=1.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9579\n",
            "  ğŸ”¹ Precision      : 0.9336\n",
            "  ğŸ”¹ Recall         : 0.9484\n",
            "  ğŸ”¹ F1-score       : 0.9399\n",
            "  â±ï¸  Training Time  : 8.06 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=1.0 | SEQ=8\n",
            "ğŸ“Š Total sequences loaded: 5047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=1.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9691\n",
            "  ğŸ”¹ Precision      : 0.9435\n",
            "  ğŸ”¹ Recall         : 0.9612\n",
            "  ğŸ”¹ F1-score       : 0.9510\n",
            "  â±ï¸  Training Time  : 1.93 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=1.0 | SEQ=10\n",
            "ğŸ“Š Total sequences loaded: 4047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=1.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9706\n",
            "  ğŸ”¹ Precision      : 0.9452\n",
            "  ğŸ”¹ Recall         : 0.9629\n",
            "  ğŸ”¹ F1-score       : 0.9528\n",
            "  â±ï¸  Training Time  : 1.61 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=10.0 | SEQ=4\n",
            "ğŸ“Š Total sequences loaded: 10116\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=10.0, SEQ=4):\n",
            "  ğŸ”¹ Final Accuracy : 0.9624\n",
            "  ğŸ”¹ Precision      : 0.9508\n",
            "  ğŸ”¹ Recall         : 0.9468\n",
            "  ğŸ”¹ F1-score       : 0.9473\n",
            "  â±ï¸  Training Time  : 12.9 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=10.0 | SEQ=8\n",
            "ğŸ“Š Total sequences loaded: 5047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=10.0, SEQ=8):\n",
            "  ğŸ”¹ Final Accuracy : 0.9723\n",
            "  ğŸ”¹ Precision      : 0.9519\n",
            "  ğŸ”¹ Recall         : 0.9602\n",
            "  ğŸ”¹ F1-score       : 0.9557\n",
            "  â±ï¸  Training Time  : 5.41 ç§’\n",
            "\n",
            "ğŸ” Kernel=linear | C=10.0 | SEQ=10\n",
            "ğŸ“Š Total sequences loaded: 4047\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (Kernel=linear, C=10.0, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9706\n",
            "  ğŸ”¹ Precision      : 0.9530\n",
            "  ğŸ”¹ Recall         : 0.9557\n",
            "  ğŸ”¹ F1-score       : 0.9536\n",
            "  â±ï¸  Training Time  : 4.21 ç§’\n",
            "\n",
            "ğŸ“„ Results saved to ./models/SVM/result/svm_experiment_results.csv\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Sequence Length = 8\n",
            "C = 10.0\n",
            "Kernel = rbf\n",
            "Accuracy = 0.9889\n",
            "Precision = 0.9837\n",
            "Recall = 0.9867\n",
            "F1-score = 0.9852\n",
            "Training Time = 0.4 ç§’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "lxVLlUCDrD_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"./models/GRU/result\"\n",
        "for sub in [\n",
        "    \"accuracy_curves\",         # âœ… æ¯æ¬¡ epoch çš„å¹³å‡æº–ç¢ºç‡\n",
        "    \"loss_curves\",             # âœ… æ¯æ¬¡ epoch çš„å¹³å‡ loss\n",
        "    \"f1_score_curves\",         # âœ… æ¯æ¬¡ epoch çš„ F1-score\n",
        "    \"confusion_matrices\",      # âœ… æ··æ·†çŸ©é™£åœ–\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === GRU æ¨¡å‹ ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === è¨“ç·´èˆ‡å„²å­˜ ===\n",
        "def train_and_search_gru(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4, k_folds=5):\n",
        "\n",
        "    RESULT_DIR = \"./models/GRU/result\"\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X = np.load(os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\"))\n",
        "                y = np.load(os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\"))\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = GRUClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        correct_train, total_train, total_loss_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train_list.append(correct_train / total_train)\n",
        "                        loss_train_list.append(total_loss_train / total_train)\n",
        "\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc_list.append(correct / total)\n",
        "                        loss_list.append(total_loss / total)\n",
        "                        f1_list.append(f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0))\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc_list[-1]:.4f} | Loss: {loss_list[-1]:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder, bs, lr, seq_len, num_epochs):\n",
        "                  plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                  plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                  plt.xlabel(\"Epoch\")\n",
        "                  plt.ylabel(metric_name.capitalize())\n",
        "                  plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                  plt.grid(True)\n",
        "                  path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                  plt.savefig(path, bbox_inches='tight')\n",
        "                  plt.close()\n",
        "\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"gru_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "ZVsy2pBvrFfh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "#batch_sizes = [4, 8, 16]\n",
        "#learning_rates = [1e-3, 1e-4]\n",
        "#seq_lens = [4, 5, 8, 10]\n",
        "#num_epochs = 100\n",
        "\n",
        "batch_sizes = [16]\n",
        "learning_rates = [1e-3]\n",
        "seq_lens = [10]\n",
        "num_epochs = 10\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVWAGumMrPmo",
        "outputId": "5a0b0fa5-ca26-44d7-909b-fd8277fbe629"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 10 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/10 | Acc: 0.9864 | Loss: 0.0738\n",
            "    [Fold 2] Epoch 10/10 | Acc: 0.9815 | Loss: 0.0646\n",
            "    [Fold 3] Epoch 10/10 | Acc: 0.9753 | Loss: 0.0837\n",
            "    [Fold 4] Epoch 10/10 | Acc: 0.9765 | Loss: 0.0631\n",
            "    [Fold 5] Epoch 10/10 | Acc: 0.9864 | Loss: 0.0582\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9864\n",
            "  ğŸ”¹ F1-score       : 0.9745\n",
            "  â±ï¸  Training Time  : 78.37 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to ./models/GRU/result/gru_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9864\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9864\n",
            "Final Loss     = 0.0582\n",
            "Precision      = 0.9731\n",
            "Recall         = 0.9761\n",
            "F1-score       = 0.9745\n",
            "Training Time  = 78.37 ç§’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNN"
      ],
      "metadata": {
        "id": "QeKn_cKbgu0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CNN æ¨¡å‹å®Œæ•´è¨“ç·´ç¨‹å¼ ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === è³‡æ–™å¤¾èˆ‡å„²å­˜è·¯å¾‘è¨­å®š ===\n",
        "RESULT_DIR = \"./models/CNN/result\"\n",
        "for sub in [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === è‡ªè¨‚ Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === CNN æ¨¡å‹ ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === ä¸»è¨“ç·´èˆ‡è¶…åƒæ•¸æœå°‹å‡½æ•¸ï¼ˆä½¿ç”¨å¿«å–ï¼‰ ===\n",
        "def train_and_search_cnn(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nğŸ§ª BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = CNNClassifier(num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric(avg_list, metric_name, color, marker, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), avg_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    plt.ylim(0, 1.0 if metric_name != 'loss' else None)\n",
        "                    plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric(np.mean(acc_curves, axis=0), \"accuracy\", \"black\", \"o\", \"accuracy_curves\")\n",
        "                plot_metric(np.mean(loss_curves, axis=0), \"loss\", \"orange\", \"s\", \"loss_curves\")\n",
        "                plot_metric(np.mean(f1_curves, axis=0), \"f1_score\", \"purple\", \"^\", \"f1_score_curves\")\n",
        "                plot_metric(np.mean(acc_train_curves, axis=0), \"accuracy\", \"red\", \"o\", \"train_accuracy\")\n",
        "                plot_metric(np.mean(loss_train_curves, axis=0), \"loss\", \"blue\", \"s\", \"train_loss\")\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nğŸ“Š çµ±è¨ˆçµæœ (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  ğŸ”¹ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  ğŸ”¹ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  â±ï¸  Training Time  : {final_result['training_time_s']} ç§’\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"cnn_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nğŸ“„ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nğŸ† Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\nâ— No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "tWrgapHBgxcw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "#batch_sizes = [4, 8, 16]\n",
        "#learning_rates = [1e-3, 1e-4]\n",
        "#seq_lens = [4, 5, 8, 10]\n",
        "#num_epochs = 100\n",
        "\n",
        "batch_sizes = [16]\n",
        "learning_rates = [1e-3]\n",
        "seq_lens = [10]\n",
        "num_epochs = 10\n",
        "\n",
        "# é¡¯ç¤ºç¸½çµ„æ•¸\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š{total_combinations}ï¼Œæ¯çµ„è¨“ç·´ {num_epochs} epochs\")\n",
        "\n",
        "# åŸ·è¡Œç¶²æ ¼æœå°‹è¨“ç·´\n",
        "results, best = train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# è¼¸å‡ºæœ€ä½³åƒæ•¸èˆ‡æŒ‡æ¨™\n",
        "if best is not None:\n",
        "    print(\"\\nğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} ç§’\")\n",
        "else:\n",
        "    print(\"â— æ²’æœ‰æœ‰æ•ˆçµæœï¼ˆæº–ç¢ºç‡å…¨éƒ¨ç‚º 1.0ï¼‰\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2P8TmkIijrl",
        "outputId": "f3540faa-38e5-4ae4-92ac-56861588bcff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¸½å…±è¨“ç·´çµ„æ•¸ï¼š1ï¼Œæ¯çµ„è¨“ç·´ 10 epochs\n",
            "\n",
            "ğŸ§ª BS=16 | LR=0.001 | SEQ=10\n",
            "    [Fold 1] Epoch 10/10 | Acc: 0.9741 | Loss: 0.0905\n",
            "    [Fold 2] Epoch 10/10 | Acc: 0.9802 | Loss: 0.0692\n",
            "    [Fold 3] Epoch 10/10 | Acc: 0.9790 | Loss: 0.0735\n",
            "    [Fold 4] Epoch 10/10 | Acc: 0.9876 | Loss: 0.0634\n",
            "    [Fold 5] Epoch 10/10 | Acc: 0.9913 | Loss: 0.0453\n",
            "\n",
            "ğŸ“Š çµ±è¨ˆçµæœ (BS=16, LR=0.001, SEQ=10):\n",
            "  ğŸ”¹ Final Accuracy : 0.9913\n",
            "  ğŸ”¹ F1-score       : 0.9752\n",
            "  â±ï¸  Training Time  : 48.62 ç§’\n",
            "\n",
            "ğŸ“„ All experiment results saved to ./models/CNN/result/cnn_experiment_results.csv\n",
            "\n",
            "ğŸ† Best: BS=16.0 | LR=0.001 | SEQ=10.0 | ACC=0.9913\n",
            "\n",
            "ğŸ¯ æœ€ä½³åƒæ•¸çµ„åˆï¼š\n",
            "Batch Size     = 16.0\n",
            "Learning Rate  = 0.001\n",
            "Sequence Length= 10.0\n",
            "Final Accuracy = 0.9913\n",
            "Final Loss     = 0.0453\n",
            "Precision      = 0.9729\n",
            "Recall         = 0.9776\n",
            "F1-score       = 0.9752\n",
            "Training Time  = 48.62 ç§’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å­˜çµæœ"
      ],
      "metadata": {
        "id": "XkiW7K9GsDV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPORT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/new\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "_BreJ33FsC_a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# æ¨¡å‹æ¸…å–®ï¼ˆä¾ç…§ä½ è¨“ç·´éçš„æ¨¡å‹å‘½åï¼‰\n",
        "model_names = [\"LSTM\", \"GRU\", \"MLP\", \"SVM\", \"CNN\"]\n",
        "\n",
        "# æ¯ç¨®åœ–è¡¨é¡å‹\n",
        "curve_folders = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "# åŒ¯å‡ºè·¯å¾‘\n",
        "EXPORT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/new\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "for model in model_names:\n",
        "    model_result_path = f\"./models/{model}/result\"\n",
        "    export_model_path = os.path.join(EXPORT_DIR, model)\n",
        "    os.makedirs(export_model_path, exist_ok=True)\n",
        "\n",
        "    # åŒ¯å‡ºå¯¦é©— CSV æª”\n",
        "    csv_name = f\"{model.lower()}_experiment_results.csv\"\n",
        "    csv_path = os.path.join(model_result_path, csv_name)\n",
        "    if os.path.exists(csv_path):\n",
        "        shutil.copy(csv_path, os.path.join(export_model_path, \"experiment_results.csv\"))\n",
        "\n",
        "    # åŒ¯å‡ºåœ–è¡¨è³‡æ–™å¤¾\n",
        "    for folder in curve_folders:\n",
        "        src_folder = os.path.join(model_result_path, folder)\n",
        "        dst_folder = os.path.join(export_model_path, folder)\n",
        "        if os.path.exists(src_folder):\n",
        "            shutil.copytree(src_folder, dst_folder, dirs_exist_ok=True)\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰æ¨¡å‹çš„çµæœå·²å„²å­˜è‡³ï¼š\", EXPORT_DIR)"
      ],
      "metadata": {
        "id": "YDoyJ9JSsIAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c39d3a-4b95-428e-b2be-c6ecddcb3704"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ‰€æœ‰æ¨¡å‹çš„çµæœå·²å„²å­˜è‡³ï¼š /content/drive/MyDrive/Colab Notebooks/test/new\n"
          ]
        }
      ]
    }
  ]
}