{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxhGf1W5Lyvs7G5JVsFHY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# ÊéõËºâ Google Èõ≤Á´ØÁ°¨Á¢ü\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ë≥áÊñôÈõÜÊ†πÁõÆÈåÑ\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "\n",
        "# ÂêÑÂàÜÈ°ûË≥áÊñôÂ§æË®≠ÂÆö\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdeWQTkaMyj",
        "outputId": "45a95c5c-c573-4587-a822-d20c66c74231"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "t0wQZBWz5Uht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ÊéõËºâ Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}\n",
        "RESULT_DIR = \"./models/LSTM/result\"\n",
        "for sub in [\"accuracy_curves\", \"loss_curves\", \"confusion_matrices\",\n",
        "            \"precision_curves\", \"recall_curves\", \"f1_score_curves\"]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === LSTM Ê®°Âûã ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === Ë≥áÊñôËôïÁêÜ ===\n",
        "def process_file(file_path, label, max_seq_len):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Âè™‰øùÁïô voltage, current, power ‰∏âÊ¨Ñ\n",
        "    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "\n",
        "    # Áî® max_seq_len ÂàÜÊÆµÂàáÂâ≤Â∫èÂàó\n",
        "    num_chunks = len(data) // max_seq_len\n",
        "    chunks = [data[i * max_seq_len : (i + 1) * max_seq_len] for i in range(num_chunks)]\n",
        "\n",
        "    return chunks, [label] * len(chunks)\n",
        "\n",
        "def load_all_sequences(max_seq_len):\n",
        "    all_seq, all_labels = [], []\n",
        "    total_sequences = 0\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".csv\"):\n",
        "                path = os.path.join(folder, fname)\n",
        "                seqs, labels = process_file(path, label, max_seq_len)\n",
        "                all_seq.extend(seqs)\n",
        "                all_labels.extend(labels)\n",
        "                total_sequences += len(seqs)\n",
        "\n",
        "    seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "    labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "    B, T, F = seq_arr.shape\n",
        "    reshaped = seq_arr.reshape(-1, F)\n",
        "    scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "    print(f\"üìä Total sequences loaded: {total_sequences}\")\n",
        "\n",
        "    return scaled, labels_arr\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4):\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X, y = load_all_sequences(seq_len)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "                train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                test_loader = DataLoader(ChargeSequenceDataset(X_test, y_test), batch_size=bs)\n",
        "                model = LSTMClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                acc_list, loss_list = [], []\n",
        "                precision_list, recall_list, f1_list = [], [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    for xb, yb in train_loader:\n",
        "                        xb, yb = xb.to(device), yb.to(device)\n",
        "                        optimizer.zero_grad()\n",
        "                        out = model(xb)\n",
        "                        loss = criterion(out, yb)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # Evaluate\n",
        "                    model.eval()\n",
        "                    correct, total, total_loss = 0, 0, 0\n",
        "                    y_pred_epoch, y_true_epoch = [], []\n",
        "                    with torch.no_grad():\n",
        "                        for xb, yb in test_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct += (pred == yb).sum().item()\n",
        "                            total += yb.size(0)\n",
        "                            total_loss += loss.item() * xb.size(0)\n",
        "                            y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                            y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                    acc = correct / total\n",
        "                    avg_loss = total_loss / total\n",
        "                    acc_list.append(acc)\n",
        "                    loss_list.append(avg_loss)\n",
        "\n",
        "                    # ÊØèÂÄã epoch ÈÉΩË®àÁÆó precision/recall/f1\n",
        "                    precision = precision_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "                    recall = recall_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "                    f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "                    precision_list.append(precision)\n",
        "                    recall_list.append(recall)\n",
        "                    f1_list.append(f1)\n",
        "\n",
        "                    print(f\"Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                if acc_list[-1] >= 1.0:\n",
        "                    print(f\"‚ö†Ô∏è Skipped BS={bs} LR={lr} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                # === Accuracy Êõ≤Á∑öÂúñ ===\n",
        "                plt.plot(range(1, num_epochs+1), acc_list, marker='o')\n",
        "                plt.title(f\"Accuracy (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.ylim(0, 1.0); plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"accuracy_curves\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # === Loss Êõ≤Á∑öÂúñ ===\n",
        "                plt.plot(range(1, num_epochs+1), loss_list, marker='s', color='orange')\n",
        "                plt.title(f\"Loss (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"loss_curves\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # === Precision / Recall / F1-score Êõ≤Á∑öÂúñ ===\n",
        "                def plot_metric(metric_list, metric_name, color, marker):\n",
        "                    plt.plot(range(1, num_epochs+1), metric_list, marker=marker, color=color)\n",
        "                    plt.title(f\"{metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\"); plt.ylabel(metric_name.capitalize()); plt.ylim(0, 1.0); plt.grid(True)\n",
        "                    folder = f\"{metric_name}_curves\"\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric(precision_list, \"precision\", \"green\", \"o\")\n",
        "                plot_metric(recall_list, \"recall\", \"blue\", \"s\")\n",
        "                plot_metric(f1_list, \"f1_score\", \"purple\", \"^\")\n",
        "\n",
        "                # === Ê∑∑Ê∑ÜÁü©Èô£ÂúñÔºàÊúÄÂæå‰∏ÄÊ¨°Ê®°ÂûãÔºâ===\n",
        "                y_pred_final, y_true_final = [], []\n",
        "                with torch.no_grad():\n",
        "                    for xb, yb in test_loader:\n",
        "                        xb = xb.to(device)\n",
        "                        out = model(xb)\n",
        "                        pred = torch.argmax(out, dim=1).cpu().numpy()\n",
        "                        y_pred_final.extend(pred)\n",
        "                        y_true_final.extend(yb.numpy())\n",
        "\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true_final, y_pred_final), display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # === ÂÑ≤Â≠òÁµêÊûú ===\n",
        "                results.append({\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_list[-1], 'recall': recall_list[-1], 'f1_score': f1_list[-1],\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"lstm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results (all accuracy = 1.0 were skipped)\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "qepGDca0Yfpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb15f10-32e1-4186-ac51-816b4a197b78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWK53YBRYxJY",
        "outputId": "a0e6cc9e-666d-408c-93b7-cd7af51b57dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö18ÔºåÊØèÁµÑË®ìÁ∑¥ 100 epochs\n",
            "\n",
            "üß™ BS=4 | LR=0.001 | SEQ=4\n",
            "üìä Total sequences loaded: 10023\n",
            "Epoch 1/100 | Acc: 0.9820 | Loss: 0.0792\n",
            "Epoch 2/100 | Acc: 0.9860 | Loss: 0.0705\n",
            "Epoch 3/100 | Acc: 0.9845 | Loss: 0.0665\n",
            "Epoch 4/100 | Acc: 0.9860 | Loss: 0.0629\n",
            "Epoch 5/100 | Acc: 0.9860 | Loss: 0.0744\n",
            "Epoch 6/100 | Acc: 0.9392 | Loss: 0.1489\n",
            "Epoch 7/100 | Acc: 0.9865 | Loss: 0.0432\n",
            "Epoch 8/100 | Acc: 0.9890 | Loss: 0.0517\n",
            "Epoch 9/100 | Acc: 0.9885 | Loss: 0.0382\n",
            "Epoch 10/100 | Acc: 0.9880 | Loss: 0.0446\n",
            "Epoch 11/100 | Acc: 0.9880 | Loss: 0.0397\n",
            "Epoch 12/100 | Acc: 0.9910 | Loss: 0.0320\n",
            "Epoch 13/100 | Acc: 0.9915 | Loss: 0.0326\n",
            "Epoch 14/100 | Acc: 0.9920 | Loss: 0.0314\n",
            "Epoch 15/100 | Acc: 0.9910 | Loss: 0.0330\n",
            "Epoch 16/100 | Acc: 0.9890 | Loss: 0.0368\n",
            "Epoch 17/100 | Acc: 0.9885 | Loss: 0.0432\n",
            "Epoch 18/100 | Acc: 0.9925 | Loss: 0.0300\n",
            "Epoch 19/100 | Acc: 0.9930 | Loss: 0.0318\n",
            "Epoch 20/100 | Acc: 0.9920 | Loss: 0.0309\n",
            "Epoch 21/100 | Acc: 0.9925 | Loss: 0.0286\n",
            "Epoch 22/100 | Acc: 0.9930 | Loss: 0.0249\n",
            "Epoch 23/100 | Acc: 0.9910 | Loss: 0.0294\n",
            "Epoch 24/100 | Acc: 0.9910 | Loss: 0.0331\n",
            "Epoch 25/100 | Acc: 0.9925 | Loss: 0.0268\n",
            "Epoch 26/100 | Acc: 0.9945 | Loss: 0.0255\n",
            "Epoch 27/100 | Acc: 0.9930 | Loss: 0.0274\n",
            "Epoch 28/100 | Acc: 0.9880 | Loss: 0.0373\n",
            "Epoch 29/100 | Acc: 0.9895 | Loss: 0.0342\n",
            "Epoch 30/100 | Acc: 0.9920 | Loss: 0.0310\n",
            "Epoch 31/100 | Acc: 0.9920 | Loss: 0.0322\n",
            "Epoch 32/100 | Acc: 0.9935 | Loss: 0.0242\n",
            "Epoch 33/100 | Acc: 0.9940 | Loss: 0.0240\n",
            "Epoch 34/100 | Acc: 0.9940 | Loss: 0.0297\n",
            "Epoch 35/100 | Acc: 0.9930 | Loss: 0.0270\n",
            "Epoch 36/100 | Acc: 0.9940 | Loss: 0.0229\n",
            "Epoch 37/100 | Acc: 0.9940 | Loss: 0.0257\n",
            "Epoch 38/100 | Acc: 0.9915 | Loss: 0.0262\n",
            "Epoch 39/100 | Acc: 0.9945 | Loss: 0.0235\n",
            "Epoch 40/100 | Acc: 0.9930 | Loss: 0.0257\n",
            "Epoch 41/100 | Acc: 0.9895 | Loss: 0.0323\n",
            "Epoch 42/100 | Acc: 0.9930 | Loss: 0.0235\n",
            "Epoch 43/100 | Acc: 0.9925 | Loss: 0.0272\n",
            "Epoch 44/100 | Acc: 0.9940 | Loss: 0.0266\n",
            "Epoch 45/100 | Acc: 0.9930 | Loss: 0.0286\n",
            "Epoch 46/100 | Acc: 0.9885 | Loss: 0.0363\n",
            "Epoch 47/100 | Acc: 0.9930 | Loss: 0.0291\n",
            "Epoch 48/100 | Acc: 0.9945 | Loss: 0.0245\n",
            "Epoch 49/100 | Acc: 0.9930 | Loss: 0.0248\n",
            "Epoch 50/100 | Acc: 0.9915 | Loss: 0.0271\n",
            "Epoch 51/100 | Acc: 0.9900 | Loss: 0.0327\n",
            "Epoch 52/100 | Acc: 0.9930 | Loss: 0.0261\n",
            "Epoch 53/100 | Acc: 0.9920 | Loss: 0.0301\n",
            "Epoch 54/100 | Acc: 0.9925 | Loss: 0.0286\n",
            "Epoch 55/100 | Acc: 0.9920 | Loss: 0.0276\n",
            "Epoch 56/100 | Acc: 0.9935 | Loss: 0.0267\n",
            "Epoch 57/100 | Acc: 0.9940 | Loss: 0.0273\n",
            "Epoch 58/100 | Acc: 0.9905 | Loss: 0.0314\n",
            "Epoch 59/100 | Acc: 0.9925 | Loss: 0.0298\n",
            "Epoch 60/100 | Acc: 0.9930 | Loss: 0.0292\n",
            "Epoch 61/100 | Acc: 0.9920 | Loss: 0.0282\n",
            "Epoch 62/100 | Acc: 0.9935 | Loss: 0.0262\n",
            "Epoch 63/100 | Acc: 0.9920 | Loss: 0.0277\n",
            "Epoch 64/100 | Acc: 0.9915 | Loss: 0.0309\n",
            "Epoch 65/100 | Acc: 0.9940 | Loss: 0.0284\n",
            "Epoch 66/100 | Acc: 0.9935 | Loss: 0.0290\n",
            "Epoch 67/100 | Acc: 0.9920 | Loss: 0.0303\n",
            "Epoch 68/100 | Acc: 0.9935 | Loss: 0.0299\n",
            "Epoch 69/100 | Acc: 0.9900 | Loss: 0.0357\n",
            "Epoch 70/100 | Acc: 0.9930 | Loss: 0.0300\n",
            "Epoch 71/100 | Acc: 0.9940 | Loss: 0.0273\n",
            "Epoch 72/100 | Acc: 0.9925 | Loss: 0.0297\n",
            "Epoch 73/100 | Acc: 0.9905 | Loss: 0.0353\n",
            "Epoch 74/100 | Acc: 0.9915 | Loss: 0.0333\n",
            "Epoch 75/100 | Acc: 0.9915 | Loss: 0.0336\n",
            "Epoch 76/100 | Acc: 0.9905 | Loss: 0.0331\n",
            "Epoch 77/100 | Acc: 0.9925 | Loss: 0.0293\n",
            "Epoch 78/100 | Acc: 0.9935 | Loss: 0.0296\n",
            "Epoch 79/100 | Acc: 0.9925 | Loss: 0.0308\n",
            "Epoch 80/100 | Acc: 0.9930 | Loss: 0.0266\n",
            "Epoch 81/100 | Acc: 0.9930 | Loss: 0.0298\n",
            "Epoch 82/100 | Acc: 0.9945 | Loss: 0.0287\n",
            "Epoch 83/100 | Acc: 0.9940 | Loss: 0.0316\n",
            "Epoch 84/100 | Acc: 0.9925 | Loss: 0.0293\n",
            "Epoch 85/100 | Acc: 0.9930 | Loss: 0.0310\n",
            "Epoch 86/100 | Acc: 0.9880 | Loss: 0.0384\n",
            "Epoch 87/100 | Acc: 0.9925 | Loss: 0.0300\n",
            "Epoch 88/100 | Acc: 0.9915 | Loss: 0.0383\n",
            "Epoch 89/100 | Acc: 0.9910 | Loss: 0.0348\n",
            "Epoch 90/100 | Acc: 0.9915 | Loss: 0.0329\n",
            "Epoch 91/100 | Acc: 0.9930 | Loss: 0.0312\n",
            "Epoch 92/100 | Acc: 0.9920 | Loss: 0.0327\n",
            "Epoch 93/100 | Acc: 0.9910 | Loss: 0.0342\n",
            "Epoch 94/100 | Acc: 0.9920 | Loss: 0.0370\n",
            "Epoch 95/100 | Acc: 0.9930 | Loss: 0.0288\n",
            "Epoch 96/100 | Acc: 0.9925 | Loss: 0.0291\n",
            "Epoch 97/100 | Acc: 0.9940 | Loss: 0.0301\n",
            "Epoch 98/100 | Acc: 0.9895 | Loss: 0.0505\n",
            "Epoch 99/100 | Acc: 0.9915 | Loss: 0.0337\n",
            "Epoch 100/100 | Acc: 0.9930 | Loss: 0.0325\n",
            "\n",
            "üß™ BS=4 | LR=0.001 | SEQ=8\n",
            "üìä Total sequences loaded: 5001\n",
            "Epoch 1/100 | Acc: 0.9900 | Loss: 0.0644\n",
            "Epoch 2/100 | Acc: 0.9780 | Loss: 0.0859\n",
            "Epoch 3/100 | Acc: 0.9930 | Loss: 0.0462\n",
            "Epoch 4/100 | Acc: 0.9930 | Loss: 0.0411\n",
            "Epoch 5/100 | Acc: 0.9930 | Loss: 0.0411\n",
            "Epoch 6/100 | Acc: 0.9910 | Loss: 0.0459\n",
            "Epoch 7/100 | Acc: 0.9930 | Loss: 0.0369\n",
            "Epoch 8/100 | Acc: 0.9880 | Loss: 0.0483\n",
            "Epoch 9/100 | Acc: 0.9930 | Loss: 0.0355\n",
            "Epoch 10/100 | Acc: 0.9920 | Loss: 0.0682\n",
            "Epoch 11/100 | Acc: 0.9930 | Loss: 0.0389\n",
            "Epoch 12/100 | Acc: 0.9930 | Loss: 0.0283\n",
            "Epoch 13/100 | Acc: 0.9930 | Loss: 0.0274\n",
            "Epoch 14/100 | Acc: 0.9930 | Loss: 0.0393\n",
            "Epoch 15/100 | Acc: 0.9930 | Loss: 0.0278\n",
            "Epoch 16/100 | Acc: 0.9431 | Loss: 0.1207\n",
            "Epoch 17/100 | Acc: 0.9930 | Loss: 0.0302\n",
            "Epoch 18/100 | Acc: 0.9940 | Loss: 0.0242\n",
            "Epoch 19/100 | Acc: 0.9940 | Loss: 0.0211\n",
            "Epoch 20/100 | Acc: 0.9950 | Loss: 0.0196\n",
            "Epoch 21/100 | Acc: 0.9950 | Loss: 0.0221\n",
            "Epoch 22/100 | Acc: 0.9930 | Loss: 0.0269\n",
            "Epoch 23/100 | Acc: 0.9940 | Loss: 0.0140\n",
            "Epoch 24/100 | Acc: 0.9940 | Loss: 0.0307\n",
            "Epoch 25/100 | Acc: 0.9840 | Loss: 0.0411\n",
            "Epoch 26/100 | Acc: 0.9940 | Loss: 0.0192\n",
            "Epoch 27/100 | Acc: 0.9940 | Loss: 0.0180\n",
            "Epoch 28/100 | Acc: 0.9940 | Loss: 0.0152\n",
            "Epoch 29/100 | Acc: 0.9940 | Loss: 0.0157\n",
            "Epoch 30/100 | Acc: 0.9940 | Loss: 0.0132\n",
            "Epoch 31/100 | Acc: 0.9930 | Loss: 0.0249\n",
            "Epoch 32/100 | Acc: 0.9940 | Loss: 0.0127\n",
            "Epoch 33/100 | Acc: 0.9940 | Loss: 0.0166\n",
            "Epoch 34/100 | Acc: 0.9930 | Loss: 0.0219\n",
            "Epoch 35/100 | Acc: 0.9960 | Loss: 0.0145\n",
            "Epoch 36/100 | Acc: 0.9930 | Loss: 0.0126\n",
            "Epoch 37/100 | Acc: 0.9930 | Loss: 0.0250\n",
            "Epoch 38/100 | Acc: 0.9940 | Loss: 0.0136\n",
            "Epoch 39/100 | Acc: 0.9950 | Loss: 0.0166\n",
            "Epoch 40/100 | Acc: 0.9960 | Loss: 0.0142\n",
            "Epoch 41/100 | Acc: 0.9950 | Loss: 0.0141\n",
            "Epoch 42/100 | Acc: 0.9950 | Loss: 0.0135\n",
            "Epoch 43/100 | Acc: 0.9940 | Loss: 0.0240\n",
            "Epoch 44/100 | Acc: 0.9950 | Loss: 0.0167\n",
            "Epoch 45/100 | Acc: 0.9950 | Loss: 0.0187\n",
            "Epoch 46/100 | Acc: 0.9960 | Loss: 0.0146\n",
            "Epoch 47/100 | Acc: 0.9950 | Loss: 0.0130\n",
            "Epoch 48/100 | Acc: 0.9940 | Loss: 0.0262\n",
            "Epoch 49/100 | Acc: 0.9960 | Loss: 0.0094\n",
            "Epoch 50/100 | Acc: 0.9960 | Loss: 0.0136\n",
            "Epoch 51/100 | Acc: 0.9950 | Loss: 0.0199\n",
            "Epoch 52/100 | Acc: 0.9940 | Loss: 0.0186\n",
            "Epoch 53/100 | Acc: 0.9930 | Loss: 0.0159\n",
            "Epoch 54/100 | Acc: 0.9940 | Loss: 0.0133\n",
            "Epoch 55/100 | Acc: 0.9950 | Loss: 0.0122\n",
            "Epoch 56/100 | Acc: 0.9940 | Loss: 0.0204\n",
            "Epoch 57/100 | Acc: 0.9940 | Loss: 0.0264\n",
            "Epoch 58/100 | Acc: 0.9930 | Loss: 0.0241\n",
            "Epoch 59/100 | Acc: 0.9950 | Loss: 0.0143\n",
            "Epoch 60/100 | Acc: 0.9950 | Loss: 0.0183\n",
            "Epoch 61/100 | Acc: 0.9930 | Loss: 0.0225\n",
            "Epoch 62/100 | Acc: 0.9950 | Loss: 0.0287\n",
            "Epoch 63/100 | Acc: 0.9940 | Loss: 0.0175\n",
            "Epoch 64/100 | Acc: 0.9940 | Loss: 0.0185\n",
            "Epoch 65/100 | Acc: 0.9940 | Loss: 0.0230\n",
            "Epoch 66/100 | Acc: 0.9970 | Loss: 0.0089\n",
            "Epoch 67/100 | Acc: 0.9960 | Loss: 0.0126\n",
            "Epoch 68/100 | Acc: 0.9960 | Loss: 0.0100\n",
            "Epoch 69/100 | Acc: 0.9970 | Loss: 0.0111\n",
            "Epoch 70/100 | Acc: 0.9940 | Loss: 0.0199\n",
            "Epoch 71/100 | Acc: 0.9970 | Loss: 0.0145\n",
            "Epoch 72/100 | Acc: 0.9960 | Loss: 0.0149\n",
            "Epoch 73/100 | Acc: 0.9960 | Loss: 0.0143\n",
            "Epoch 74/100 | Acc: 0.9960 | Loss: 0.0130\n",
            "Epoch 75/100 | Acc: 0.9960 | Loss: 0.0127\n",
            "Epoch 76/100 | Acc: 0.9960 | Loss: 0.0183\n",
            "Epoch 77/100 | Acc: 0.9950 | Loss: 0.0211\n",
            "Epoch 78/100 | Acc: 0.9940 | Loss: 0.0326\n",
            "Epoch 79/100 | Acc: 0.9950 | Loss: 0.0142\n",
            "Epoch 80/100 | Acc: 0.9950 | Loss: 0.0189\n",
            "Epoch 81/100 | Acc: 0.9970 | Loss: 0.0160\n",
            "Epoch 82/100 | Acc: 0.9900 | Loss: 0.0266\n",
            "Epoch 83/100 | Acc: 0.9970 | Loss: 0.0159\n",
            "Epoch 84/100 | Acc: 0.9950 | Loss: 0.0195\n",
            "Epoch 85/100 | Acc: 0.9960 | Loss: 0.0108\n",
            "Epoch 86/100 | Acc: 0.9950 | Loss: 0.0431\n",
            "Epoch 87/100 | Acc: 0.9940 | Loss: 0.0138\n",
            "Epoch 88/100 | Acc: 0.9940 | Loss: 0.0175\n",
            "Epoch 89/100 | Acc: 0.9950 | Loss: 0.0124\n",
            "Epoch 90/100 | Acc: 0.9960 | Loss: 0.0111\n",
            "Epoch 91/100 | Acc: 0.9970 | Loss: 0.0149\n",
            "Epoch 92/100 | Acc: 0.9970 | Loss: 0.0133\n",
            "Epoch 93/100 | Acc: 0.9960 | Loss: 0.0117\n",
            "Epoch 94/100 | Acc: 0.9950 | Loss: 0.0092\n",
            "Epoch 95/100 | Acc: 0.9920 | Loss: 0.0307\n",
            "Epoch 96/100 | Acc: 0.9950 | Loss: 0.0122\n",
            "Epoch 97/100 | Acc: 0.9980 | Loss: 0.0135\n",
            "Epoch 98/100 | Acc: 0.9950 | Loss: 0.0139\n",
            "Epoch 99/100 | Acc: 0.9930 | Loss: 0.0259\n",
            "Epoch 100/100 | Acc: 0.9940 | Loss: 0.0169\n",
            "\n",
            "üß™ BS=4 | LR=0.001 | SEQ=10\n",
            "üìä Total sequences loaded: 4010\n",
            "Epoch 1/100 | Acc: 0.9888 | Loss: 0.0819\n",
            "Epoch 2/100 | Acc: 0.9863 | Loss: 0.0634\n",
            "Epoch 3/100 | Acc: 0.9863 | Loss: 0.0687\n",
            "Epoch 4/100 | Acc: 0.9888 | Loss: 0.0658\n",
            "Epoch 5/100 | Acc: 0.9900 | Loss: 0.0602\n",
            "Epoch 6/100 | Acc: 0.9875 | Loss: 0.0716\n",
            "Epoch 7/100 | Acc: 0.9900 | Loss: 0.0669\n",
            "Epoch 8/100 | Acc: 0.9900 | Loss: 0.0651\n",
            "Epoch 9/100 | Acc: 0.9875 | Loss: 0.0705\n",
            "Epoch 10/100 | Acc: 0.9850 | Loss: 0.1238\n",
            "Epoch 11/100 | Acc: 0.9900 | Loss: 0.0640\n",
            "Epoch 12/100 | Acc: 0.9900 | Loss: 0.0628\n",
            "Epoch 13/100 | Acc: 0.9888 | Loss: 0.0719\n",
            "Epoch 14/100 | Acc: 0.9900 | Loss: 0.0689\n",
            "Epoch 15/100 | Acc: 0.9825 | Loss: 0.0748\n",
            "Epoch 16/100 | Acc: 0.9900 | Loss: 0.0595\n",
            "Epoch 17/100 | Acc: 0.9900 | Loss: 0.0583\n",
            "Epoch 18/100 | Acc: 0.9913 | Loss: 0.0601\n",
            "Epoch 19/100 | Acc: 0.9888 | Loss: 0.0571\n",
            "Epoch 20/100 | Acc: 0.9900 | Loss: 0.0486\n",
            "Epoch 21/100 | Acc: 0.9900 | Loss: 0.0533\n",
            "Epoch 22/100 | Acc: 0.9913 | Loss: 0.0458\n",
            "Epoch 23/100 | Acc: 0.9913 | Loss: 0.0415\n",
            "Epoch 24/100 | Acc: 0.9888 | Loss: 0.0509\n",
            "Epoch 25/100 | Acc: 0.9913 | Loss: 0.0447\n",
            "Epoch 26/100 | Acc: 0.9913 | Loss: 0.0421\n",
            "Epoch 27/100 | Acc: 0.9913 | Loss: 0.0458\n",
            "Epoch 28/100 | Acc: 0.9900 | Loss: 0.0494\n",
            "Epoch 29/100 | Acc: 0.9913 | Loss: 0.0452\n",
            "Epoch 30/100 | Acc: 0.9913 | Loss: 0.0426\n",
            "Epoch 31/100 | Acc: 0.9913 | Loss: 0.0426\n",
            "Epoch 32/100 | Acc: 0.9888 | Loss: 0.0593\n",
            "Epoch 33/100 | Acc: 0.9938 | Loss: 0.0441\n",
            "Epoch 34/100 | Acc: 0.9925 | Loss: 0.0373\n",
            "Epoch 35/100 | Acc: 0.9900 | Loss: 0.0424\n",
            "Epoch 36/100 | Acc: 0.9925 | Loss: 0.0428\n",
            "Epoch 37/100 | Acc: 0.9888 | Loss: 0.0627\n",
            "Epoch 38/100 | Acc: 0.9875 | Loss: 0.0472\n",
            "Epoch 39/100 | Acc: 0.9900 | Loss: 0.0523\n",
            "Epoch 40/100 | Acc: 0.9888 | Loss: 0.0419\n",
            "Epoch 41/100 | Acc: 0.9900 | Loss: 0.0483\n",
            "Epoch 42/100 | Acc: 0.9900 | Loss: 0.0422\n",
            "Epoch 43/100 | Acc: 0.9900 | Loss: 0.0457\n",
            "Epoch 44/100 | Acc: 0.9900 | Loss: 0.0362\n",
            "Epoch 45/100 | Acc: 0.9925 | Loss: 0.0341\n",
            "Epoch 46/100 | Acc: 0.9925 | Loss: 0.0456\n",
            "Epoch 47/100 | Acc: 0.9913 | Loss: 0.0382\n",
            "Epoch 48/100 | Acc: 0.9913 | Loss: 0.0439\n",
            "Epoch 49/100 | Acc: 0.9913 | Loss: 0.0371\n",
            "Epoch 50/100 | Acc: 0.9913 | Loss: 0.0432\n",
            "Epoch 51/100 | Acc: 0.9913 | Loss: 0.0359\n",
            "Epoch 52/100 | Acc: 0.9913 | Loss: 0.0405\n",
            "Epoch 53/100 | Acc: 0.9888 | Loss: 0.0534\n",
            "Epoch 54/100 | Acc: 0.9875 | Loss: 0.0508\n",
            "Epoch 55/100 | Acc: 0.9925 | Loss: 0.0358\n",
            "Epoch 56/100 | Acc: 0.9900 | Loss: 0.0346\n",
            "Epoch 57/100 | Acc: 0.9925 | Loss: 0.0307\n",
            "Epoch 58/100 | Acc: 0.9900 | Loss: 0.0322\n",
            "Epoch 59/100 | Acc: 0.9925 | Loss: 0.0261\n",
            "Epoch 60/100 | Acc: 0.9900 | Loss: 0.0371\n",
            "Epoch 61/100 | Acc: 0.9913 | Loss: 0.0399\n",
            "Epoch 62/100 | Acc: 0.9900 | Loss: 0.0396\n",
            "Epoch 63/100 | Acc: 0.9913 | Loss: 0.0375\n",
            "Epoch 64/100 | Acc: 0.9913 | Loss: 0.0497\n",
            "Epoch 65/100 | Acc: 0.9913 | Loss: 0.0264\n",
            "Epoch 66/100 | Acc: 0.9925 | Loss: 0.0422\n",
            "Epoch 67/100 | Acc: 0.9913 | Loss: 0.0367\n",
            "Epoch 68/100 | Acc: 0.9913 | Loss: 0.0425\n",
            "Epoch 69/100 | Acc: 0.9900 | Loss: 0.0419\n",
            "Epoch 70/100 | Acc: 0.9925 | Loss: 0.0291\n",
            "Epoch 71/100 | Acc: 0.9900 | Loss: 0.0246\n",
            "Epoch 72/100 | Acc: 0.9913 | Loss: 0.0386\n",
            "Epoch 73/100 | Acc: 0.9913 | Loss: 0.0353\n",
            "Epoch 74/100 | Acc: 0.9913 | Loss: 0.0389\n",
            "Epoch 75/100 | Acc: 0.9938 | Loss: 0.0345\n",
            "Epoch 76/100 | Acc: 0.9913 | Loss: 0.0312\n",
            "Epoch 77/100 | Acc: 0.9925 | Loss: 0.0263\n",
            "Epoch 78/100 | Acc: 0.9913 | Loss: 0.0291\n",
            "Epoch 79/100 | Acc: 0.9900 | Loss: 0.0387\n",
            "Epoch 80/100 | Acc: 0.9913 | Loss: 0.0342\n",
            "Epoch 81/100 | Acc: 0.9501 | Loss: 0.1808\n",
            "Epoch 82/100 | Acc: 0.9913 | Loss: 0.0479\n",
            "Epoch 83/100 | Acc: 0.9913 | Loss: 0.0448\n",
            "Epoch 84/100 | Acc: 0.9913 | Loss: 0.0411\n",
            "Epoch 85/100 | Acc: 0.9925 | Loss: 0.0380\n",
            "Epoch 86/100 | Acc: 0.9913 | Loss: 0.0400\n",
            "Epoch 87/100 | Acc: 0.9888 | Loss: 0.0510\n",
            "Epoch 88/100 | Acc: 0.9900 | Loss: 0.0467\n",
            "Epoch 89/100 | Acc: 0.9913 | Loss: 0.0434\n",
            "Epoch 90/100 | Acc: 0.9913 | Loss: 0.0352\n",
            "Epoch 91/100 | Acc: 0.9913 | Loss: 0.0368\n",
            "Epoch 92/100 | Acc: 0.9925 | Loss: 0.0388\n",
            "Epoch 93/100 | Acc: 0.9900 | Loss: 0.0275\n",
            "Epoch 94/100 | Acc: 0.9900 | Loss: 0.0410\n",
            "Epoch 95/100 | Acc: 0.9913 | Loss: 0.0346\n",
            "Epoch 96/100 | Acc: 0.9913 | Loss: 0.0380\n",
            "Epoch 97/100 | Acc: 0.9900 | Loss: 0.0382\n",
            "Epoch 98/100 | Acc: 0.9888 | Loss: 0.0402\n",
            "Epoch 99/100 | Acc: 0.9913 | Loss: 0.0257\n",
            "Epoch 100/100 | Acc: 0.9925 | Loss: 0.0253\n",
            "\n",
            "üß™ BS=4 | LR=0.0001 | SEQ=4\n",
            "üìä Total sequences loaded: 10023\n",
            "Epoch 1/100 | Acc: 0.8564 | Loss: 0.4228\n",
            "Epoch 2/100 | Acc: 0.9696 | Loss: 0.1607\n",
            "Epoch 3/100 | Acc: 0.9756 | Loss: 0.1203\n",
            "Epoch 4/100 | Acc: 0.9800 | Loss: 0.1039\n",
            "Epoch 5/100 | Acc: 0.9805 | Loss: 0.0989\n",
            "Epoch 6/100 | Acc: 0.9835 | Loss: 0.0869\n",
            "Epoch 7/100 | Acc: 0.9835 | Loss: 0.0843\n",
            "Epoch 8/100 | Acc: 0.9825 | Loss: 0.0792\n",
            "Epoch 9/100 | Acc: 0.9850 | Loss: 0.0746\n",
            "Epoch 10/100 | Acc: 0.9865 | Loss: 0.0730\n",
            "Epoch 11/100 | Acc: 0.9870 | Loss: 0.0692\n",
            "Epoch 12/100 | Acc: 0.9870 | Loss: 0.0684\n",
            "Epoch 13/100 | Acc: 0.9840 | Loss: 0.0701\n",
            "Epoch 14/100 | Acc: 0.9865 | Loss: 0.0660\n",
            "Epoch 15/100 | Acc: 0.9865 | Loss: 0.0644\n",
            "Epoch 16/100 | Acc: 0.9870 | Loss: 0.0623\n",
            "Epoch 17/100 | Acc: 0.9875 | Loss: 0.0616\n",
            "Epoch 18/100 | Acc: 0.9880 | Loss: 0.0592\n",
            "Epoch 19/100 | Acc: 0.9875 | Loss: 0.0598\n",
            "Epoch 20/100 | Acc: 0.9865 | Loss: 0.0604\n",
            "Epoch 21/100 | Acc: 0.9885 | Loss: 0.0570\n",
            "Epoch 22/100 | Acc: 0.9875 | Loss: 0.0573\n",
            "Epoch 23/100 | Acc: 0.9880 | Loss: 0.0557\n",
            "Epoch 24/100 | Acc: 0.9870 | Loss: 0.0563\n",
            "Epoch 25/100 | Acc: 0.9860 | Loss: 0.0545\n",
            "Epoch 26/100 | Acc: 0.9890 | Loss: 0.0526\n",
            "Epoch 27/100 | Acc: 0.9890 | Loss: 0.0532\n",
            "Epoch 28/100 | Acc: 0.9880 | Loss: 0.0523\n",
            "Epoch 29/100 | Acc: 0.9880 | Loss: 0.0515\n",
            "Epoch 30/100 | Acc: 0.9880 | Loss: 0.0508\n",
            "Epoch 31/100 | Acc: 0.9880 | Loss: 0.0492\n",
            "Epoch 32/100 | Acc: 0.9870 | Loss: 0.0504\n",
            "Epoch 33/100 | Acc: 0.9885 | Loss: 0.0490\n",
            "Epoch 34/100 | Acc: 0.9885 | Loss: 0.0480\n",
            "Epoch 35/100 | Acc: 0.9885 | Loss: 0.0469\n",
            "Epoch 36/100 | Acc: 0.9885 | Loss: 0.0490\n",
            "Epoch 37/100 | Acc: 0.9885 | Loss: 0.0463\n",
            "Epoch 38/100 | Acc: 0.9885 | Loss: 0.0460\n",
            "Epoch 39/100 | Acc: 0.9875 | Loss: 0.0457\n",
            "Epoch 40/100 | Acc: 0.9885 | Loss: 0.0479\n",
            "Epoch 41/100 | Acc: 0.9875 | Loss: 0.0447\n",
            "Epoch 42/100 | Acc: 0.9880 | Loss: 0.0450\n",
            "Epoch 43/100 | Acc: 0.9890 | Loss: 0.0445\n",
            "Epoch 44/100 | Acc: 0.9880 | Loss: 0.0436\n",
            "Epoch 45/100 | Acc: 0.9885 | Loss: 0.0432\n",
            "Epoch 46/100 | Acc: 0.9885 | Loss: 0.0433\n",
            "Epoch 47/100 | Acc: 0.9885 | Loss: 0.0439\n",
            "Epoch 48/100 | Acc: 0.9900 | Loss: 0.0436\n",
            "Epoch 49/100 | Acc: 0.9900 | Loss: 0.0431\n",
            "Epoch 50/100 | Acc: 0.9900 | Loss: 0.0423\n",
            "Epoch 51/100 | Acc: 0.9905 | Loss: 0.0429\n",
            "Epoch 52/100 | Acc: 0.9905 | Loss: 0.0410\n",
            "Epoch 53/100 | Acc: 0.9905 | Loss: 0.0407\n",
            "Epoch 54/100 | Acc: 0.9905 | Loss: 0.0424\n",
            "Epoch 55/100 | Acc: 0.9915 | Loss: 0.0412\n",
            "Epoch 56/100 | Acc: 0.9915 | Loss: 0.0410\n",
            "Epoch 57/100 | Acc: 0.9925 | Loss: 0.0403\n",
            "Epoch 58/100 | Acc: 0.9900 | Loss: 0.0404\n",
            "Epoch 59/100 | Acc: 0.9905 | Loss: 0.0404\n",
            "Epoch 60/100 | Acc: 0.9900 | Loss: 0.0409\n",
            "Epoch 61/100 | Acc: 0.9910 | Loss: 0.0403\n",
            "Epoch 62/100 | Acc: 0.9915 | Loss: 0.0412\n",
            "Epoch 63/100 | Acc: 0.9890 | Loss: 0.0425\n",
            "Epoch 64/100 | Acc: 0.9910 | Loss: 0.0386\n",
            "Epoch 65/100 | Acc: 0.9910 | Loss: 0.0390\n",
            "Epoch 66/100 | Acc: 0.9915 | Loss: 0.0401\n",
            "Epoch 67/100 | Acc: 0.9910 | Loss: 0.0383\n",
            "Epoch 68/100 | Acc: 0.9900 | Loss: 0.0395\n",
            "Epoch 69/100 | Acc: 0.9915 | Loss: 0.0382\n",
            "Epoch 70/100 | Acc: 0.9920 | Loss: 0.0383\n",
            "Epoch 71/100 | Acc: 0.9915 | Loss: 0.0381\n",
            "Epoch 72/100 | Acc: 0.9905 | Loss: 0.0394\n",
            "Epoch 73/100 | Acc: 0.9925 | Loss: 0.0372\n",
            "Epoch 74/100 | Acc: 0.9900 | Loss: 0.0391\n",
            "Epoch 75/100 | Acc: 0.9920 | Loss: 0.0366\n",
            "Epoch 76/100 | Acc: 0.9925 | Loss: 0.0380\n",
            "Epoch 77/100 | Acc: 0.9920 | Loss: 0.0382\n",
            "Epoch 78/100 | Acc: 0.9920 | Loss: 0.0369\n",
            "Epoch 79/100 | Acc: 0.9885 | Loss: 0.0429\n",
            "Epoch 80/100 | Acc: 0.9915 | Loss: 0.0376\n",
            "Epoch 81/100 | Acc: 0.9920 | Loss: 0.0369\n",
            "Epoch 82/100 | Acc: 0.9930 | Loss: 0.0360\n",
            "Epoch 83/100 | Acc: 0.9925 | Loss: 0.0357\n",
            "Epoch 84/100 | Acc: 0.9925 | Loss: 0.0370\n",
            "Epoch 85/100 | Acc: 0.9920 | Loss: 0.0353\n",
            "Epoch 86/100 | Acc: 0.9920 | Loss: 0.0358\n",
            "Epoch 87/100 | Acc: 0.9925 | Loss: 0.0348\n",
            "Epoch 88/100 | Acc: 0.9915 | Loss: 0.0362\n",
            "Epoch 89/100 | Acc: 0.9925 | Loss: 0.0343\n",
            "Epoch 90/100 | Acc: 0.9910 | Loss: 0.0365\n",
            "Epoch 91/100 | Acc: 0.9920 | Loss: 0.0363\n",
            "Epoch 92/100 | Acc: 0.9920 | Loss: 0.0350\n",
            "Epoch 93/100 | Acc: 0.9915 | Loss: 0.0344\n",
            "Epoch 94/100 | Acc: 0.9925 | Loss: 0.0340\n",
            "Epoch 95/100 | Acc: 0.9930 | Loss: 0.0336\n",
            "Epoch 96/100 | Acc: 0.9880 | Loss: 0.0377\n",
            "Epoch 97/100 | Acc: 0.9920 | Loss: 0.0337\n",
            "Epoch 98/100 | Acc: 0.9925 | Loss: 0.0322\n",
            "Epoch 99/100 | Acc: 0.9925 | Loss: 0.0324\n",
            "Epoch 100/100 | Acc: 0.9930 | Loss: 0.0330\n",
            "\n",
            "üß™ BS=4 | LR=0.0001 | SEQ=8\n",
            "üìä Total sequences loaded: 5001\n",
            "Epoch 1/100 | Acc: 0.7882 | Loss: 0.6012\n",
            "Epoch 2/100 | Acc: 0.9461 | Loss: 0.2106\n",
            "Epoch 3/100 | Acc: 0.9530 | Loss: 0.1307\n",
            "Epoch 4/100 | Acc: 0.9800 | Loss: 0.1005\n",
            "Epoch 5/100 | Acc: 0.9860 | Loss: 0.0749\n",
            "Epoch 6/100 | Acc: 0.9920 | Loss: 0.0641\n",
            "Epoch 7/100 | Acc: 0.9920 | Loss: 0.0570\n",
            "Epoch 8/100 | Acc: 0.9920 | Loss: 0.0549\n",
            "Epoch 9/100 | Acc: 0.9920 | Loss: 0.0534\n",
            "Epoch 10/100 | Acc: 0.9920 | Loss: 0.0496\n",
            "Epoch 11/100 | Acc: 0.9920 | Loss: 0.0479\n",
            "Epoch 12/100 | Acc: 0.9930 | Loss: 0.0474\n",
            "Epoch 13/100 | Acc: 0.9920 | Loss: 0.0490\n",
            "Epoch 14/100 | Acc: 0.9930 | Loss: 0.0441\n",
            "Epoch 15/100 | Acc: 0.9930 | Loss: 0.0443\n",
            "Epoch 16/100 | Acc: 0.9930 | Loss: 0.0436\n",
            "Epoch 17/100 | Acc: 0.9920 | Loss: 0.0446\n",
            "Epoch 18/100 | Acc: 0.9930 | Loss: 0.0439\n",
            "Epoch 19/100 | Acc: 0.9920 | Loss: 0.0441\n",
            "Epoch 20/100 | Acc: 0.9920 | Loss: 0.0434\n",
            "Epoch 21/100 | Acc: 0.9900 | Loss: 0.0461\n",
            "Epoch 22/100 | Acc: 0.9920 | Loss: 0.0419\n",
            "Epoch 23/100 | Acc: 0.9930 | Loss: 0.0428\n",
            "Epoch 24/100 | Acc: 0.9930 | Loss: 0.0391\n",
            "Epoch 25/100 | Acc: 0.9930 | Loss: 0.0392\n",
            "Epoch 26/100 | Acc: 0.9930 | Loss: 0.0382\n",
            "Epoch 27/100 | Acc: 0.9920 | Loss: 0.0410\n",
            "Epoch 28/100 | Acc: 0.9930 | Loss: 0.0392\n",
            "Epoch 29/100 | Acc: 0.9920 | Loss: 0.0393\n",
            "Epoch 30/100 | Acc: 0.9930 | Loss: 0.0378\n",
            "Epoch 31/100 | Acc: 0.9930 | Loss: 0.0349\n",
            "Epoch 32/100 | Acc: 0.9930 | Loss: 0.0337\n",
            "Epoch 33/100 | Acc: 0.9930 | Loss: 0.0353\n",
            "Epoch 34/100 | Acc: 0.9930 | Loss: 0.0327\n",
            "Epoch 35/100 | Acc: 0.9930 | Loss: 0.0326\n",
            "Epoch 36/100 | Acc: 0.9930 | Loss: 0.0322\n",
            "Epoch 37/100 | Acc: 0.9930 | Loss: 0.0332\n",
            "Epoch 38/100 | Acc: 0.9930 | Loss: 0.0349\n",
            "Epoch 39/100 | Acc: 0.9930 | Loss: 0.0321\n",
            "Epoch 40/100 | Acc: 0.9930 | Loss: 0.0301\n",
            "Epoch 41/100 | Acc: 0.9930 | Loss: 0.0285\n",
            "Epoch 42/100 | Acc: 0.9930 | Loss: 0.0316\n",
            "Epoch 43/100 | Acc: 0.9930 | Loss: 0.0303\n",
            "Epoch 44/100 | Acc: 0.9910 | Loss: 0.0300\n",
            "Epoch 45/100 | Acc: 0.9920 | Loss: 0.0337\n",
            "Epoch 46/100 | Acc: 0.9930 | Loss: 0.0301\n",
            "Epoch 47/100 | Acc: 0.9930 | Loss: 0.0304\n",
            "Epoch 48/100 | Acc: 0.9930 | Loss: 0.0302\n",
            "Epoch 49/100 | Acc: 0.9930 | Loss: 0.0262\n",
            "Epoch 50/100 | Acc: 0.9860 | Loss: 0.0398\n",
            "Epoch 51/100 | Acc: 0.9930 | Loss: 0.0262\n",
            "Epoch 52/100 | Acc: 0.9920 | Loss: 0.0290\n",
            "Epoch 53/100 | Acc: 0.9930 | Loss: 0.0310\n",
            "Epoch 54/100 | Acc: 0.9930 | Loss: 0.0272\n",
            "Epoch 55/100 | Acc: 0.9930 | Loss: 0.0290\n",
            "Epoch 56/100 | Acc: 0.9930 | Loss: 0.0264\n",
            "Epoch 57/100 | Acc: 0.9930 | Loss: 0.0286\n",
            "Epoch 58/100 | Acc: 0.9930 | Loss: 0.0280\n",
            "Epoch 59/100 | Acc: 0.9840 | Loss: 0.0404\n",
            "Epoch 60/100 | Acc: 0.9910 | Loss: 0.0267\n",
            "Epoch 61/100 | Acc: 0.9930 | Loss: 0.0245\n",
            "Epoch 62/100 | Acc: 0.9930 | Loss: 0.0277\n",
            "Epoch 63/100 | Acc: 0.9930 | Loss: 0.0268\n",
            "Epoch 64/100 | Acc: 0.9930 | Loss: 0.0253\n",
            "Epoch 65/100 | Acc: 0.9930 | Loss: 0.0237\n",
            "Epoch 66/100 | Acc: 0.9910 | Loss: 0.0271\n",
            "Epoch 67/100 | Acc: 0.9930 | Loss: 0.0242\n",
            "Epoch 68/100 | Acc: 0.9930 | Loss: 0.0282\n",
            "Epoch 69/100 | Acc: 0.9920 | Loss: 0.0261\n",
            "Epoch 70/100 | Acc: 0.9940 | Loss: 0.0259\n",
            "Epoch 71/100 | Acc: 0.9930 | Loss: 0.0231\n",
            "Epoch 72/100 | Acc: 0.9930 | Loss: 0.0259\n",
            "Epoch 73/100 | Acc: 0.9920 | Loss: 0.0236\n",
            "Epoch 74/100 | Acc: 0.9920 | Loss: 0.0250\n",
            "Epoch 75/100 | Acc: 0.9920 | Loss: 0.0263\n",
            "Epoch 76/100 | Acc: 0.9930 | Loss: 0.0239\n",
            "Epoch 77/100 | Acc: 0.9920 | Loss: 0.0257\n",
            "Epoch 78/100 | Acc: 0.9930 | Loss: 0.0236\n",
            "Epoch 79/100 | Acc: 0.9950 | Loss: 0.0239\n",
            "Epoch 80/100 | Acc: 0.9950 | Loss: 0.0221\n",
            "Epoch 81/100 | Acc: 0.9930 | Loss: 0.0226\n",
            "Epoch 82/100 | Acc: 0.9860 | Loss: 0.0368\n",
            "Epoch 83/100 | Acc: 0.9950 | Loss: 0.0226\n",
            "Epoch 84/100 | Acc: 0.9940 | Loss: 0.0224\n",
            "Epoch 85/100 | Acc: 0.9940 | Loss: 0.0240\n",
            "Epoch 86/100 | Acc: 0.9940 | Loss: 0.0230\n",
            "Epoch 87/100 | Acc: 0.9930 | Loss: 0.0249\n",
            "Epoch 88/100 | Acc: 0.9940 | Loss: 0.0233\n",
            "Epoch 89/100 | Acc: 0.9940 | Loss: 0.0215\n",
            "Epoch 90/100 | Acc: 0.9940 | Loss: 0.0244\n",
            "Epoch 91/100 | Acc: 0.9940 | Loss: 0.0233\n",
            "Epoch 92/100 | Acc: 0.9940 | Loss: 0.0240\n",
            "Epoch 93/100 | Acc: 0.9940 | Loss: 0.0222\n",
            "Epoch 94/100 | Acc: 0.9940 | Loss: 0.0241\n",
            "Epoch 95/100 | Acc: 0.9950 | Loss: 0.0246\n",
            "Epoch 96/100 | Acc: 0.9940 | Loss: 0.0236\n",
            "Epoch 97/100 | Acc: 0.9940 | Loss: 0.0202\n",
            "Epoch 98/100 | Acc: 0.9940 | Loss: 0.0234\n",
            "Epoch 99/100 | Acc: 0.9940 | Loss: 0.0238\n",
            "Epoch 100/100 | Acc: 0.9940 | Loss: 0.0202\n",
            "\n",
            "üß™ BS=4 | LR=0.0001 | SEQ=10\n",
            "üìä Total sequences loaded: 4010\n",
            "Epoch 1/100 | Acc: 0.7656 | Loss: 0.6489\n",
            "Epoch 2/100 | Acc: 0.8105 | Loss: 0.3516\n",
            "Epoch 3/100 | Acc: 0.9626 | Loss: 0.1848\n",
            "Epoch 4/100 | Acc: 0.9863 | Loss: 0.1056\n",
            "Epoch 5/100 | Acc: 0.9838 | Loss: 0.0855\n",
            "Epoch 6/100 | Acc: 0.9863 | Loss: 0.0721\n",
            "Epoch 7/100 | Acc: 0.9875 | Loss: 0.0668\n",
            "Epoch 8/100 | Acc: 0.9888 | Loss: 0.0626\n",
            "Epoch 9/100 | Acc: 0.9875 | Loss: 0.0653\n",
            "Epoch 10/100 | Acc: 0.9888 | Loss: 0.0610\n",
            "Epoch 11/100 | Acc: 0.9888 | Loss: 0.0612\n",
            "Epoch 12/100 | Acc: 0.9888 | Loss: 0.0609\n",
            "Epoch 13/100 | Acc: 0.9863 | Loss: 0.0656\n",
            "Epoch 14/100 | Acc: 0.9888 | Loss: 0.0623\n",
            "Epoch 15/100 | Acc: 0.9888 | Loss: 0.0622\n",
            "Epoch 16/100 | Acc: 0.9863 | Loss: 0.0684\n",
            "Epoch 17/100 | Acc: 0.9888 | Loss: 0.0619\n",
            "Epoch 18/100 | Acc: 0.9875 | Loss: 0.0626\n",
            "Epoch 19/100 | Acc: 0.9888 | Loss: 0.0647\n",
            "Epoch 20/100 | Acc: 0.9875 | Loss: 0.0644\n",
            "Epoch 21/100 | Acc: 0.9888 | Loss: 0.0636\n",
            "Epoch 22/100 | Acc: 0.9888 | Loss: 0.0643\n",
            "Epoch 23/100 | Acc: 0.9888 | Loss: 0.0644\n",
            "Epoch 24/100 | Acc: 0.9875 | Loss: 0.0694\n",
            "Epoch 25/100 | Acc: 0.9888 | Loss: 0.0644\n",
            "Epoch 26/100 | Acc: 0.9875 | Loss: 0.0640\n",
            "Epoch 27/100 | Acc: 0.9888 | Loss: 0.0647\n",
            "Epoch 28/100 | Acc: 0.9875 | Loss: 0.0645\n",
            "Epoch 29/100 | Acc: 0.9888 | Loss: 0.0642\n",
            "Epoch 30/100 | Acc: 0.9888 | Loss: 0.0664\n",
            "Epoch 31/100 | Acc: 0.9888 | Loss: 0.0646\n",
            "Epoch 32/100 | Acc: 0.9888 | Loss: 0.0686\n",
            "Epoch 33/100 | Acc: 0.9875 | Loss: 0.0654\n",
            "Epoch 34/100 | Acc: 0.9875 | Loss: 0.0645\n",
            "Epoch 35/100 | Acc: 0.9888 | Loss: 0.0674\n",
            "Epoch 36/100 | Acc: 0.9888 | Loss: 0.0654\n",
            "Epoch 37/100 | Acc: 0.9875 | Loss: 0.0710\n",
            "Epoch 38/100 | Acc: 0.9888 | Loss: 0.0680\n",
            "Epoch 39/100 | Acc: 0.9888 | Loss: 0.0672\n",
            "Epoch 40/100 | Acc: 0.9888 | Loss: 0.0641\n",
            "Epoch 41/100 | Acc: 0.9875 | Loss: 0.0761\n",
            "Epoch 42/100 | Acc: 0.9888 | Loss: 0.0687\n",
            "Epoch 43/100 | Acc: 0.9888 | Loss: 0.0658\n",
            "Epoch 44/100 | Acc: 0.9888 | Loss: 0.0624\n",
            "Epoch 45/100 | Acc: 0.9838 | Loss: 0.0698\n",
            "Epoch 46/100 | Acc: 0.9888 | Loss: 0.0642\n",
            "Epoch 47/100 | Acc: 0.9888 | Loss: 0.0623\n",
            "Epoch 48/100 | Acc: 0.9888 | Loss: 0.0640\n",
            "Epoch 49/100 | Acc: 0.9888 | Loss: 0.0713\n",
            "Epoch 50/100 | Acc: 0.9900 | Loss: 0.0612\n",
            "Epoch 51/100 | Acc: 0.9900 | Loss: 0.0635\n",
            "Epoch 52/100 | Acc: 0.9888 | Loss: 0.0672\n",
            "Epoch 53/100 | Acc: 0.9888 | Loss: 0.0691\n",
            "Epoch 54/100 | Acc: 0.9900 | Loss: 0.0576\n",
            "Epoch 55/100 | Acc: 0.9863 | Loss: 0.0618\n",
            "Epoch 56/100 | Acc: 0.9888 | Loss: 0.0637\n",
            "Epoch 57/100 | Acc: 0.9888 | Loss: 0.0661\n",
            "Epoch 58/100 | Acc: 0.9888 | Loss: 0.0630\n",
            "Epoch 59/100 | Acc: 0.9888 | Loss: 0.0637\n",
            "Epoch 60/100 | Acc: 0.9888 | Loss: 0.0667\n",
            "Epoch 61/100 | Acc: 0.9900 | Loss: 0.0625\n",
            "Epoch 62/100 | Acc: 0.9888 | Loss: 0.0626\n",
            "Epoch 63/100 | Acc: 0.9888 | Loss: 0.0662\n",
            "Epoch 64/100 | Acc: 0.9888 | Loss: 0.0642\n",
            "Epoch 65/100 | Acc: 0.9888 | Loss: 0.0622\n",
            "Epoch 66/100 | Acc: 0.9888 | Loss: 0.0636\n",
            "Epoch 67/100 | Acc: 0.9888 | Loss: 0.0656\n",
            "Epoch 68/100 | Acc: 0.9888 | Loss: 0.0679\n",
            "Epoch 69/100 | Acc: 0.9875 | Loss: 0.0681\n",
            "Epoch 70/100 | Acc: 0.9888 | Loss: 0.0712\n",
            "Epoch 71/100 | Acc: 0.9900 | Loss: 0.0616\n",
            "Epoch 72/100 | Acc: 0.9888 | Loss: 0.0623\n",
            "Epoch 73/100 | Acc: 0.9888 | Loss: 0.0733\n",
            "Epoch 74/100 | Acc: 0.9888 | Loss: 0.0625\n",
            "Epoch 75/100 | Acc: 0.9888 | Loss: 0.0658\n",
            "Epoch 76/100 | Acc: 0.9888 | Loss: 0.0662\n",
            "Epoch 77/100 | Acc: 0.9900 | Loss: 0.0604\n",
            "Epoch 78/100 | Acc: 0.9888 | Loss: 0.0614\n",
            "Epoch 79/100 | Acc: 0.9888 | Loss: 0.0628\n",
            "Epoch 80/100 | Acc: 0.9900 | Loss: 0.0639\n",
            "Epoch 81/100 | Acc: 0.9888 | Loss: 0.0612\n",
            "Epoch 82/100 | Acc: 0.9900 | Loss: 0.0628\n",
            "Epoch 83/100 | Acc: 0.9888 | Loss: 0.0733\n",
            "Epoch 84/100 | Acc: 0.9900 | Loss: 0.0588\n",
            "Epoch 85/100 | Acc: 0.9900 | Loss: 0.0592\n",
            "Epoch 86/100 | Acc: 0.9888 | Loss: 0.0614\n",
            "Epoch 87/100 | Acc: 0.9900 | Loss: 0.0606\n",
            "Epoch 88/100 | Acc: 0.9900 | Loss: 0.0606\n",
            "Epoch 89/100 | Acc: 0.9900 | Loss: 0.0632\n",
            "Epoch 90/100 | Acc: 0.9900 | Loss: 0.0592\n",
            "Epoch 91/100 | Acc: 0.9863 | Loss: 0.0649\n",
            "Epoch 92/100 | Acc: 0.9900 | Loss: 0.0581\n",
            "Epoch 93/100 | Acc: 0.9888 | Loss: 0.0681\n",
            "Epoch 94/100 | Acc: 0.9888 | Loss: 0.0610\n",
            "Epoch 95/100 | Acc: 0.9888 | Loss: 0.0679\n",
            "Epoch 96/100 | Acc: 0.9875 | Loss: 0.0580\n",
            "Epoch 97/100 | Acc: 0.9838 | Loss: 0.0719\n",
            "Epoch 98/100 | Acc: 0.9900 | Loss: 0.0604\n",
            "Epoch 99/100 | Acc: 0.9900 | Loss: 0.0604\n",
            "Epoch 100/100 | Acc: 0.9850 | Loss: 0.0582\n",
            "\n",
            "üß™ BS=8 | LR=0.001 | SEQ=4\n",
            "üìä Total sequences loaded: 10023\n",
            "Epoch 1/100 | Acc: 0.9830 | Loss: 0.0875\n",
            "Epoch 2/100 | Acc: 0.9805 | Loss: 0.0769\n",
            "Epoch 3/100 | Acc: 0.9870 | Loss: 0.0631\n",
            "Epoch 4/100 | Acc: 0.9830 | Loss: 0.0683\n",
            "Epoch 5/100 | Acc: 0.9870 | Loss: 0.0595\n",
            "Epoch 6/100 | Acc: 0.9865 | Loss: 0.0515\n",
            "Epoch 7/100 | Acc: 0.9895 | Loss: 0.0415\n",
            "Epoch 8/100 | Acc: 0.9875 | Loss: 0.0478\n",
            "Epoch 9/100 | Acc: 0.9840 | Loss: 0.0507\n",
            "Epoch 10/100 | Acc: 0.9880 | Loss: 0.0429\n",
            "Epoch 11/100 | Acc: 0.9885 | Loss: 0.0427\n",
            "Epoch 12/100 | Acc: 0.9885 | Loss: 0.0426\n",
            "Epoch 13/100 | Acc: 0.9885 | Loss: 0.0374\n",
            "Epoch 14/100 | Acc: 0.9895 | Loss: 0.0396\n",
            "Epoch 15/100 | Acc: 0.9885 | Loss: 0.0375\n",
            "Epoch 16/100 | Acc: 0.9895 | Loss: 0.0370\n",
            "Epoch 17/100 | Acc: 0.9905 | Loss: 0.0374\n",
            "Epoch 18/100 | Acc: 0.9915 | Loss: 0.0348\n",
            "Epoch 19/100 | Acc: 0.9925 | Loss: 0.0310\n",
            "Epoch 20/100 | Acc: 0.9905 | Loss: 0.0337\n",
            "Epoch 21/100 | Acc: 0.9915 | Loss: 0.0311\n",
            "Epoch 22/100 | Acc: 0.9895 | Loss: 0.0318\n",
            "Epoch 23/100 | Acc: 0.9935 | Loss: 0.0293\n",
            "Epoch 24/100 | Acc: 0.9890 | Loss: 0.0336\n",
            "Epoch 25/100 | Acc: 0.9915 | Loss: 0.0327\n",
            "Epoch 26/100 | Acc: 0.9875 | Loss: 0.0353\n",
            "Epoch 27/100 | Acc: 0.9915 | Loss: 0.0281\n",
            "Epoch 28/100 | Acc: 0.9905 | Loss: 0.0307\n",
            "Epoch 29/100 | Acc: 0.9905 | Loss: 0.0309\n",
            "Epoch 30/100 | Acc: 0.9935 | Loss: 0.0310\n",
            "Epoch 31/100 | Acc: 0.9935 | Loss: 0.0275\n",
            "Epoch 32/100 | Acc: 0.9930 | Loss: 0.0256\n",
            "Epoch 33/100 | Acc: 0.9930 | Loss: 0.0252\n",
            "Epoch 34/100 | Acc: 0.9935 | Loss: 0.0236\n",
            "Epoch 35/100 | Acc: 0.9940 | Loss: 0.0235\n",
            "Epoch 36/100 | Acc: 0.9925 | Loss: 0.0253\n",
            "Epoch 37/100 | Acc: 0.9895 | Loss: 0.0314\n",
            "Epoch 38/100 | Acc: 0.9925 | Loss: 0.0268\n",
            "Epoch 39/100 | Acc: 0.9925 | Loss: 0.0266\n",
            "Epoch 40/100 | Acc: 0.9940 | Loss: 0.0252\n",
            "Epoch 41/100 | Acc: 0.9930 | Loss: 0.0240\n",
            "Epoch 42/100 | Acc: 0.9925 | Loss: 0.0250\n",
            "Epoch 43/100 | Acc: 0.9925 | Loss: 0.0288\n",
            "Epoch 44/100 | Acc: 0.9925 | Loss: 0.0289\n",
            "Epoch 45/100 | Acc: 0.9940 | Loss: 0.0246\n",
            "Epoch 46/100 | Acc: 0.9920 | Loss: 0.0265\n",
            "Epoch 47/100 | Acc: 0.9935 | Loss: 0.0246\n",
            "Epoch 48/100 | Acc: 0.9910 | Loss: 0.0247\n",
            "Epoch 49/100 | Acc: 0.9910 | Loss: 0.0341\n",
            "Epoch 50/100 | Acc: 0.9945 | Loss: 0.0229\n",
            "Epoch 51/100 | Acc: 0.9935 | Loss: 0.0213\n",
            "Epoch 52/100 | Acc: 0.9935 | Loss: 0.0247\n",
            "Epoch 53/100 | Acc: 0.9945 | Loss: 0.0238\n",
            "Epoch 54/100 | Acc: 0.9925 | Loss: 0.0309\n",
            "Epoch 55/100 | Acc: 0.9940 | Loss: 0.0239\n",
            "Epoch 56/100 | Acc: 0.9950 | Loss: 0.0271\n",
            "Epoch 57/100 | Acc: 0.9920 | Loss: 0.0265\n",
            "Epoch 58/100 | Acc: 0.9920 | Loss: 0.0292\n",
            "Epoch 59/100 | Acc: 0.9930 | Loss: 0.0236\n",
            "Epoch 60/100 | Acc: 0.9930 | Loss: 0.0251\n",
            "Epoch 61/100 | Acc: 0.9945 | Loss: 0.0208\n",
            "Epoch 62/100 | Acc: 0.9930 | Loss: 0.0294\n",
            "Epoch 63/100 | Acc: 0.9940 | Loss: 0.0230\n",
            "Epoch 64/100 | Acc: 0.9925 | Loss: 0.0268\n",
            "Epoch 65/100 | Acc: 0.9930 | Loss: 0.0251\n",
            "Epoch 66/100 | Acc: 0.9900 | Loss: 0.0320\n",
            "Epoch 67/100 | Acc: 0.9940 | Loss: 0.0248\n",
            "Epoch 68/100 | Acc: 0.9915 | Loss: 0.0250\n",
            "Epoch 69/100 | Acc: 0.9930 | Loss: 0.0218\n",
            "Epoch 70/100 | Acc: 0.9930 | Loss: 0.0227\n",
            "Epoch 71/100 | Acc: 0.9935 | Loss: 0.0227\n",
            "Epoch 72/100 | Acc: 0.9940 | Loss: 0.0222\n",
            "Epoch 73/100 | Acc: 0.9945 | Loss: 0.0203\n",
            "Epoch 74/100 | Acc: 0.9935 | Loss: 0.0285\n",
            "Epoch 75/100 | Acc: 0.9940 | Loss: 0.0243\n",
            "Epoch 76/100 | Acc: 0.9930 | Loss: 0.0252\n",
            "Epoch 77/100 | Acc: 0.9935 | Loss: 0.0202\n",
            "Epoch 78/100 | Acc: 0.9915 | Loss: 0.0241\n",
            "Epoch 79/100 | Acc: 0.9945 | Loss: 0.0209\n",
            "Epoch 80/100 | Acc: 0.9935 | Loss: 0.0240\n",
            "Epoch 81/100 | Acc: 0.9940 | Loss: 0.0222\n",
            "Epoch 82/100 | Acc: 0.9925 | Loss: 0.0244\n",
            "Epoch 83/100 | Acc: 0.9935 | Loss: 0.0243\n",
            "Epoch 84/100 | Acc: 0.9905 | Loss: 0.0286\n",
            "Epoch 85/100 | Acc: 0.9925 | Loss: 0.0227\n",
            "Epoch 86/100 | Acc: 0.9935 | Loss: 0.0235\n",
            "Epoch 87/100 | Acc: 0.9935 | Loss: 0.0247\n",
            "Epoch 88/100 | Acc: 0.9925 | Loss: 0.0235\n",
            "Epoch 89/100 | Acc: 0.9925 | Loss: 0.0242\n",
            "Epoch 90/100 | Acc: 0.9920 | Loss: 0.0272\n",
            "Epoch 91/100 | Acc: 0.9930 | Loss: 0.0234\n",
            "Epoch 92/100 | Acc: 0.9925 | Loss: 0.0311\n",
            "Epoch 93/100 | Acc: 0.9905 | Loss: 0.0315\n",
            "Epoch 94/100 | Acc: 0.9935 | Loss: 0.0234\n",
            "Epoch 95/100 | Acc: 0.9925 | Loss: 0.0270\n",
            "Epoch 96/100 | Acc: 0.9890 | Loss: 0.0412\n",
            "Epoch 97/100 | Acc: 0.9940 | Loss: 0.0228\n",
            "Epoch 98/100 | Acc: 0.9915 | Loss: 0.0307\n",
            "Epoch 99/100 | Acc: 0.9930 | Loss: 0.0258\n",
            "Epoch 100/100 | Acc: 0.9930 | Loss: 0.0267\n",
            "\n",
            "üß™ BS=8 | LR=0.001 | SEQ=8\n",
            "üìä Total sequences loaded: 5001\n",
            "Epoch 1/100 | Acc: 0.9820 | Loss: 0.0790\n",
            "Epoch 2/100 | Acc: 0.9920 | Loss: 0.0600\n",
            "Epoch 3/100 | Acc: 0.9920 | Loss: 0.0480\n",
            "Epoch 4/100 | Acc: 0.9920 | Loss: 0.0462\n",
            "Epoch 5/100 | Acc: 0.9910 | Loss: 0.0474\n",
            "Epoch 6/100 | Acc: 0.9930 | Loss: 0.0456\n",
            "Epoch 7/100 | Acc: 0.9930 | Loss: 0.0407\n",
            "Epoch 8/100 | Acc: 0.9920 | Loss: 0.0416\n",
            "Epoch 9/100 | Acc: 0.9930 | Loss: 0.0391\n",
            "Epoch 10/100 | Acc: 0.9920 | Loss: 0.0398\n",
            "Epoch 11/100 | Acc: 0.9930 | Loss: 0.0364\n",
            "Epoch 12/100 | Acc: 0.9920 | Loss: 0.0332\n",
            "Epoch 13/100 | Acc: 0.9920 | Loss: 0.0406\n",
            "Epoch 14/100 | Acc: 0.9690 | Loss: 0.1198\n",
            "Epoch 15/100 | Acc: 0.9930 | Loss: 0.0284\n",
            "Epoch 16/100 | Acc: 0.9930 | Loss: 0.0292\n",
            "Epoch 17/100 | Acc: 0.9930 | Loss: 0.0300\n",
            "Epoch 18/100 | Acc: 0.9930 | Loss: 0.0309\n",
            "Epoch 19/100 | Acc: 0.9930 | Loss: 0.0264\n",
            "Epoch 20/100 | Acc: 0.9930 | Loss: 0.0294\n",
            "Epoch 21/100 | Acc: 0.9930 | Loss: 0.0252\n",
            "Epoch 22/100 | Acc: 0.9940 | Loss: 0.0279\n",
            "Epoch 23/100 | Acc: 0.9940 | Loss: 0.0257\n",
            "Epoch 24/100 | Acc: 0.9950 | Loss: 0.0257\n",
            "Epoch 25/100 | Acc: 0.9920 | Loss: 0.0255\n",
            "Epoch 26/100 | Acc: 0.9930 | Loss: 0.0368\n",
            "Epoch 27/100 | Acc: 0.9930 | Loss: 0.0210\n",
            "Epoch 28/100 | Acc: 0.9940 | Loss: 0.0190\n",
            "Epoch 29/100 | Acc: 0.9940 | Loss: 0.0167\n",
            "Epoch 30/100 | Acc: 0.9950 | Loss: 0.0184\n",
            "Epoch 31/100 | Acc: 0.9950 | Loss: 0.0249\n",
            "Epoch 32/100 | Acc: 0.9950 | Loss: 0.0172\n",
            "Epoch 33/100 | Acc: 0.9940 | Loss: 0.0277\n",
            "Epoch 34/100 | Acc: 0.9940 | Loss: 0.0503\n",
            "Epoch 35/100 | Acc: 0.9950 | Loss: 0.0157\n",
            "Epoch 36/100 | Acc: 0.9920 | Loss: 0.0170\n",
            "Epoch 37/100 | Acc: 0.9910 | Loss: 0.0343\n",
            "Epoch 38/100 | Acc: 0.9950 | Loss: 0.0166\n",
            "Epoch 39/100 | Acc: 0.9950 | Loss: 0.0122\n",
            "Epoch 40/100 | Acc: 0.9950 | Loss: 0.0150\n",
            "Epoch 41/100 | Acc: 0.9890 | Loss: 0.0255\n",
            "Epoch 42/100 | Acc: 0.9950 | Loss: 0.0141\n",
            "Epoch 43/100 | Acc: 0.9930 | Loss: 0.0201\n",
            "Epoch 44/100 | Acc: 0.9940 | Loss: 0.0354\n",
            "Epoch 45/100 | Acc: 0.9930 | Loss: 0.0201\n",
            "Epoch 46/100 | Acc: 0.9940 | Loss: 0.0114\n",
            "Epoch 47/100 | Acc: 0.9950 | Loss: 0.0110\n",
            "Epoch 48/100 | Acc: 0.9950 | Loss: 0.0220\n",
            "Epoch 49/100 | Acc: 0.9920 | Loss: 0.0305\n",
            "Epoch 50/100 | Acc: 0.9960 | Loss: 0.0151\n",
            "Epoch 51/100 | Acc: 0.9950 | Loss: 0.0108\n",
            "Epoch 52/100 | Acc: 0.9970 | Loss: 0.0113\n",
            "Epoch 53/100 | Acc: 0.9960 | Loss: 0.0183\n",
            "Epoch 54/100 | Acc: 0.9940 | Loss: 0.0191\n",
            "Epoch 55/100 | Acc: 0.9950 | Loss: 0.0134\n",
            "Epoch 56/100 | Acc: 0.9960 | Loss: 0.0258\n",
            "Epoch 57/100 | Acc: 0.9940 | Loss: 0.0179\n",
            "Epoch 58/100 | Acc: 0.9950 | Loss: 0.0170\n",
            "Epoch 59/100 | Acc: 0.9950 | Loss: 0.0240\n",
            "Epoch 60/100 | Acc: 0.9970 | Loss: 0.0108\n",
            "Epoch 61/100 | Acc: 0.9960 | Loss: 0.0138\n",
            "Epoch 62/100 | Acc: 0.9960 | Loss: 0.0097\n",
            "Epoch 63/100 | Acc: 0.9960 | Loss: 0.0105\n",
            "Epoch 64/100 | Acc: 0.9970 | Loss: 0.0084\n",
            "Epoch 65/100 | Acc: 0.9950 | Loss: 0.0149\n",
            "Epoch 66/100 | Acc: 0.9980 | Loss: 0.0096\n",
            "Epoch 67/100 | Acc: 0.9940 | Loss: 0.0151\n",
            "Epoch 68/100 | Acc: 0.9950 | Loss: 0.0183\n",
            "Epoch 69/100 | Acc: 0.9950 | Loss: 0.0219\n",
            "Epoch 70/100 | Acc: 0.9950 | Loss: 0.0142\n",
            "Epoch 71/100 | Acc: 0.9950 | Loss: 0.0168\n",
            "Epoch 72/100 | Acc: 0.9950 | Loss: 0.0224\n",
            "Epoch 73/100 | Acc: 0.9950 | Loss: 0.0270\n",
            "Epoch 74/100 | Acc: 0.9960 | Loss: 0.0111\n",
            "Epoch 75/100 | Acc: 0.9980 | Loss: 0.0119\n",
            "Epoch 76/100 | Acc: 0.9910 | Loss: 0.0285\n",
            "Epoch 77/100 | Acc: 0.9960 | Loss: 0.0128\n",
            "Epoch 78/100 | Acc: 0.9960 | Loss: 0.0131\n",
            "Epoch 79/100 | Acc: 0.9970 | Loss: 0.0072\n",
            "Epoch 80/100 | Acc: 0.9960 | Loss: 0.0166\n",
            "Epoch 81/100 | Acc: 0.9960 | Loss: 0.0123\n",
            "Epoch 82/100 | Acc: 0.9950 | Loss: 0.0157\n",
            "Epoch 83/100 | Acc: 0.9950 | Loss: 0.0145\n",
            "Epoch 84/100 | Acc: 0.9940 | Loss: 0.0221\n",
            "Epoch 85/100 | Acc: 0.9960 | Loss: 0.0175\n",
            "Epoch 86/100 | Acc: 0.9950 | Loss: 0.0195\n",
            "Epoch 87/100 | Acc: 0.9960 | Loss: 0.0129\n",
            "Epoch 88/100 | Acc: 0.9960 | Loss: 0.0143\n",
            "Epoch 89/100 | Acc: 0.9960 | Loss: 0.0173\n",
            "Epoch 90/100 | Acc: 0.9980 | Loss: 0.0089\n",
            "Epoch 91/100 | Acc: 0.9940 | Loss: 0.0252\n",
            "Epoch 92/100 | Acc: 0.9960 | Loss: 0.0088\n",
            "Epoch 93/100 | Acc: 0.9950 | Loss: 0.0129\n",
            "Epoch 94/100 | Acc: 0.9960 | Loss: 0.0116\n",
            "Epoch 95/100 | Acc: 0.9960 | Loss: 0.0194\n",
            "Epoch 96/100 | Acc: 0.9960 | Loss: 0.0106\n",
            "Epoch 97/100 | Acc: 0.9930 | Loss: 0.0196\n",
            "Epoch 98/100 | Acc: 0.9970 | Loss: 0.0103\n",
            "Epoch 99/100 | Acc: 0.9960 | Loss: 0.0203\n",
            "Epoch 100/100 | Acc: 0.9960 | Loss: 0.0128\n",
            "\n",
            "üß™ BS=8 | LR=0.001 | SEQ=10\n",
            "üìä Total sequences loaded: 4010\n",
            "Epoch 1/100 | Acc: 0.9838 | Loss: 0.1111\n",
            "Epoch 2/100 | Acc: 0.9800 | Loss: 0.0831\n",
            "Epoch 3/100 | Acc: 0.9850 | Loss: 0.0642\n",
            "Epoch 4/100 | Acc: 0.9900 | Loss: 0.0600\n",
            "Epoch 5/100 | Acc: 0.9888 | Loss: 0.0563\n",
            "Epoch 6/100 | Acc: 0.9850 | Loss: 0.0684\n",
            "Epoch 7/100 | Acc: 0.9900 | Loss: 0.0550\n",
            "Epoch 8/100 | Acc: 0.9863 | Loss: 0.0654\n",
            "Epoch 9/100 | Acc: 0.9875 | Loss: 0.0689\n",
            "Epoch 10/100 | Acc: 0.9900 | Loss: 0.0666\n",
            "Epoch 11/100 | Acc: 0.9888 | Loss: 0.0720\n",
            "Epoch 12/100 | Acc: 0.9900 | Loss: 0.0592\n",
            "Epoch 13/100 | Acc: 0.9900 | Loss: 0.0538\n",
            "Epoch 14/100 | Acc: 0.9900 | Loss: 0.0527\n",
            "Epoch 15/100 | Acc: 0.9888 | Loss: 0.0568\n",
            "Epoch 16/100 | Acc: 0.9888 | Loss: 0.0622\n",
            "Epoch 17/100 | Acc: 0.9900 | Loss: 0.0555\n",
            "Epoch 18/100 | Acc: 0.9900 | Loss: 0.0614\n",
            "Epoch 19/100 | Acc: 0.9900 | Loss: 0.0502\n",
            "Epoch 20/100 | Acc: 0.9888 | Loss: 0.0591\n",
            "Epoch 21/100 | Acc: 0.9900 | Loss: 0.0601\n",
            "Epoch 22/100 | Acc: 0.9888 | Loss: 0.0521\n",
            "Epoch 23/100 | Acc: 0.9900 | Loss: 0.0491\n",
            "Epoch 24/100 | Acc: 0.9900 | Loss: 0.0594\n",
            "Epoch 25/100 | Acc: 0.9900 | Loss: 0.0625\n",
            "Epoch 26/100 | Acc: 0.9913 | Loss: 0.0457\n",
            "Epoch 27/100 | Acc: 0.9863 | Loss: 0.0599\n",
            "Epoch 28/100 | Acc: 0.9913 | Loss: 0.0426\n",
            "Epoch 29/100 | Acc: 0.9900 | Loss: 0.0614\n",
            "Epoch 30/100 | Acc: 0.9913 | Loss: 0.0380\n",
            "Epoch 31/100 | Acc: 0.9863 | Loss: 0.0496\n",
            "Epoch 32/100 | Acc: 0.9913 | Loss: 0.0462\n",
            "Epoch 33/100 | Acc: 0.9863 | Loss: 0.0566\n",
            "Epoch 34/100 | Acc: 0.9913 | Loss: 0.0497\n",
            "Epoch 35/100 | Acc: 0.9900 | Loss: 0.0484\n",
            "Epoch 36/100 | Acc: 0.9900 | Loss: 0.0474\n",
            "Epoch 37/100 | Acc: 0.9913 | Loss: 0.0455\n",
            "Epoch 38/100 | Acc: 0.9925 | Loss: 0.0435\n",
            "Epoch 39/100 | Acc: 0.9925 | Loss: 0.0321\n",
            "Epoch 40/100 | Acc: 0.9913 | Loss: 0.0326\n",
            "Epoch 41/100 | Acc: 0.9888 | Loss: 0.0394\n",
            "Epoch 42/100 | Acc: 0.9925 | Loss: 0.0362\n",
            "Epoch 43/100 | Acc: 0.9913 | Loss: 0.0309\n",
            "Epoch 44/100 | Acc: 0.9913 | Loss: 0.0390\n",
            "Epoch 45/100 | Acc: 0.9688 | Loss: 0.0723\n",
            "Epoch 46/100 | Acc: 0.9925 | Loss: 0.0360\n",
            "Epoch 47/100 | Acc: 0.9913 | Loss: 0.0315\n",
            "Epoch 48/100 | Acc: 0.9913 | Loss: 0.0347\n",
            "Epoch 49/100 | Acc: 0.9900 | Loss: 0.0388\n",
            "Epoch 50/100 | Acc: 0.9900 | Loss: 0.0501\n",
            "Epoch 51/100 | Acc: 0.9863 | Loss: 0.0536\n",
            "Epoch 52/100 | Acc: 0.9913 | Loss: 0.0325\n",
            "Epoch 53/100 | Acc: 0.9900 | Loss: 0.0429\n",
            "Epoch 54/100 | Acc: 0.9925 | Loss: 0.0409\n",
            "Epoch 55/100 | Acc: 0.9863 | Loss: 0.0454\n",
            "Epoch 56/100 | Acc: 0.9913 | Loss: 0.0364\n",
            "Epoch 57/100 | Acc: 0.9913 | Loss: 0.0406\n",
            "Epoch 58/100 | Acc: 0.9938 | Loss: 0.0326\n",
            "Epoch 59/100 | Acc: 0.9925 | Loss: 0.0306\n",
            "Epoch 60/100 | Acc: 0.9925 | Loss: 0.0359\n",
            "Epoch 61/100 | Acc: 0.9925 | Loss: 0.0369\n",
            "Epoch 62/100 | Acc: 0.9800 | Loss: 0.0626\n",
            "Epoch 63/100 | Acc: 0.9913 | Loss: 0.0381\n",
            "Epoch 64/100 | Acc: 0.9913 | Loss: 0.0360\n",
            "Epoch 65/100 | Acc: 0.9913 | Loss: 0.0376\n",
            "Epoch 66/100 | Acc: 0.9875 | Loss: 0.0483\n",
            "Epoch 67/100 | Acc: 0.9913 | Loss: 0.0372\n",
            "Epoch 68/100 | Acc: 0.9913 | Loss: 0.0409\n",
            "Epoch 69/100 | Acc: 0.9900 | Loss: 0.0418\n",
            "Epoch 70/100 | Acc: 0.9950 | Loss: 0.0264\n",
            "Epoch 71/100 | Acc: 0.9913 | Loss: 0.0297\n",
            "Epoch 72/100 | Acc: 0.9925 | Loss: 0.0421\n",
            "Epoch 73/100 | Acc: 0.9913 | Loss: 0.0375\n",
            "Epoch 74/100 | Acc: 0.9913 | Loss: 0.0331\n",
            "Epoch 75/100 | Acc: 0.9913 | Loss: 0.0343\n",
            "Epoch 76/100 | Acc: 0.9913 | Loss: 0.0350\n",
            "Epoch 77/100 | Acc: 0.9913 | Loss: 0.0353\n",
            "Epoch 78/100 | Acc: 0.9925 | Loss: 0.0408\n",
            "Epoch 79/100 | Acc: 0.9938 | Loss: 0.0378\n",
            "Epoch 80/100 | Acc: 0.9925 | Loss: 0.0348\n",
            "Epoch 81/100 | Acc: 0.9925 | Loss: 0.0383\n",
            "Epoch 82/100 | Acc: 0.9913 | Loss: 0.0503\n",
            "Epoch 83/100 | Acc: 0.9913 | Loss: 0.0360\n",
            "Epoch 84/100 | Acc: 0.9913 | Loss: 0.0340\n",
            "Epoch 85/100 | Acc: 0.9925 | Loss: 0.0429\n",
            "Epoch 86/100 | Acc: 0.9925 | Loss: 0.0403\n",
            "Epoch 87/100 | Acc: 0.9913 | Loss: 0.0460\n",
            "Epoch 88/100 | Acc: 0.9913 | Loss: 0.0323\n",
            "Epoch 89/100 | Acc: 0.9913 | Loss: 0.0362\n",
            "Epoch 90/100 | Acc: 0.9913 | Loss: 0.0479\n",
            "Epoch 91/100 | Acc: 0.9913 | Loss: 0.0355\n",
            "Epoch 92/100 | Acc: 0.9913 | Loss: 0.0399\n",
            "Epoch 93/100 | Acc: 0.9913 | Loss: 0.0330\n",
            "Epoch 94/100 | Acc: 0.9913 | Loss: 0.0387\n",
            "Epoch 95/100 | Acc: 0.9913 | Loss: 0.0340\n",
            "Epoch 96/100 | Acc: 0.9938 | Loss: 0.0449\n",
            "Epoch 97/100 | Acc: 0.9913 | Loss: 0.0373\n",
            "Epoch 98/100 | Acc: 0.9938 | Loss: 0.0444\n",
            "Epoch 99/100 | Acc: 0.9925 | Loss: 0.0488\n",
            "Epoch 100/100 | Acc: 0.9913 | Loss: 0.0430\n",
            "\n",
            "üß™ BS=8 | LR=0.0001 | SEQ=4\n",
            "üìä Total sequences loaded: 10023\n",
            "Epoch 1/100 | Acc: 0.7900 | Loss: 0.6843\n",
            "Epoch 2/100 | Acc: 0.8918 | Loss: 0.3642\n",
            "Epoch 3/100 | Acc: 0.9451 | Loss: 0.1748\n",
            "Epoch 4/100 | Acc: 0.9741 | Loss: 0.1282\n",
            "Epoch 5/100 | Acc: 0.9796 | Loss: 0.1090\n",
            "Epoch 6/100 | Acc: 0.9815 | Loss: 0.0996\n",
            "Epoch 7/100 | Acc: 0.9830 | Loss: 0.0916\n",
            "Epoch 8/100 | Acc: 0.9815 | Loss: 0.0857\n",
            "Epoch 9/100 | Acc: 0.9840 | Loss: 0.0807\n",
            "Epoch 10/100 | Acc: 0.9835 | Loss: 0.0771\n",
            "Epoch 11/100 | Acc: 0.9845 | Loss: 0.0749\n",
            "Epoch 12/100 | Acc: 0.9845 | Loss: 0.0721\n",
            "Epoch 13/100 | Acc: 0.9860 | Loss: 0.0702\n",
            "Epoch 14/100 | Acc: 0.9845 | Loss: 0.0689\n",
            "Epoch 15/100 | Acc: 0.9860 | Loss: 0.0670\n",
            "Epoch 16/100 | Acc: 0.9875 | Loss: 0.0665\n",
            "Epoch 17/100 | Acc: 0.9870 | Loss: 0.0643\n",
            "Epoch 18/100 | Acc: 0.9855 | Loss: 0.0647\n",
            "Epoch 19/100 | Acc: 0.9870 | Loss: 0.0628\n",
            "Epoch 20/100 | Acc: 0.9860 | Loss: 0.0651\n",
            "Epoch 21/100 | Acc: 0.9860 | Loss: 0.0630\n",
            "Epoch 22/100 | Acc: 0.9880 | Loss: 0.0611\n",
            "Epoch 23/100 | Acc: 0.9875 | Loss: 0.0630\n",
            "Epoch 24/100 | Acc: 0.9865 | Loss: 0.0623\n",
            "Epoch 25/100 | Acc: 0.9870 | Loss: 0.0598\n",
            "Epoch 26/100 | Acc: 0.9875 | Loss: 0.0596\n",
            "Epoch 27/100 | Acc: 0.9870 | Loss: 0.0588\n",
            "Epoch 28/100 | Acc: 0.9885 | Loss: 0.0571\n",
            "Epoch 29/100 | Acc: 0.9875 | Loss: 0.0568\n",
            "Epoch 30/100 | Acc: 0.9885 | Loss: 0.0571\n",
            "Epoch 31/100 | Acc: 0.9875 | Loss: 0.0561\n",
            "Epoch 32/100 | Acc: 0.9885 | Loss: 0.0556\n",
            "Epoch 33/100 | Acc: 0.9880 | Loss: 0.0544\n",
            "Epoch 34/100 | Acc: 0.9880 | Loss: 0.0550\n",
            "Epoch 35/100 | Acc: 0.9880 | Loss: 0.0527\n",
            "Epoch 36/100 | Acc: 0.9880 | Loss: 0.0534\n",
            "Epoch 37/100 | Acc: 0.9885 | Loss: 0.0528\n",
            "Epoch 38/100 | Acc: 0.9880 | Loss: 0.0530\n",
            "Epoch 39/100 | Acc: 0.9885 | Loss: 0.0517\n",
            "Epoch 40/100 | Acc: 0.9885 | Loss: 0.0501\n",
            "Epoch 41/100 | Acc: 0.9890 | Loss: 0.0500\n",
            "Epoch 42/100 | Acc: 0.9880 | Loss: 0.0501\n",
            "Epoch 43/100 | Acc: 0.9870 | Loss: 0.0507\n",
            "Epoch 44/100 | Acc: 0.9890 | Loss: 0.0483\n",
            "Epoch 45/100 | Acc: 0.9885 | Loss: 0.0499\n",
            "Epoch 46/100 | Acc: 0.9885 | Loss: 0.0483\n",
            "Epoch 47/100 | Acc: 0.9890 | Loss: 0.0478\n",
            "Epoch 48/100 | Acc: 0.9885 | Loss: 0.0487\n",
            "Epoch 49/100 | Acc: 0.9875 | Loss: 0.0488\n",
            "Epoch 50/100 | Acc: 0.9885 | Loss: 0.0472\n",
            "Epoch 51/100 | Acc: 0.9900 | Loss: 0.0462\n",
            "Epoch 52/100 | Acc: 0.9885 | Loss: 0.0470\n",
            "Epoch 53/100 | Acc: 0.9885 | Loss: 0.0469\n",
            "Epoch 54/100 | Acc: 0.9885 | Loss: 0.0468\n",
            "Epoch 55/100 | Acc: 0.9880 | Loss: 0.0464\n",
            "Epoch 56/100 | Acc: 0.9885 | Loss: 0.0456\n",
            "Epoch 57/100 | Acc: 0.9890 | Loss: 0.0453\n",
            "Epoch 58/100 | Acc: 0.9890 | Loss: 0.0449\n",
            "Epoch 59/100 | Acc: 0.9885 | Loss: 0.0440\n",
            "Epoch 60/100 | Acc: 0.9895 | Loss: 0.0443\n",
            "Epoch 61/100 | Acc: 0.9885 | Loss: 0.0458\n",
            "Epoch 62/100 | Acc: 0.9890 | Loss: 0.0444\n",
            "Epoch 63/100 | Acc: 0.9890 | Loss: 0.0424\n",
            "Epoch 64/100 | Acc: 0.9885 | Loss: 0.0430\n",
            "Epoch 65/100 | Acc: 0.9900 | Loss: 0.0421\n",
            "Epoch 66/100 | Acc: 0.9890 | Loss: 0.0424\n",
            "Epoch 67/100 | Acc: 0.9900 | Loss: 0.0437\n",
            "Epoch 68/100 | Acc: 0.9890 | Loss: 0.0421\n",
            "Epoch 69/100 | Acc: 0.9910 | Loss: 0.0414\n",
            "Epoch 70/100 | Acc: 0.9895 | Loss: 0.0431\n",
            "Epoch 71/100 | Acc: 0.9905 | Loss: 0.0424\n",
            "Epoch 72/100 | Acc: 0.9915 | Loss: 0.0405\n",
            "Epoch 73/100 | Acc: 0.9910 | Loss: 0.0417\n",
            "Epoch 74/100 | Acc: 0.9905 | Loss: 0.0425\n",
            "Epoch 75/100 | Acc: 0.9910 | Loss: 0.0407\n",
            "Epoch 76/100 | Acc: 0.9900 | Loss: 0.0413\n",
            "Epoch 77/100 | Acc: 0.9905 | Loss: 0.0413\n",
            "Epoch 78/100 | Acc: 0.9915 | Loss: 0.0401\n",
            "Epoch 79/100 | Acc: 0.9910 | Loss: 0.0396\n",
            "Epoch 80/100 | Acc: 0.9920 | Loss: 0.0393\n",
            "Epoch 81/100 | Acc: 0.9900 | Loss: 0.0428\n",
            "Epoch 82/100 | Acc: 0.9900 | Loss: 0.0408\n",
            "Epoch 83/100 | Acc: 0.9905 | Loss: 0.0414\n",
            "Epoch 84/100 | Acc: 0.9910 | Loss: 0.0391\n",
            "Epoch 85/100 | Acc: 0.9890 | Loss: 0.0420\n",
            "Epoch 86/100 | Acc: 0.9915 | Loss: 0.0408\n",
            "Epoch 87/100 | Acc: 0.9920 | Loss: 0.0388\n",
            "Epoch 88/100 | Acc: 0.9915 | Loss: 0.0384\n",
            "Epoch 89/100 | Acc: 0.9915 | Loss: 0.0383\n",
            "Epoch 90/100 | Acc: 0.9920 | Loss: 0.0390\n",
            "Epoch 91/100 | Acc: 0.9915 | Loss: 0.0388\n",
            "Epoch 92/100 | Acc: 0.9920 | Loss: 0.0382\n",
            "Epoch 93/100 | Acc: 0.9925 | Loss: 0.0402\n",
            "Epoch 94/100 | Acc: 0.9925 | Loss: 0.0371\n",
            "Epoch 95/100 | Acc: 0.9915 | Loss: 0.0388\n",
            "Epoch 96/100 | Acc: 0.9920 | Loss: 0.0371\n",
            "Epoch 97/100 | Acc: 0.9920 | Loss: 0.0369\n",
            "Epoch 98/100 | Acc: 0.9910 | Loss: 0.0377\n",
            "Epoch 99/100 | Acc: 0.9915 | Loss: 0.0376\n",
            "Epoch 100/100 | Acc: 0.9910 | Loss: 0.0384\n",
            "\n",
            "üß™ BS=8 | LR=0.0001 | SEQ=8\n",
            "üìä Total sequences loaded: 5001\n",
            "Epoch 1/100 | Acc: 0.6603 | Loss: 0.7862\n",
            "Epoch 2/100 | Acc: 0.8561 | Loss: 0.4398\n",
            "Epoch 3/100 | Acc: 0.9520 | Loss: 0.2163\n",
            "Epoch 4/100 | Acc: 0.9740 | Loss: 0.1384\n",
            "Epoch 5/100 | Acc: 0.9810 | Loss: 0.1095\n",
            "Epoch 6/100 | Acc: 0.9800 | Loss: 0.0937\n",
            "Epoch 7/100 | Acc: 0.9840 | Loss: 0.0785\n",
            "Epoch 8/100 | Acc: 0.9880 | Loss: 0.0720\n",
            "Epoch 9/100 | Acc: 0.9920 | Loss: 0.0618\n",
            "Epoch 10/100 | Acc: 0.9930 | Loss: 0.0573\n",
            "Epoch 11/100 | Acc: 0.9920 | Loss: 0.0540\n",
            "Epoch 12/100 | Acc: 0.9920 | Loss: 0.0537\n",
            "Epoch 13/100 | Acc: 0.9930 | Loss: 0.0497\n",
            "Epoch 14/100 | Acc: 0.9920 | Loss: 0.0534\n",
            "Epoch 15/100 | Acc: 0.9920 | Loss: 0.0484\n",
            "Epoch 16/100 | Acc: 0.9930 | Loss: 0.0467\n",
            "Epoch 17/100 | Acc: 0.9920 | Loss: 0.0466\n",
            "Epoch 18/100 | Acc: 0.9930 | Loss: 0.0458\n",
            "Epoch 19/100 | Acc: 0.9930 | Loss: 0.0451\n",
            "Epoch 20/100 | Acc: 0.9930 | Loss: 0.0437\n",
            "Epoch 21/100 | Acc: 0.9930 | Loss: 0.0452\n",
            "Epoch 22/100 | Acc: 0.9930 | Loss: 0.0437\n",
            "Epoch 23/100 | Acc: 0.9900 | Loss: 0.0478\n",
            "Epoch 24/100 | Acc: 0.9930 | Loss: 0.0414\n",
            "Epoch 25/100 | Acc: 0.9930 | Loss: 0.0421\n",
            "Epoch 26/100 | Acc: 0.9930 | Loss: 0.0414\n",
            "Epoch 27/100 | Acc: 0.9930 | Loss: 0.0397\n",
            "Epoch 28/100 | Acc: 0.9930 | Loss: 0.0393\n",
            "Epoch 29/100 | Acc: 0.9930 | Loss: 0.0393\n",
            "Epoch 30/100 | Acc: 0.9930 | Loss: 0.0384\n",
            "Epoch 31/100 | Acc: 0.9920 | Loss: 0.0385\n",
            "Epoch 32/100 | Acc: 0.9920 | Loss: 0.0375\n",
            "Epoch 33/100 | Acc: 0.9930 | Loss: 0.0375\n",
            "Epoch 34/100 | Acc: 0.9930 | Loss: 0.0372\n",
            "Epoch 35/100 | Acc: 0.9930 | Loss: 0.0362\n",
            "Epoch 36/100 | Acc: 0.9930 | Loss: 0.0351\n",
            "Epoch 37/100 | Acc: 0.9930 | Loss: 0.0355\n",
            "Epoch 38/100 | Acc: 0.9920 | Loss: 0.0365\n",
            "Epoch 39/100 | Acc: 0.9930 | Loss: 0.0330\n",
            "Epoch 40/100 | Acc: 0.9920 | Loss: 0.0356\n",
            "Epoch 41/100 | Acc: 0.9910 | Loss: 0.0347\n",
            "Epoch 42/100 | Acc: 0.9930 | Loss: 0.0318\n",
            "Epoch 43/100 | Acc: 0.9930 | Loss: 0.0322\n",
            "Epoch 44/100 | Acc: 0.9920 | Loss: 0.0324\n",
            "Epoch 45/100 | Acc: 0.9930 | Loss: 0.0309\n",
            "Epoch 46/100 | Acc: 0.9940 | Loss: 0.0318\n",
            "Epoch 47/100 | Acc: 0.9940 | Loss: 0.0313\n",
            "Epoch 48/100 | Acc: 0.9940 | Loss: 0.0296\n",
            "Epoch 49/100 | Acc: 0.9930 | Loss: 0.0295\n",
            "Epoch 50/100 | Acc: 0.9940 | Loss: 0.0284\n",
            "Epoch 51/100 | Acc: 0.9940 | Loss: 0.0289\n",
            "Epoch 52/100 | Acc: 0.9940 | Loss: 0.0305\n",
            "Epoch 53/100 | Acc: 0.9940 | Loss: 0.0291\n",
            "Epoch 54/100 | Acc: 0.9930 | Loss: 0.0269\n",
            "Epoch 55/100 | Acc: 0.9930 | Loss: 0.0283\n",
            "Epoch 56/100 | Acc: 0.9930 | Loss: 0.0268\n",
            "Epoch 57/100 | Acc: 0.9920 | Loss: 0.0305\n",
            "Epoch 58/100 | Acc: 0.9880 | Loss: 0.0363\n",
            "Epoch 59/100 | Acc: 0.9930 | Loss: 0.0271\n",
            "Epoch 60/100 | Acc: 0.9910 | Loss: 0.0275\n",
            "Epoch 61/100 | Acc: 0.9930 | Loss: 0.0246\n",
            "Epoch 62/100 | Acc: 0.9940 | Loss: 0.0248\n",
            "Epoch 63/100 | Acc: 0.9940 | Loss: 0.0247\n",
            "Epoch 64/100 | Acc: 0.9930 | Loss: 0.0270\n",
            "Epoch 65/100 | Acc: 0.9930 | Loss: 0.0269\n",
            "Epoch 66/100 | Acc: 0.9930 | Loss: 0.0266\n",
            "Epoch 67/100 | Acc: 0.9930 | Loss: 0.0280\n",
            "Epoch 68/100 | Acc: 0.9940 | Loss: 0.0258\n",
            "Epoch 69/100 | Acc: 0.9930 | Loss: 0.0260\n",
            "Epoch 70/100 | Acc: 0.9940 | Loss: 0.0268\n",
            "Epoch 71/100 | Acc: 0.9930 | Loss: 0.0252\n",
            "Epoch 72/100 | Acc: 0.9930 | Loss: 0.0266\n",
            "Epoch 73/100 | Acc: 0.9930 | Loss: 0.0269\n",
            "Epoch 74/100 | Acc: 0.9940 | Loss: 0.0261\n",
            "Epoch 75/100 | Acc: 0.9930 | Loss: 0.0260\n",
            "Epoch 76/100 | Acc: 0.9940 | Loss: 0.0250\n",
            "Epoch 77/100 | Acc: 0.9940 | Loss: 0.0250\n",
            "Epoch 78/100 | Acc: 0.9910 | Loss: 0.0313\n",
            "Epoch 79/100 | Acc: 0.9930 | Loss: 0.0246\n",
            "Epoch 80/100 | Acc: 0.9930 | Loss: 0.0249\n",
            "Epoch 81/100 | Acc: 0.9940 | Loss: 0.0259\n",
            "Epoch 82/100 | Acc: 0.9920 | Loss: 0.0240\n",
            "Epoch 83/100 | Acc: 0.9940 | Loss: 0.0266\n",
            "Epoch 84/100 | Acc: 0.9930 | Loss: 0.0241\n",
            "Epoch 85/100 | Acc: 0.9930 | Loss: 0.0243\n",
            "Epoch 86/100 | Acc: 0.9940 | Loss: 0.0214\n",
            "Epoch 87/100 | Acc: 0.9940 | Loss: 0.0223\n",
            "Epoch 88/100 | Acc: 0.9940 | Loss: 0.0269\n",
            "Epoch 89/100 | Acc: 0.9940 | Loss: 0.0261\n",
            "Epoch 90/100 | Acc: 0.9940 | Loss: 0.0226\n",
            "Epoch 91/100 | Acc: 0.9940 | Loss: 0.0252\n",
            "Epoch 92/100 | Acc: 0.9940 | Loss: 0.0230\n",
            "Epoch 93/100 | Acc: 0.9940 | Loss: 0.0245\n",
            "Epoch 94/100 | Acc: 0.9940 | Loss: 0.0240\n",
            "Epoch 95/100 | Acc: 0.9940 | Loss: 0.0242\n",
            "Epoch 96/100 | Acc: 0.9930 | Loss: 0.0222\n",
            "Epoch 97/100 | Acc: 0.9940 | Loss: 0.0195\n",
            "Epoch 98/100 | Acc: 0.9940 | Loss: 0.0267\n",
            "Epoch 99/100 | Acc: 0.9940 | Loss: 0.0246\n",
            "Epoch 100/100 | Acc: 0.9940 | Loss: 0.0217\n",
            "\n",
            "üß™ BS=8 | LR=0.0001 | SEQ=10\n",
            "üìä Total sequences loaded: 4010\n",
            "Epoch 1/100 | Acc: 0.6621 | Loss: 0.7927\n",
            "Epoch 2/100 | Acc: 0.8067 | Loss: 0.5112\n",
            "Epoch 3/100 | Acc: 0.8703 | Loss: 0.2902\n",
            "Epoch 4/100 | Acc: 0.9638 | Loss: 0.1853\n",
            "Epoch 5/100 | Acc: 0.9651 | Loss: 0.1299\n",
            "Epoch 6/100 | Acc: 0.9875 | Loss: 0.1048\n",
            "Epoch 7/100 | Acc: 0.9875 | Loss: 0.0943\n",
            "Epoch 8/100 | Acc: 0.9875 | Loss: 0.0848\n",
            "Epoch 9/100 | Acc: 0.9888 | Loss: 0.0783\n",
            "Epoch 10/100 | Acc: 0.9888 | Loss: 0.0771\n",
            "Epoch 11/100 | Acc: 0.9875 | Loss: 0.0725\n",
            "Epoch 12/100 | Acc: 0.9875 | Loss: 0.0715\n",
            "Epoch 13/100 | Acc: 0.9888 | Loss: 0.0724\n",
            "Epoch 14/100 | Acc: 0.9875 | Loss: 0.0704\n",
            "Epoch 15/100 | Acc: 0.9875 | Loss: 0.0698\n",
            "Epoch 16/100 | Acc: 0.9888 | Loss: 0.0714\n",
            "Epoch 17/100 | Acc: 0.9875 | Loss: 0.0692\n",
            "Epoch 18/100 | Acc: 0.9888 | Loss: 0.0693\n",
            "Epoch 19/100 | Acc: 0.9888 | Loss: 0.0678\n",
            "Epoch 20/100 | Acc: 0.9888 | Loss: 0.0700\n",
            "Epoch 21/100 | Acc: 0.9863 | Loss: 0.0683\n",
            "Epoch 22/100 | Acc: 0.9875 | Loss: 0.0686\n",
            "Epoch 23/100 | Acc: 0.9888 | Loss: 0.0676\n",
            "Epoch 24/100 | Acc: 0.9863 | Loss: 0.0715\n",
            "Epoch 25/100 | Acc: 0.9888 | Loss: 0.0670\n",
            "Epoch 26/100 | Acc: 0.9875 | Loss: 0.0666\n",
            "Epoch 27/100 | Acc: 0.9875 | Loss: 0.0672\n",
            "Epoch 28/100 | Acc: 0.9863 | Loss: 0.0738\n",
            "Epoch 29/100 | Acc: 0.9875 | Loss: 0.0699\n",
            "Epoch 30/100 | Acc: 0.9875 | Loss: 0.0665\n",
            "Epoch 31/100 | Acc: 0.9875 | Loss: 0.0695\n",
            "Epoch 32/100 | Acc: 0.9888 | Loss: 0.0677\n",
            "Epoch 33/100 | Acc: 0.9875 | Loss: 0.0664\n",
            "Epoch 34/100 | Acc: 0.9875 | Loss: 0.0668\n",
            "Epoch 35/100 | Acc: 0.9875 | Loss: 0.0674\n",
            "Epoch 36/100 | Acc: 0.9888 | Loss: 0.0664\n",
            "Epoch 37/100 | Acc: 0.9888 | Loss: 0.0665\n",
            "Epoch 38/100 | Acc: 0.9888 | Loss: 0.0686\n",
            "Epoch 39/100 | Acc: 0.9875 | Loss: 0.0694\n",
            "Epoch 40/100 | Acc: 0.9875 | Loss: 0.0707\n",
            "Epoch 41/100 | Acc: 0.9875 | Loss: 0.0707\n",
            "Epoch 42/100 | Acc: 0.9888 | Loss: 0.0672\n",
            "Epoch 43/100 | Acc: 0.9875 | Loss: 0.0675\n",
            "Epoch 44/100 | Acc: 0.9888 | Loss: 0.0673\n",
            "Epoch 45/100 | Acc: 0.9888 | Loss: 0.0662\n",
            "Epoch 46/100 | Acc: 0.9875 | Loss: 0.0708\n",
            "Epoch 47/100 | Acc: 0.9888 | Loss: 0.0676\n",
            "Epoch 48/100 | Acc: 0.9838 | Loss: 0.0709\n",
            "Epoch 49/100 | Acc: 0.9863 | Loss: 0.0692\n",
            "Epoch 50/100 | Acc: 0.9888 | Loss: 0.0648\n",
            "Epoch 51/100 | Acc: 0.9888 | Loss: 0.0668\n",
            "Epoch 52/100 | Acc: 0.9875 | Loss: 0.0745\n",
            "Epoch 53/100 | Acc: 0.9888 | Loss: 0.0663\n",
            "Epoch 54/100 | Acc: 0.9888 | Loss: 0.0651\n",
            "Epoch 55/100 | Acc: 0.9875 | Loss: 0.0681\n",
            "Epoch 56/100 | Acc: 0.9888 | Loss: 0.0641\n",
            "Epoch 57/100 | Acc: 0.9888 | Loss: 0.0630\n",
            "Epoch 58/100 | Acc: 0.9888 | Loss: 0.0704\n",
            "Epoch 59/100 | Acc: 0.9875 | Loss: 0.0648\n",
            "Epoch 60/100 | Acc: 0.9888 | Loss: 0.0628\n",
            "Epoch 61/100 | Acc: 0.9875 | Loss: 0.0733\n",
            "Epoch 62/100 | Acc: 0.9863 | Loss: 0.0632\n",
            "Epoch 63/100 | Acc: 0.9850 | Loss: 0.0656\n",
            "Epoch 64/100 | Acc: 0.9888 | Loss: 0.0650\n",
            "Epoch 65/100 | Acc: 0.9888 | Loss: 0.0584\n",
            "Epoch 66/100 | Acc: 0.9875 | Loss: 0.0610\n",
            "Epoch 67/100 | Acc: 0.9888 | Loss: 0.0631\n",
            "Epoch 68/100 | Acc: 0.9888 | Loss: 0.0618\n",
            "Epoch 69/100 | Acc: 0.9825 | Loss: 0.0718\n",
            "Epoch 70/100 | Acc: 0.9875 | Loss: 0.0580\n",
            "Epoch 71/100 | Acc: 0.9888 | Loss: 0.0613\n",
            "Epoch 72/100 | Acc: 0.9825 | Loss: 0.0667\n",
            "Epoch 73/100 | Acc: 0.9875 | Loss: 0.0667\n",
            "Epoch 74/100 | Acc: 0.9888 | Loss: 0.0577\n",
            "Epoch 75/100 | Acc: 0.9838 | Loss: 0.0624\n",
            "Epoch 76/100 | Acc: 0.9888 | Loss: 0.0541\n",
            "Epoch 77/100 | Acc: 0.9900 | Loss: 0.0521\n",
            "Epoch 78/100 | Acc: 0.9888 | Loss: 0.0574\n",
            "Epoch 79/100 | Acc: 0.9875 | Loss: 0.0568\n",
            "Epoch 80/100 | Acc: 0.9875 | Loss: 0.0586\n",
            "Epoch 81/100 | Acc: 0.9888 | Loss: 0.0587\n",
            "Epoch 82/100 | Acc: 0.9875 | Loss: 0.0533\n",
            "Epoch 83/100 | Acc: 0.9900 | Loss: 0.0528\n",
            "Epoch 84/100 | Acc: 0.9888 | Loss: 0.0558\n",
            "Epoch 85/100 | Acc: 0.9888 | Loss: 0.0527\n",
            "Epoch 86/100 | Acc: 0.9888 | Loss: 0.0558\n",
            "Epoch 87/100 | Acc: 0.9913 | Loss: 0.0508\n",
            "Epoch 88/100 | Acc: 0.9888 | Loss: 0.0548\n",
            "Epoch 89/100 | Acc: 0.9825 | Loss: 0.0609\n",
            "Epoch 90/100 | Acc: 0.9913 | Loss: 0.0503\n",
            "Epoch 91/100 | Acc: 0.9913 | Loss: 0.0510\n",
            "Epoch 92/100 | Acc: 0.9913 | Loss: 0.0511\n",
            "Epoch 93/100 | Acc: 0.9900 | Loss: 0.0530\n",
            "Epoch 94/100 | Acc: 0.9888 | Loss: 0.0535\n",
            "Epoch 95/100 | Acc: 0.9925 | Loss: 0.0650\n",
            "Epoch 96/100 | Acc: 0.9888 | Loss: 0.0633\n",
            "Epoch 97/100 | Acc: 0.9888 | Loss: 0.0541\n",
            "Epoch 98/100 | Acc: 0.9888 | Loss: 0.0542\n",
            "Epoch 99/100 | Acc: 0.9888 | Loss: 0.0508\n",
            "Epoch 100/100 | Acc: 0.9900 | Loss: 0.0543\n",
            "\n",
            "üß™ BS=16 | LR=0.001 | SEQ=4\n",
            "üìä Total sequences loaded: 10023\n",
            "Epoch 1/100 | Acc: 0.9805 | Loss: 0.1004\n",
            "Epoch 2/100 | Acc: 0.9835 | Loss: 0.0755\n",
            "Epoch 3/100 | Acc: 0.9815 | Loss: 0.0818\n",
            "Epoch 4/100 | Acc: 0.9855 | Loss: 0.0671\n",
            "Epoch 5/100 | Acc: 0.9835 | Loss: 0.0800\n",
            "Epoch 6/100 | Acc: 0.9870 | Loss: 0.0581\n",
            "Epoch 7/100 | Acc: 0.9875 | Loss: 0.0530\n",
            "Epoch 8/100 | Acc: 0.9870 | Loss: 0.0561\n",
            "Epoch 9/100 | Acc: 0.9865 | Loss: 0.0587\n",
            "Epoch 10/100 | Acc: 0.9885 | Loss: 0.0495\n",
            "Epoch 11/100 | Acc: 0.9885 | Loss: 0.0496\n",
            "Epoch 12/100 | Acc: 0.9885 | Loss: 0.0476\n",
            "Epoch 13/100 | Acc: 0.9870 | Loss: 0.0470\n",
            "Epoch 14/100 | Acc: 0.9885 | Loss: 0.0428\n",
            "Epoch 15/100 | Acc: 0.9860 | Loss: 0.0445\n",
            "Epoch 16/100 | Acc: 0.9890 | Loss: 0.0425\n",
            "Epoch 17/100 | Acc: 0.9890 | Loss: 0.0407\n",
            "Epoch 18/100 | Acc: 0.9860 | Loss: 0.0495\n",
            "Epoch 19/100 | Acc: 0.9895 | Loss: 0.0386\n",
            "Epoch 20/100 | Acc: 0.9895 | Loss: 0.0471\n",
            "Epoch 21/100 | Acc: 0.9920 | Loss: 0.0337\n",
            "Epoch 22/100 | Acc: 0.9910 | Loss: 0.0389\n",
            "Epoch 23/100 | Acc: 0.9905 | Loss: 0.0341\n",
            "Epoch 24/100 | Acc: 0.9885 | Loss: 0.0364\n",
            "Epoch 25/100 | Acc: 0.9915 | Loss: 0.0379\n",
            "Epoch 26/100 | Acc: 0.9850 | Loss: 0.0480\n",
            "Epoch 27/100 | Acc: 0.9935 | Loss: 0.0297\n",
            "Epoch 28/100 | Acc: 0.9885 | Loss: 0.0382\n",
            "Epoch 29/100 | Acc: 0.9915 | Loss: 0.0360\n",
            "Epoch 30/100 | Acc: 0.9925 | Loss: 0.0327\n",
            "Epoch 31/100 | Acc: 0.9930 | Loss: 0.0276\n",
            "Epoch 32/100 | Acc: 0.9910 | Loss: 0.0304\n",
            "Epoch 33/100 | Acc: 0.9910 | Loss: 0.0419\n",
            "Epoch 34/100 | Acc: 0.9925 | Loss: 0.0303\n",
            "Epoch 35/100 | Acc: 0.9900 | Loss: 0.0339\n",
            "Epoch 36/100 | Acc: 0.9895 | Loss: 0.0294\n",
            "Epoch 37/100 | Acc: 0.9925 | Loss: 0.0293\n",
            "Epoch 38/100 | Acc: 0.9905 | Loss: 0.0343\n",
            "Epoch 39/100 | Acc: 0.9935 | Loss: 0.0273\n",
            "Epoch 40/100 | Acc: 0.9940 | Loss: 0.0280\n",
            "Epoch 41/100 | Acc: 0.9930 | Loss: 0.0330\n",
            "Epoch 42/100 | Acc: 0.9910 | Loss: 0.0317\n",
            "Epoch 43/100 | Acc: 0.9935 | Loss: 0.0272\n",
            "Epoch 44/100 | Acc: 0.9915 | Loss: 0.0332\n",
            "Epoch 45/100 | Acc: 0.9940 | Loss: 0.0250\n",
            "Epoch 46/100 | Acc: 0.9935 | Loss: 0.0272\n",
            "Epoch 47/100 | Acc: 0.9890 | Loss: 0.0303\n",
            "Epoch 48/100 | Acc: 0.9945 | Loss: 0.0238\n",
            "Epoch 49/100 | Acc: 0.9935 | Loss: 0.0246\n",
            "Epoch 50/100 | Acc: 0.9945 | Loss: 0.0251\n",
            "Epoch 51/100 | Acc: 0.9920 | Loss: 0.0273\n",
            "Epoch 52/100 | Acc: 0.9935 | Loss: 0.0233\n",
            "Epoch 53/100 | Acc: 0.9935 | Loss: 0.0237\n",
            "Epoch 54/100 | Acc: 0.9920 | Loss: 0.0269\n",
            "Epoch 55/100 | Acc: 0.9940 | Loss: 0.0223\n",
            "Epoch 56/100 | Acc: 0.9930 | Loss: 0.0259\n",
            "Epoch 57/100 | Acc: 0.9895 | Loss: 0.0282\n",
            "Epoch 58/100 | Acc: 0.9925 | Loss: 0.0282\n",
            "Epoch 59/100 | Acc: 0.9890 | Loss: 0.0337\n",
            "Epoch 60/100 | Acc: 0.9950 | Loss: 0.0236\n",
            "Epoch 61/100 | Acc: 0.9945 | Loss: 0.0270\n",
            "Epoch 62/100 | Acc: 0.9935 | Loss: 0.0233\n",
            "Epoch 63/100 | Acc: 0.9945 | Loss: 0.0246\n",
            "Epoch 64/100 | Acc: 0.9925 | Loss: 0.0217\n",
            "Epoch 65/100 | Acc: 0.9925 | Loss: 0.0239\n",
            "Epoch 66/100 | Acc: 0.9915 | Loss: 0.0312\n",
            "Epoch 67/100 | Acc: 0.9940 | Loss: 0.0239\n",
            "Epoch 68/100 | Acc: 0.9940 | Loss: 0.0203\n",
            "Epoch 69/100 | Acc: 0.9930 | Loss: 0.0224\n",
            "Epoch 70/100 | Acc: 0.9940 | Loss: 0.0208\n",
            "Epoch 71/100 | Acc: 0.9930 | Loss: 0.0229\n",
            "Epoch 72/100 | Acc: 0.9930 | Loss: 0.0203\n",
            "Epoch 73/100 | Acc: 0.9940 | Loss: 0.0224\n",
            "Epoch 74/100 | Acc: 0.9945 | Loss: 0.0211\n",
            "Epoch 75/100 | Acc: 0.9925 | Loss: 0.0224\n",
            "Epoch 76/100 | Acc: 0.9930 | Loss: 0.0231\n",
            "Epoch 77/100 | Acc: 0.9930 | Loss: 0.0231\n",
            "Epoch 78/100 | Acc: 0.9940 | Loss: 0.0210\n",
            "Epoch 79/100 | Acc: 0.9940 | Loss: 0.0214\n",
            "Epoch 80/100 | Acc: 0.9920 | Loss: 0.0250\n",
            "Epoch 81/100 | Acc: 0.9935 | Loss: 0.0199\n",
            "Epoch 82/100 | Acc: 0.9930 | Loss: 0.0205\n",
            "Epoch 83/100 | Acc: 0.9935 | Loss: 0.0241\n",
            "Epoch 84/100 | Acc: 0.9935 | Loss: 0.0205\n",
            "Epoch 85/100 | Acc: 0.9930 | Loss: 0.0224\n",
            "Epoch 86/100 | Acc: 0.9935 | Loss: 0.0209\n",
            "Epoch 87/100 | Acc: 0.9925 | Loss: 0.0224\n",
            "Epoch 88/100 | Acc: 0.9915 | Loss: 0.0235\n",
            "Epoch 89/100 | Acc: 0.9940 | Loss: 0.0206\n",
            "Epoch 90/100 | Acc: 0.9930 | Loss: 0.0232\n",
            "Epoch 91/100 | Acc: 0.9945 | Loss: 0.0212\n",
            "Epoch 92/100 | Acc: 0.9935 | Loss: 0.0208\n",
            "Epoch 93/100 | Acc: 0.9925 | Loss: 0.0206\n",
            "Epoch 94/100 | Acc: 0.9915 | Loss: 0.0248\n",
            "Epoch 95/100 | Acc: 0.9915 | Loss: 0.0201\n",
            "Epoch 96/100 | Acc: 0.9930 | Loss: 0.0224\n",
            "Epoch 97/100 | Acc: 0.9925 | Loss: 0.0215\n",
            "Epoch 98/100 | Acc: 0.9940 | Loss: 0.0192\n",
            "Epoch 99/100 | Acc: 0.9910 | Loss: 0.0251\n",
            "Epoch 100/100 | Acc: 0.9935 | Loss: 0.0184\n",
            "\n",
            "üß™ BS=16 | LR=0.001 | SEQ=8\n",
            "üìä Total sequences loaded: 5001\n",
            "Epoch 1/100 | Acc: 0.9770 | Loss: 0.1134\n",
            "Epoch 2/100 | Acc: 0.9890 | Loss: 0.0639\n",
            "Epoch 3/100 | Acc: 0.9930 | Loss: 0.0486\n",
            "Epoch 4/100 | Acc: 0.9910 | Loss: 0.0488\n",
            "Epoch 5/100 | Acc: 0.9900 | Loss: 0.0517\n",
            "Epoch 6/100 | Acc: 0.9920 | Loss: 0.0437\n",
            "Epoch 7/100 | Acc: 0.9930 | Loss: 0.0389\n",
            "Epoch 8/100 | Acc: 0.9930 | Loss: 0.0410\n",
            "Epoch 9/100 | Acc: 0.9910 | Loss: 0.0416\n",
            "Epoch 10/100 | Acc: 0.9920 | Loss: 0.0357\n",
            "Epoch 11/100 | Acc: 0.9930 | Loss: 0.0437\n",
            "Epoch 12/100 | Acc: 0.9930 | Loss: 0.0402\n",
            "Epoch 13/100 | Acc: 0.9920 | Loss: 0.0408\n",
            "Epoch 14/100 | Acc: 0.9930 | Loss: 0.0381\n",
            "Epoch 15/100 | Acc: 0.9920 | Loss: 0.0357\n",
            "Epoch 16/100 | Acc: 0.9890 | Loss: 0.0450\n",
            "Epoch 17/100 | Acc: 0.9930 | Loss: 0.0338\n",
            "Epoch 18/100 | Acc: 0.9930 | Loss: 0.0319\n",
            "Epoch 19/100 | Acc: 0.9930 | Loss: 0.0344\n",
            "Epoch 20/100 | Acc: 0.9920 | Loss: 0.0389\n",
            "Epoch 21/100 | Acc: 0.9930 | Loss: 0.0330\n",
            "Epoch 22/100 | Acc: 0.9930 | Loss: 0.0296\n",
            "Epoch 23/100 | Acc: 0.9940 | Loss: 0.0283\n",
            "Epoch 24/100 | Acc: 0.9940 | Loss: 0.0312\n",
            "Epoch 25/100 | Acc: 0.9940 | Loss: 0.0217\n",
            "Epoch 26/100 | Acc: 0.9940 | Loss: 0.0270\n",
            "Epoch 27/100 | Acc: 0.9930 | Loss: 0.0226\n",
            "Epoch 28/100 | Acc: 0.9920 | Loss: 0.0292\n",
            "Epoch 29/100 | Acc: 0.9910 | Loss: 0.0198\n",
            "Epoch 30/100 | Acc: 0.9940 | Loss: 0.0276\n",
            "Epoch 31/100 | Acc: 0.9930 | Loss: 0.0277\n",
            "Epoch 32/100 | Acc: 0.9910 | Loss: 0.0300\n",
            "Epoch 33/100 | Acc: 0.9940 | Loss: 0.0230\n",
            "Epoch 34/100 | Acc: 0.9940 | Loss: 0.0188\n",
            "Epoch 35/100 | Acc: 0.9960 | Loss: 0.0221\n",
            "Epoch 36/100 | Acc: 0.9950 | Loss: 0.0136\n",
            "Epoch 37/100 | Acc: 0.9940 | Loss: 0.0180\n",
            "Epoch 38/100 | Acc: 0.9950 | Loss: 0.0142\n",
            "Epoch 39/100 | Acc: 0.9940 | Loss: 0.0252\n",
            "Epoch 40/100 | Acc: 0.9960 | Loss: 0.0171\n",
            "Epoch 41/100 | Acc: 0.9940 | Loss: 0.0144\n",
            "Epoch 42/100 | Acc: 0.9930 | Loss: 0.0153\n",
            "Epoch 43/100 | Acc: 0.9950 | Loss: 0.0163\n",
            "Epoch 44/100 | Acc: 0.9950 | Loss: 0.0165\n",
            "Epoch 45/100 | Acc: 0.9960 | Loss: 0.0096\n",
            "Epoch 46/100 | Acc: 0.9940 | Loss: 0.0183\n",
            "Epoch 47/100 | Acc: 0.9940 | Loss: 0.0301\n",
            "Epoch 48/100 | Acc: 0.9930 | Loss: 0.0227\n",
            "Epoch 49/100 | Acc: 0.9940 | Loss: 0.0282\n",
            "Epoch 50/100 | Acc: 0.9960 | Loss: 0.0190\n",
            "Epoch 51/100 | Acc: 0.9940 | Loss: 0.0186\n",
            "Epoch 52/100 | Acc: 0.9990 | Loss: 0.0098\n",
            "Epoch 53/100 | Acc: 0.9940 | Loss: 0.0432\n",
            "Epoch 54/100 | Acc: 0.9950 | Loss: 0.0159\n",
            "Epoch 55/100 | Acc: 0.9950 | Loss: 0.0172\n",
            "Epoch 56/100 | Acc: 0.9930 | Loss: 0.0225\n",
            "Epoch 57/100 | Acc: 0.9950 | Loss: 0.0197\n",
            "Epoch 58/100 | Acc: 0.9940 | Loss: 0.0206\n",
            "Epoch 59/100 | Acc: 0.9960 | Loss: 0.0171\n",
            "Epoch 60/100 | Acc: 0.9930 | Loss: 0.0268\n",
            "Epoch 61/100 | Acc: 0.9950 | Loss: 0.0157\n",
            "Epoch 62/100 | Acc: 0.9960 | Loss: 0.0145\n",
            "Epoch 63/100 | Acc: 0.9950 | Loss: 0.0168\n",
            "Epoch 64/100 | Acc: 0.9960 | Loss: 0.0183\n",
            "Epoch 65/100 | Acc: 0.9960 | Loss: 0.0152\n",
            "Epoch 66/100 | Acc: 0.9960 | Loss: 0.0153\n",
            "Epoch 67/100 | Acc: 0.9960 | Loss: 0.0167\n",
            "Epoch 68/100 | Acc: 0.9950 | Loss: 0.0153\n",
            "Epoch 69/100 | Acc: 0.9950 | Loss: 0.0239\n",
            "Epoch 70/100 | Acc: 0.9950 | Loss: 0.0157\n",
            "Epoch 71/100 | Acc: 0.9940 | Loss: 0.0199\n",
            "Epoch 72/100 | Acc: 0.9960 | Loss: 0.0107\n",
            "Epoch 73/100 | Acc: 0.9960 | Loss: 0.0185\n",
            "Epoch 74/100 | Acc: 0.9950 | Loss: 0.0198\n",
            "Epoch 75/100 | Acc: 0.9960 | Loss: 0.0130\n",
            "Epoch 76/100 | Acc: 0.9960 | Loss: 0.0182\n",
            "Epoch 77/100 | Acc: 0.9950 | Loss: 0.0217\n",
            "Epoch 78/100 | Acc: 0.9970 | Loss: 0.0114\n",
            "Epoch 79/100 | Acc: 0.9950 | Loss: 0.0186\n",
            "Epoch 80/100 | Acc: 0.9970 | Loss: 0.0097\n",
            "Epoch 81/100 | Acc: 0.9970 | Loss: 0.0117\n",
            "Epoch 82/100 | Acc: 0.9960 | Loss: 0.0141\n",
            "Epoch 83/100 | Acc: 0.9970 | Loss: 0.0120\n",
            "Epoch 84/100 | Acc: 0.9940 | Loss: 0.0213\n",
            "Epoch 85/100 | Acc: 0.9970 | Loss: 0.0186\n",
            "Epoch 86/100 | Acc: 0.9960 | Loss: 0.0109\n",
            "Epoch 87/100 | Acc: 0.9960 | Loss: 0.0136\n",
            "Epoch 88/100 | Acc: 0.9970 | Loss: 0.0129\n",
            "Epoch 89/100 | Acc: 0.9950 | Loss: 0.0121\n",
            "Epoch 90/100 | Acc: 0.9950 | Loss: 0.0178\n",
            "Epoch 91/100 | Acc: 0.9960 | Loss: 0.0137\n",
            "Epoch 92/100 | Acc: 0.9950 | Loss: 0.0159\n",
            "Epoch 93/100 | Acc: 0.9960 | Loss: 0.0141\n",
            "Epoch 94/100 | Acc: 0.9970 | Loss: 0.0112\n",
            "Epoch 95/100 | Acc: 0.9960 | Loss: 0.0129\n",
            "Epoch 96/100 | Acc: 0.9960 | Loss: 0.0118\n",
            "Epoch 97/100 | Acc: 0.9950 | Loss: 0.0199\n",
            "Epoch 98/100 | Acc: 0.9970 | Loss: 0.0125\n",
            "Epoch 99/100 | Acc: 0.9920 | Loss: 0.0228\n",
            "Epoch 100/100 | Acc: 0.9940 | Loss: 0.0206\n",
            "\n",
            "üß™ BS=16 | LR=0.001 | SEQ=10\n",
            "üìä Total sequences loaded: 4010\n",
            "Epoch 1/100 | Acc: 0.9589 | Loss: 0.1593\n",
            "Epoch 2/100 | Acc: 0.9838 | Loss: 0.0940\n",
            "Epoch 3/100 | Acc: 0.9863 | Loss: 0.0646\n",
            "Epoch 4/100 | Acc: 0.9875 | Loss: 0.0661\n",
            "Epoch 5/100 | Acc: 0.9888 | Loss: 0.0604\n",
            "Epoch 6/100 | Acc: 0.9900 | Loss: 0.0608\n",
            "Epoch 7/100 | Acc: 0.9900 | Loss: 0.0600\n",
            "Epoch 8/100 | Acc: 0.9838 | Loss: 0.0696\n",
            "Epoch 9/100 | Acc: 0.9875 | Loss: 0.0624\n",
            "Epoch 10/100 | Acc: 0.9900 | Loss: 0.0587\n",
            "Epoch 11/100 | Acc: 0.9900 | Loss: 0.0595\n",
            "Epoch 12/100 | Acc: 0.9888 | Loss: 0.0621\n",
            "Epoch 13/100 | Acc: 0.9875 | Loss: 0.0604\n",
            "Epoch 14/100 | Acc: 0.9888 | Loss: 0.0602\n",
            "Epoch 15/100 | Acc: 0.9900 | Loss: 0.0627\n",
            "Epoch 16/100 | Acc: 0.9900 | Loss: 0.0596\n",
            "Epoch 17/100 | Acc: 0.9900 | Loss: 0.0630\n",
            "Epoch 18/100 | Acc: 0.9888 | Loss: 0.0587\n",
            "Epoch 19/100 | Acc: 0.9900 | Loss: 0.0659\n",
            "Epoch 20/100 | Acc: 0.9888 | Loss: 0.0701\n",
            "Epoch 21/100 | Acc: 0.9900 | Loss: 0.0539\n",
            "Epoch 22/100 | Acc: 0.9900 | Loss: 0.0470\n",
            "Epoch 23/100 | Acc: 0.9900 | Loss: 0.0510\n",
            "Epoch 24/100 | Acc: 0.9900 | Loss: 0.0621\n",
            "Epoch 25/100 | Acc: 0.9913 | Loss: 0.0491\n",
            "Epoch 26/100 | Acc: 0.9900 | Loss: 0.0557\n",
            "Epoch 27/100 | Acc: 0.9888 | Loss: 0.0598\n",
            "Epoch 28/100 | Acc: 0.9900 | Loss: 0.0527\n",
            "Epoch 29/100 | Acc: 0.9900 | Loss: 0.0585\n",
            "Epoch 30/100 | Acc: 0.9875 | Loss: 0.0753\n",
            "Epoch 31/100 | Acc: 0.9875 | Loss: 0.0689\n",
            "Epoch 32/100 | Acc: 0.9888 | Loss: 0.0699\n",
            "Epoch 33/100 | Acc: 0.9888 | Loss: 0.0601\n",
            "Epoch 34/100 | Acc: 0.9825 | Loss: 0.0708\n",
            "Epoch 35/100 | Acc: 0.9863 | Loss: 0.0564\n",
            "Epoch 36/100 | Acc: 0.9900 | Loss: 0.0458\n",
            "Epoch 37/100 | Acc: 0.9863 | Loss: 0.0586\n",
            "Epoch 38/100 | Acc: 0.9863 | Loss: 0.0667\n",
            "Epoch 39/100 | Acc: 0.9888 | Loss: 0.0601\n",
            "Epoch 40/100 | Acc: 0.9875 | Loss: 0.0458\n",
            "Epoch 41/100 | Acc: 0.9875 | Loss: 0.0499\n",
            "Epoch 42/100 | Acc: 0.9863 | Loss: 0.0566\n",
            "Epoch 43/100 | Acc: 0.9888 | Loss: 0.0617\n",
            "Epoch 44/100 | Acc: 0.9788 | Loss: 0.0795\n",
            "Epoch 45/100 | Acc: 0.9875 | Loss: 0.0495\n",
            "Epoch 46/100 | Acc: 0.9913 | Loss: 0.0435\n",
            "Epoch 47/100 | Acc: 0.9913 | Loss: 0.0559\n",
            "Epoch 48/100 | Acc: 0.9838 | Loss: 0.0524\n",
            "Epoch 49/100 | Acc: 0.9875 | Loss: 0.0423\n",
            "Epoch 50/100 | Acc: 0.9875 | Loss: 0.0499\n",
            "Epoch 51/100 | Acc: 0.9875 | Loss: 0.0516\n",
            "Epoch 52/100 | Acc: 0.9863 | Loss: 0.0693\n",
            "Epoch 53/100 | Acc: 0.9875 | Loss: 0.0496\n",
            "Epoch 54/100 | Acc: 0.9900 | Loss: 0.0497\n",
            "Epoch 55/100 | Acc: 0.9925 | Loss: 0.0442\n",
            "Epoch 56/100 | Acc: 0.9888 | Loss: 0.0423\n",
            "Epoch 57/100 | Acc: 0.9913 | Loss: 0.0442\n",
            "Epoch 58/100 | Acc: 0.9925 | Loss: 0.0547\n",
            "Epoch 59/100 | Acc: 0.9925 | Loss: 0.0449\n",
            "Epoch 60/100 | Acc: 0.9888 | Loss: 0.0431\n",
            "Epoch 61/100 | Acc: 0.9913 | Loss: 0.0447\n",
            "Epoch 62/100 | Acc: 0.9913 | Loss: 0.0419\n",
            "Epoch 63/100 | Acc: 0.9913 | Loss: 0.0413\n",
            "Epoch 64/100 | Acc: 0.9925 | Loss: 0.0425\n",
            "Epoch 65/100 | Acc: 0.9888 | Loss: 0.0585\n",
            "Epoch 66/100 | Acc: 0.9900 | Loss: 0.0363\n",
            "Epoch 67/100 | Acc: 0.9925 | Loss: 0.0395\n",
            "Epoch 68/100 | Acc: 0.9900 | Loss: 0.0357\n",
            "Epoch 69/100 | Acc: 0.9925 | Loss: 0.0339\n",
            "Epoch 70/100 | Acc: 0.9913 | Loss: 0.0436\n",
            "Epoch 71/100 | Acc: 0.9938 | Loss: 0.0307\n",
            "Epoch 72/100 | Acc: 0.9925 | Loss: 0.0354\n",
            "Epoch 73/100 | Acc: 0.9888 | Loss: 0.0417\n",
            "Epoch 74/100 | Acc: 0.9925 | Loss: 0.0369\n",
            "Epoch 75/100 | Acc: 0.9701 | Loss: 0.0596\n",
            "Epoch 76/100 | Acc: 0.9938 | Loss: 0.0336\n",
            "Epoch 77/100 | Acc: 0.9925 | Loss: 0.0315\n",
            "Epoch 78/100 | Acc: 0.9913 | Loss: 0.0387\n",
            "Epoch 79/100 | Acc: 0.9913 | Loss: 0.0399\n",
            "Epoch 80/100 | Acc: 0.9888 | Loss: 0.0553\n",
            "Epoch 81/100 | Acc: 0.9925 | Loss: 0.0367\n",
            "Epoch 82/100 | Acc: 0.9925 | Loss: 0.0465\n",
            "Epoch 83/100 | Acc: 0.9925 | Loss: 0.0333\n",
            "Epoch 84/100 | Acc: 0.9925 | Loss: 0.0405\n",
            "Epoch 85/100 | Acc: 0.9913 | Loss: 0.0378\n",
            "Epoch 86/100 | Acc: 0.9900 | Loss: 0.0383\n",
            "Epoch 87/100 | Acc: 0.9925 | Loss: 0.0301\n",
            "Epoch 88/100 | Acc: 0.9913 | Loss: 0.0368\n",
            "Epoch 89/100 | Acc: 0.9925 | Loss: 0.0332\n",
            "Epoch 90/100 | Acc: 0.9913 | Loss: 0.0389\n",
            "Epoch 91/100 | Acc: 0.9900 | Loss: 0.0423\n",
            "Epoch 92/100 | Acc: 0.9913 | Loss: 0.0458\n",
            "Epoch 93/100 | Acc: 0.9913 | Loss: 0.0355\n",
            "Epoch 94/100 | Acc: 0.9913 | Loss: 0.0491\n",
            "Epoch 95/100 | Acc: 0.9900 | Loss: 0.0620\n",
            "Epoch 96/100 | Acc: 0.9913 | Loss: 0.0359\n",
            "Epoch 97/100 | Acc: 0.9925 | Loss: 0.0420\n",
            "Epoch 98/100 | Acc: 0.9925 | Loss: 0.0333\n",
            "Epoch 99/100 | Acc: 0.9925 | Loss: 0.0341\n",
            "Epoch 100/100 | Acc: 0.9925 | Loss: 0.0395\n",
            "\n",
            "üß™ BS=16 | LR=0.0001 | SEQ=4\n",
            "üìä Total sequences loaded: 10023\n",
            "Epoch 1/100 | Acc: 0.6608 | Loss: 0.8463\n",
            "Epoch 2/100 | Acc: 0.7925 | Loss: 0.5541\n",
            "Epoch 3/100 | Acc: 0.9387 | Loss: 0.3154\n",
            "Epoch 4/100 | Acc: 0.9441 | Loss: 0.2003\n",
            "Epoch 5/100 | Acc: 0.9721 | Loss: 0.1533\n",
            "Epoch 6/100 | Acc: 0.9751 | Loss: 0.1320\n",
            "Epoch 7/100 | Acc: 0.9786 | Loss: 0.1190\n",
            "Epoch 8/100 | Acc: 0.9800 | Loss: 0.1105\n",
            "Epoch 9/100 | Acc: 0.9810 | Loss: 0.1030\n",
            "Epoch 10/100 | Acc: 0.9820 | Loss: 0.0976\n",
            "Epoch 11/100 | Acc: 0.9825 | Loss: 0.0927\n",
            "Epoch 12/100 | Acc: 0.9820 | Loss: 0.0906\n",
            "Epoch 13/100 | Acc: 0.9820 | Loss: 0.0868\n",
            "Epoch 14/100 | Acc: 0.9830 | Loss: 0.0820\n",
            "Epoch 15/100 | Acc: 0.9840 | Loss: 0.0802\n",
            "Epoch 16/100 | Acc: 0.9840 | Loss: 0.0775\n",
            "Epoch 17/100 | Acc: 0.9845 | Loss: 0.0757\n",
            "Epoch 18/100 | Acc: 0.9850 | Loss: 0.0739\n",
            "Epoch 19/100 | Acc: 0.9850 | Loss: 0.0734\n",
            "Epoch 20/100 | Acc: 0.9860 | Loss: 0.0706\n",
            "Epoch 21/100 | Acc: 0.9860 | Loss: 0.0696\n",
            "Epoch 22/100 | Acc: 0.9865 | Loss: 0.0684\n",
            "Epoch 23/100 | Acc: 0.9865 | Loss: 0.0675\n",
            "Epoch 24/100 | Acc: 0.9860 | Loss: 0.0670\n",
            "Epoch 25/100 | Acc: 0.9860 | Loss: 0.0654\n",
            "Epoch 26/100 | Acc: 0.9875 | Loss: 0.0640\n",
            "Epoch 27/100 | Acc: 0.9880 | Loss: 0.0636\n",
            "Epoch 28/100 | Acc: 0.9880 | Loss: 0.0632\n",
            "Epoch 29/100 | Acc: 0.9880 | Loss: 0.0626\n",
            "Epoch 30/100 | Acc: 0.9865 | Loss: 0.0628\n",
            "Epoch 31/100 | Acc: 0.9880 | Loss: 0.0619\n",
            "Epoch 32/100 | Acc: 0.9875 | Loss: 0.0599\n",
            "Epoch 33/100 | Acc: 0.9875 | Loss: 0.0604\n",
            "Epoch 34/100 | Acc: 0.9870 | Loss: 0.0589\n",
            "Epoch 35/100 | Acc: 0.9880 | Loss: 0.0584\n",
            "Epoch 36/100 | Acc: 0.9880 | Loss: 0.0586\n",
            "Epoch 37/100 | Acc: 0.9870 | Loss: 0.0578\n",
            "Epoch 38/100 | Acc: 0.9865 | Loss: 0.0577\n",
            "Epoch 39/100 | Acc: 0.9870 | Loss: 0.0570\n",
            "Epoch 40/100 | Acc: 0.9875 | Loss: 0.0567\n",
            "Epoch 41/100 | Acc: 0.9875 | Loss: 0.0556\n",
            "Epoch 42/100 | Acc: 0.9880 | Loss: 0.0564\n",
            "Epoch 43/100 | Acc: 0.9875 | Loss: 0.0554\n",
            "Epoch 44/100 | Acc: 0.9880 | Loss: 0.0538\n",
            "Epoch 45/100 | Acc: 0.9875 | Loss: 0.0535\n",
            "Epoch 46/100 | Acc: 0.9870 | Loss: 0.0544\n",
            "Epoch 47/100 | Acc: 0.9880 | Loss: 0.0533\n",
            "Epoch 48/100 | Acc: 0.9875 | Loss: 0.0541\n",
            "Epoch 49/100 | Acc: 0.9880 | Loss: 0.0533\n",
            "Epoch 50/100 | Acc: 0.9875 | Loss: 0.0517\n",
            "Epoch 51/100 | Acc: 0.9880 | Loss: 0.0528\n",
            "Epoch 52/100 | Acc: 0.9890 | Loss: 0.0513\n",
            "Epoch 53/100 | Acc: 0.9875 | Loss: 0.0513\n",
            "Epoch 54/100 | Acc: 0.9880 | Loss: 0.0511\n",
            "Epoch 55/100 | Acc: 0.9880 | Loss: 0.0514\n",
            "Epoch 56/100 | Acc: 0.9880 | Loss: 0.0506\n",
            "Epoch 57/100 | Acc: 0.9885 | Loss: 0.0497\n",
            "Epoch 58/100 | Acc: 0.9880 | Loss: 0.0505\n",
            "Epoch 59/100 | Acc: 0.9885 | Loss: 0.0497\n",
            "Epoch 60/100 | Acc: 0.9880 | Loss: 0.0488\n",
            "Epoch 61/100 | Acc: 0.9880 | Loss: 0.0493\n",
            "Epoch 62/100 | Acc: 0.9885 | Loss: 0.0486\n",
            "Epoch 63/100 | Acc: 0.9880 | Loss: 0.0498\n",
            "Epoch 64/100 | Acc: 0.9880 | Loss: 0.0481\n",
            "Epoch 65/100 | Acc: 0.9885 | Loss: 0.0482\n",
            "Epoch 66/100 | Acc: 0.9885 | Loss: 0.0470\n",
            "Epoch 67/100 | Acc: 0.9895 | Loss: 0.0467\n",
            "Epoch 68/100 | Acc: 0.9880 | Loss: 0.0477\n",
            "Epoch 69/100 | Acc: 0.9885 | Loss: 0.0468\n",
            "Epoch 70/100 | Acc: 0.9855 | Loss: 0.0501\n",
            "Epoch 71/100 | Acc: 0.9880 | Loss: 0.0470\n",
            "Epoch 72/100 | Acc: 0.9885 | Loss: 0.0466\n",
            "Epoch 73/100 | Acc: 0.9880 | Loss: 0.0466\n",
            "Epoch 74/100 | Acc: 0.9895 | Loss: 0.0463\n",
            "Epoch 75/100 | Acc: 0.9865 | Loss: 0.0482\n",
            "Epoch 76/100 | Acc: 0.9895 | Loss: 0.0452\n",
            "Epoch 77/100 | Acc: 0.9905 | Loss: 0.0446\n",
            "Epoch 78/100 | Acc: 0.9895 | Loss: 0.0448\n",
            "Epoch 79/100 | Acc: 0.9900 | Loss: 0.0443\n",
            "Epoch 80/100 | Acc: 0.9905 | Loss: 0.0444\n",
            "Epoch 81/100 | Acc: 0.9905 | Loss: 0.0442\n",
            "Epoch 82/100 | Acc: 0.9905 | Loss: 0.0442\n",
            "Epoch 83/100 | Acc: 0.9890 | Loss: 0.0451\n",
            "Epoch 84/100 | Acc: 0.9905 | Loss: 0.0437\n",
            "Epoch 85/100 | Acc: 0.9875 | Loss: 0.0467\n",
            "Epoch 86/100 | Acc: 0.9900 | Loss: 0.0439\n",
            "Epoch 87/100 | Acc: 0.9915 | Loss: 0.0433\n",
            "Epoch 88/100 | Acc: 0.9905 | Loss: 0.0430\n",
            "Epoch 89/100 | Acc: 0.9910 | Loss: 0.0426\n",
            "Epoch 90/100 | Acc: 0.9905 | Loss: 0.0429\n",
            "Epoch 91/100 | Acc: 0.9910 | Loss: 0.0431\n",
            "Epoch 92/100 | Acc: 0.9900 | Loss: 0.0432\n",
            "Epoch 93/100 | Acc: 0.9910 | Loss: 0.0422\n",
            "Epoch 94/100 | Acc: 0.9915 | Loss: 0.0419\n",
            "Epoch 95/100 | Acc: 0.9905 | Loss: 0.0418\n",
            "Epoch 96/100 | Acc: 0.9910 | Loss: 0.0418\n",
            "Epoch 97/100 | Acc: 0.9900 | Loss: 0.0428\n",
            "Epoch 98/100 | Acc: 0.9910 | Loss: 0.0423\n",
            "Epoch 99/100 | Acc: 0.9915 | Loss: 0.0418\n",
            "Epoch 100/100 | Acc: 0.9910 | Loss: 0.0411\n",
            "\n",
            "üß™ BS=16 | LR=0.0001 | SEQ=8\n",
            "üìä Total sequences loaded: 5001\n",
            "Epoch 1/100 | Acc: 0.6384 | Loss: 1.0427\n",
            "Epoch 2/100 | Acc: 0.6583 | Loss: 0.7852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "_q3inndRoyYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMxndCGUhHqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æË®≠ÂÆö ===\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}\n",
        "RESULT_DIR = \"./models/MLP/result\"\n",
        "for sub in [\"accuracy_curves\", \"loss_curves\", \"confusion_matrices\",\n",
        "            \"precision_curves\", \"recall_curves\", \"f1_score_curves\"]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === MLP Ê®°Âûã ===\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Ë≥áÊñôËôïÁêÜ ===\n",
        "def process_file(file_path, label, max_seq_len):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # ‚úÖ Âè™‰øùÁïô voltage, current, power ‰∏âÊ¨Ñ\n",
        "    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "\n",
        "    # ÂàáÂâ≤ÊàêÂõ∫ÂÆöÈï∑Â∫¶Â∫èÂàó\n",
        "    num_chunks = len(data) // max_seq_len\n",
        "    chunks = [data[i * max_seq_len : (i + 1) * max_seq_len] for i in range(num_chunks)]\n",
        "\n",
        "    return chunks, [label] * len(chunks)\n",
        "\n",
        "def load_all_sequences(max_seq_len):\n",
        "    all_seq, all_labels = [], []\n",
        "\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".csv\"):\n",
        "                path = os.path.join(folder, fname)\n",
        "                seqs, labels = process_file(path, label, max_seq_len)\n",
        "                all_seq.extend(seqs)\n",
        "                all_labels.extend(labels)\n",
        "\n",
        "    seq_arr = np.array(all_seq, dtype=np.float32)  # shape = (samples, seq_len, features)\n",
        "    num_samples = seq_arr.shape[0]\n",
        "\n",
        "    # ‚úÖ Êî§Âπ≥Êàê 2DÔºöËΩâÁÇ∫ (samples, seq_len * features)\n",
        "    seq_reshaped = seq_arr.reshape(num_samples, -1)\n",
        "\n",
        "    # ‚úÖ Ê®ôÊ∫ñÂåñÔºàÂè™Â∞ç 2D Ë≥áÊñôÂÅöÔºâ\n",
        "    scaled = StandardScaler().fit_transform(seq_reshaped)\n",
        "\n",
        "    print(f\"üìä Total sequences loaded: {num_samples}\")\n",
        "\n",
        "    return scaled, np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=100, hidden_dim=128, num_classes=4):\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n\\U0001f9ea BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X, y = load_all_sequences(seq_len)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "                train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                test_loader = DataLoader(ChargeSequenceDataset(X_test, y_test), batch_size=bs)\n",
        "                input_dim = X_train.shape[1]\n",
        "                model = MLPClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                acc_list, loss_list = [], []\n",
        "                precision_list, recall_list, f1_list = [], [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    for xb, yb in train_loader:\n",
        "                        xb, yb = xb.to(device), yb.to(device)\n",
        "                        optimizer.zero_grad()\n",
        "                        out = model(xb)\n",
        "                        loss = criterion(out, yb)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # Evaluate\n",
        "                    model.eval()\n",
        "                    correct, total, total_loss = 0, 0, 0\n",
        "                    y_pred, y_true = [], []\n",
        "                    with torch.no_grad():\n",
        "                        for xb, yb in test_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct += (pred == yb).sum().item()\n",
        "                            total += yb.size(0)\n",
        "                            total_loss += loss.item() * xb.size(0)\n",
        "                            y_pred.extend(pred.cpu().numpy())\n",
        "                            y_true.extend(yb.cpu().numpy())\n",
        "                    acc = correct / total\n",
        "                    avg_loss = total_loss / total\n",
        "                    acc_list.append(acc)\n",
        "                    loss_list.append(avg_loss)\n",
        "                    precision_list.append(precision_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "                    recall_list.append(recall_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "                    f1_list.append(f1_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "                    print(f\"Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "                t1 = time.time()\n",
        "\n",
        "                if acc_list[-1] >= 1.0:\n",
        "                    print(f\"\\u26a0\\ufe0f Skipped BS={bs} LR={lr} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                # ÂÑ≤Â≠òÊäòÁ∑öÂúñ\n",
        "                def save_curve(data, ylabel, folder):\n",
        "                    plt.plot(range(1, num_epochs+1), data, marker='o')\n",
        "                    plt.title(f\"{ylabel} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\"); plt.ylabel(ylabel); plt.grid(True)\n",
        "                    if ylabel == \"Accuracy\": plt.ylim(0, 1.0)\n",
        "                    plt.savefig(os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                save_curve(acc_list, \"Accuracy\", \"accuracy_curves\")\n",
        "                save_curve(loss_list, \"Loss\", \"loss_curves\")\n",
        "                save_curve(precision_list, \"Precision\", \"precision_curves\")\n",
        "                save_curve(recall_list, \"Recall\", \"recall_curves\")\n",
        "                save_curve(f1_list, \"F1-score\", \"f1_score_curves\")\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£Âúñ\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred), display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                results.append({\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_list[-1], 'recall': recall_list[-1], 'f1_score': f1_list[-1],\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"mlp_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n\\U0001f4dc All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\n\\U0001f3c6 Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results (all accuracy = 1.0 were skipped)\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")\n"
      ],
      "metadata": {
        "id": "zA1ydN20jMiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "8cmAFC98o2iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === Ë∑ØÂæëË®≠ÂÆö ===\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}\n",
        "RESULT_DIR = \"./models/SVM/result\"\n",
        "os.makedirs(os.path.join(RESULT_DIR, \"confusion_matrices\"), exist_ok=True)\n",
        "\n",
        "# === Ë≥áÊñôËôïÁêÜ ===\n",
        "def process_file(file_path, label, max_seq_len):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Âè™‰øùÁïô voltage, current, power ‰∏âÊ¨Ñ\n",
        "    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "\n",
        "    # Áî® max_seq_len ÂàÜÊÆµÂàáÂâ≤Â∫èÂàó\n",
        "    num_chunks = len(data) // max_seq_len\n",
        "    chunks = [data[i * max_seq_len : (i + 1) * max_seq_len] for i in range(num_chunks)]\n",
        "\n",
        "    return chunks, [label] * len(chunks)\n",
        "\n",
        "def load_all_sequences(max_seq_len):\n",
        "    all_seq, all_labels = [], []\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".csv\"):\n",
        "                path = os.path.join(folder, fname)\n",
        "                seqs, labels = process_file(path, label, max_seq_len)\n",
        "                all_seq.extend(seqs)\n",
        "                all_labels.extend(labels)\n",
        "\n",
        "    seq_arr = np.array(all_seq, dtype=np.float32)  # shape = (samples, seq_len, features)\n",
        "    num_samples = seq_arr.shape[0]\n",
        "\n",
        "    # Êî§Âπ≥Êàê 2DÔºöËÆäÊàê (samples, seq_len * features)\n",
        "    seq_reshaped = seq_arr.reshape(num_samples, -1)\n",
        "\n",
        "    # Ê®ôÊ∫ñÂåñ\n",
        "    scaled = StandardScaler().fit_transform(seq_reshaped)\n",
        "\n",
        "    print(f\"üìä Total sequences loaded: {num_samples}\")\n",
        "\n",
        "    return scaled, np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÊ∏¨Ë©¶ SVM ===\n",
        "import time\n",
        "def train_svm_once(kernels, Cs, seq_lens):\n",
        "    results = []\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for C_val in Cs:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüîé Kernel={kernel} | C={C_val} | SEQ={seq_len}\")\n",
        "                X, y = load_all_sequences(seq_len)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "                t0 = time.time()\n",
        "                clf = SVC(kernel=kernel, C=C_val)\n",
        "                clf.fit(X_train, y_train)\n",
        "                t1 = time.time()\n",
        "\n",
        "                y_pred = clf.predict(X_test)\n",
        "                acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                # ‚ö†Ô∏è Ë∑≥ÈÅéÊ∫ñÁ¢∫ÁéáÁÇ∫ 1.0 ÁöÑÊÉÖÊ≥Å\n",
        "                if acc >= 1.0:\n",
        "                    print(f\"‚ö†Ô∏è Skipped Kernel={kernel} C={C_val} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£Âúñ\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                results.append({\n",
        "                    'kernel': kernel, 'C': C_val, 'seq_len': seq_len,\n",
        "                    'final_acc': acc, 'precision': precision, 'recall': recall,\n",
        "                    'f1_score': f1, 'training_time_s': round(t1 - t0, 2)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"svm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ Results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: Kernel={best['kernel']} | C={best['C']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "Gxnqos1Eo4UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_lens = [4, 8, 10]\n",
        "C_list = [1.0, 10.0]\n",
        "kernel_list = [\"rbf\", \"linear\"]\n",
        "\n",
        "results, best = train_svm_once(kernel_list, C_list, seq_lens)\n",
        "\n",
        "print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "print(f\"Sequence Length = {best['seq_len']}\")\n",
        "print(f\"C = {best['C']}\")\n",
        "print(f\"Kernel = {best['kernel']}\")\n",
        "print(f\"Accuracy = {best['final_acc']:.4f}\")\n",
        "print(f\"Precision = {best['precision']:.4f}\")\n",
        "print(f\"Recall = {best['recall']:.4f}\")\n",
        "print(f\"F1-score = {best['f1_score']:.4f}\")\n",
        "print(f\"Training Time = {best['training_time_s']} Áßí\")"
      ],
      "metadata": {
        "id": "yB0a2PFxo8TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "lxVLlUCDrD_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æË®≠ÂÆö ===\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}\n",
        "RESULT_DIR = \"./models/GRU/result\"\n",
        "for sub in [\"accuracy_curves\", \"loss_curves\", \"confusion_matrices\",\n",
        "            \"precision_curves\", \"recall_curves\", \"f1_score_curves\"]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === GRU Ê®°Âûã ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === Ë≥áÊñôËôïÁêÜ ===\n",
        "def process_file(file_path, label, max_seq_len):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Âè™‰øùÁïô voltage, current, power ‰∏âÊ¨Ñ\n",
        "    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "\n",
        "    # Áî® max_seq_len ÂàÜÊÆµÂàáÂâ≤Â∫èÂàó\n",
        "    num_chunks = len(data) // max_seq_len\n",
        "    chunks = [data[i * max_seq_len : (i + 1) * max_seq_len] for i in range(num_chunks)]\n",
        "\n",
        "    return chunks, [label] * len(chunks)\n",
        "\n",
        "def load_all_sequences(max_seq_len):\n",
        "    all_seq, all_labels = [], []\n",
        "    total_sequences = 0\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".csv\"):\n",
        "                path = os.path.join(folder, fname)\n",
        "                seqs, labels = process_file(path, label, max_seq_len)\n",
        "                all_seq.extend(seqs)\n",
        "                all_labels.extend(labels)\n",
        "                total_sequences += len(seqs)\n",
        "\n",
        "    seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "    labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "    B, T, F = seq_arr.shape\n",
        "    reshaped = seq_arr.reshape(-1, F)\n",
        "    scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "    print(f\"üìä Total sequences loaded: {total_sequences}\")\n",
        "\n",
        "    return scaled, labels_arr\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4):\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X, y = load_all_sequences(seq_len)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "                train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                test_loader = DataLoader(ChargeSequenceDataset(X_test, y_test), batch_size=bs)\n",
        "                model = GRUClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                acc_list, loss_list = [], []\n",
        "                precision_list, recall_list, f1_list = [], [], []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    for xb, yb in train_loader:\n",
        "                        xb, yb = xb.to(device), yb.to(device)\n",
        "                        optimizer.zero_grad()\n",
        "                        out = model(xb)\n",
        "                        loss = criterion(out, yb)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # Evaluate\n",
        "                    model.eval()\n",
        "                    correct, total, total_loss = 0, 0, 0\n",
        "                    y_pred, y_true = [], []\n",
        "                    with torch.no_grad():\n",
        "                        for xb, yb in test_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct += (pred == yb).sum().item()\n",
        "                            total += yb.size(0)\n",
        "                            total_loss += loss.item() * xb.size(0)\n",
        "                            y_pred.extend(pred.cpu().numpy())\n",
        "                            y_true.extend(yb.cpu().numpy())\n",
        "\n",
        "                    acc = correct / total\n",
        "                    avg_loss = total_loss / total\n",
        "                    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "                    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "                    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                    acc_list.append(acc)\n",
        "                    loss_list.append(avg_loss)\n",
        "                    precision_list.append(precision)\n",
        "                    recall_list.append(recall)\n",
        "                    f1_list.append(f1)\n",
        "\n",
        "                    print(f\"Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                if acc_list[-1] >= 1.0:\n",
        "                    print(f\"‚ö†Ô∏è Skipped BS={bs} LR={lr} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                # === ÂÑ≤Â≠òÊäòÁ∑öÂúñ ===\n",
        "                def save_curve(data, name, ylabel):\n",
        "                    plt.plot(range(1, num_epochs+1), data, marker='o')\n",
        "                    plt.title(f\"{ylabel} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\"); plt.ylabel(ylabel); plt.ylim(0, 1.0); plt.grid(True)\n",
        "                    path = os.path.join(RESULT_DIR, f\"{name}_curves\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                save_curve(acc_list, \"accuracy\", \"Accuracy\")\n",
        "                save_curve(loss_list, \"loss\", \"Loss\")\n",
        "                save_curve(precision_list, \"precision\", \"Precision\")\n",
        "                save_curve(recall_list, \"recall\", \"Recall\")\n",
        "                save_curve(f1_list, \"f1_score\", \"F1-score\")\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred), display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # ÂÑ≤Â≠òÁµêÊûú\n",
        "                results.append({\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_list[-1], 'recall': recall_list[-1], 'f1_score': f1_list[-1],\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"gru_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results (all accuracy = 1.0 were skipped)\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "ZVsy2pBvrFfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ],
      "metadata": {
        "id": "YVWAGumMrPmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D CNN"
      ],
      "metadata": {
        "id": "QeKn_cKbgu0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CNN Ê®°ÂûãÂÆåÊï¥Ë®ìÁ∑¥Á®ãÂºè ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æË®≠ÂÆö ===\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "}\n",
        "RESULT_DIR = \"./models/CNN/result\"\n",
        "for sub in [\"accuracy_curves\", \"loss_curves\", \"confusion_matrices\",\n",
        "            \"precision_curves\", \"recall_curves\", \"f1_score_curves\"]:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32).unsqueeze(1)  # [B, 1, T, F]\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === CNN Ê®°Âûã ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Ë≥áÊñôËôïÁêÜ ===\n",
        "def process_file(file_path, label, max_seq_len):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Âè™‰øùÁïô voltage, current, power ‰∏âÊ¨Ñ\n",
        "    data = df[[\"voltage\", \"current\", \"power\"]].values.astype(np.float32)\n",
        "\n",
        "    # Áî® max_seq_len ÂàÜÊÆµÂàáÂâ≤Â∫èÂàó\n",
        "    num_chunks = len(data) // max_seq_len\n",
        "    chunks = [data[i * max_seq_len : (i + 1) * max_seq_len] for i in range(num_chunks)]\n",
        "\n",
        "    return chunks, [label] * len(chunks)\n",
        "\n",
        "def load_all_sequences(max_seq_len):\n",
        "    all_seq, all_labels = [], []\n",
        "    total_sequences = 0\n",
        "    for label, folder in LABEL_DIRS.items():\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".csv\"):\n",
        "                path = os.path.join(folder, fname)\n",
        "                seqs, labels = process_file(path, label, max_seq_len)\n",
        "                all_seq.extend(seqs)\n",
        "                all_labels.extend(labels)\n",
        "                total_sequences += len(seqs)\n",
        "\n",
        "    seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "    labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "    B, T, F = seq_arr.shape\n",
        "    reshaped = seq_arr.reshape(-1, F)\n",
        "    scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "    print(f\"üìä Total sequences loaded: {total_sequences}\")\n",
        "\n",
        "    return scaled, labels_arr\n",
        "\n",
        "# === Ë®ìÁ∑¥‰∏ªÂáΩÂºè ===\n",
        "def train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=100, num_classes=4):\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X, y = load_all_sequences(seq_len)\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "                train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                test_loader = DataLoader(ChargeSequenceDataset(X_test, y_test), batch_size=bs)\n",
        "                model = CNNClassifier(num_classes=num_classes).to(device)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                acc_list, loss_list, prec_list, rec_list, f1_list = [], [], [], [], []\n",
        "                t0 = time.time()\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    for xb, yb in train_loader:\n",
        "                        xb, yb = xb.to(device), yb.to(device)\n",
        "                        optimizer.zero_grad()\n",
        "                        out = model(xb)\n",
        "                        loss = criterion(out, yb)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    # Evaluate\n",
        "                    model.eval()\n",
        "                    correct, total, total_loss = 0, 0, 0\n",
        "                    y_true, y_pred = [], []\n",
        "                    with torch.no_grad():\n",
        "                        for xb, yb in test_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct += (pred == yb).sum().item()\n",
        "                            total += yb.size(0)\n",
        "                            total_loss += loss.item() * xb.size(0)\n",
        "                            y_pred.extend(pred.cpu().numpy())\n",
        "                            y_true.extend(yb.cpu().numpy())\n",
        "                    acc = correct / total\n",
        "                    avg_loss = total_loss / total\n",
        "                    acc_list.append(acc)\n",
        "                    loss_list.append(avg_loss)\n",
        "                    prec_list.append(precision_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "                    rec_list.append(recall_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "                    f1_list.append(f1_score(y_true, y_pred, average='macro', zero_division=0))\n",
        "                    print(f\"Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "                t1 = time.time()\n",
        "\n",
        "                if acc_list[-1] >= 1.0:\n",
        "                    print(f\"‚ö†Ô∏è Skipped BS={bs} LR={lr} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                # ÂÑ≤Â≠òÊâÄÊúâÊõ≤Á∑öÂúñ\n",
        "                metrics = {\n",
        "                    \"accuracy\": acc_list,\n",
        "                    \"loss\": loss_list,\n",
        "                    \"precision\": prec_list,\n",
        "                    \"recall\": rec_list,\n",
        "                    \"f1_score\": f1_list\n",
        "                }\n",
        "                for name, values in metrics.items():\n",
        "                    plt.plot(range(1, num_epochs+1), values, marker='o')\n",
        "                    plt.title(f\"{name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\"); plt.ylabel(name.capitalize())\n",
        "                    if name != \"loss\": plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    folder = f\"{name}_curves\"\n",
        "                    fname = f\"bs{bs}_lr{lr}_seq{seq_len}.png\"\n",
        "                    plt.savefig(os.path.join(RESULT_DIR, folder, fname), bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(y_true, y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # ÂÑ≤Â≠òÁµêÊûú\n",
        "                results.append({\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': prec_list[-1], 'recall': rec_list[-1], 'f1_score': f1_list[-1],\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"cnn_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results (all accuracy = 1.0 were skipped)\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "tWrgapHBgxcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16]\n",
        "learning_rates = [1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ],
      "metadata": {
        "id": "D2P8TmkIijrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Â≠òÁµêÊûú"
      ],
      "metadata": {
        "id": "XkiW7K9GsDV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPORT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/new\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "_BreJ33FsC_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Ê®°ÂûãÊ∏ÖÂñÆÔºà‰æùÁÖß‰Ω†Ë®ìÁ∑¥ÈÅéÁöÑÊ®°ÂûãÂëΩÂêçÔºâ\n",
        "model_names = [\"LSTM\", \"GRU\", \"MLP\", \"SVM\", \"CNN\"]\n",
        "\n",
        "# ÊØèÁ®ÆÂúñË°®È°ûÂûãÔºàÂåÖÂê´Êñ∞Â¢ûÁöÑ precision Ëàá recallÔºâ\n",
        "curve_folders = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"precision_curves\",\n",
        "    \"recall_curves\",\n",
        "    \"f1_curves\"\n",
        "]\n",
        "\n",
        "# ÂåØÂá∫Ë∑ØÂæë\n",
        "EXPORT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/new\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "for model in model_names:\n",
        "    model_result_path = f\"./models/{model}/result\"\n",
        "    export_model_path = os.path.join(EXPORT_DIR, model)\n",
        "    os.makedirs(export_model_path, exist_ok=True)\n",
        "\n",
        "    # ÂåØÂá∫ÂØ¶È©ó CSV Ê™î\n",
        "    csv_name = f\"{model.lower()}_experiment_results.csv\"\n",
        "    csv_path = os.path.join(model_result_path, csv_name)\n",
        "    if os.path.exists(csv_path):\n",
        "        shutil.copy(csv_path, os.path.join(export_model_path, \"experiment_results.csv\"))\n",
        "\n",
        "    # ÂåØÂá∫ÂúñË°®Ë≥áÊñôÂ§æ\n",
        "    for folder in curve_folders:\n",
        "        src_folder = os.path.join(model_result_path, folder)\n",
        "        dst_folder = os.path.join(export_model_path, folder)\n",
        "        if os.path.exists(src_folder):\n",
        "            shutil.copytree(src_folder, dst_folder, dirs_exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ ÊâÄÊúâÊ®°ÂûãÁöÑÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥Ôºö\", EXPORT_DIR)"
      ],
      "metadata": {
        "id": "YDoyJ9JSsIAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}