{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/machine_learning_compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdeWQTkaMyj",
        "outputId": "89b12d99-6c20-4d15-860f-5b0972ba742d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 掛載 Google 雲端硬碟\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 資料集根目錄\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4/hardware\"\n",
        "\n",
        "# 各分類資料夾設定\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal\", \"wire_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal\", \"transformer_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal\", \"transformer_overheating\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBh_Hm3gvQA1"
      },
      "source": [
        "定義快取函數(三維)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oay5nopuvOx3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def generate_kfold_preprocessed_cache_3d(\n",
        "    seq_lens,\n",
        "    label_dirs,\n",
        "    k_folds=5,\n",
        "    cache_dir=\"/content/preprocessed_kfold\"\n",
        "):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        all_seq, all_labels = [], []\n",
        "\n",
        "        # === 讀取所有 CSV，切 chunk ===\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    try:\n",
        "                        data = df[[\"current\", \"voltage\", \"power\", \"temp_C\"]].values.astype(np.float32)\n",
        "                    except KeyError as e:\n",
        "                        print(f\"❌ 缺少欄位 {e}：{path}\")\n",
        "                        continue\n",
        "\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "                    all_seq.extend(chunks)\n",
        "                    all_labels.extend([label] * len(chunks))\n",
        "\n",
        "        seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "        labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "        print(f\"📊 Total sequences: {len(seq_arr)} | seq_len: {seq_len}\")\n",
        "\n",
        "        # === Stratified K-Fold ===\n",
        "        skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(seq_arr, labels_arr), 1):\n",
        "            X_train, X_val = seq_arr[train_idx], seq_arr[val_idx]\n",
        "            y_train, y_val = labels_arr[train_idx], labels_arr[val_idx]\n",
        "\n",
        "            # === Train-only Scaler ===\n",
        "            B, T, F = X_train.shape\n",
        "            train_reshaped = X_train.reshape(-1, F)\n",
        "            scaler = StandardScaler().fit(train_reshaped)\n",
        "            X_train_scaled = scaler.transform(train_reshaped).reshape(B, T, F)\n",
        "\n",
        "            Bv, Tv, Fv = X_val.shape\n",
        "            val_reshaped = X_val.reshape(-1, Fv)\n",
        "            X_val_scaled = scaler.transform(val_reshaped).reshape(Bv, Tv, Fv)\n",
        "\n",
        "            # === Save ===\n",
        "            np.save(os.path.join(cache_dir, f\"X_train_fold{fold_idx}_seq{seq_len}_3d.npy\"), X_train_scaled)\n",
        "            np.save(os.path.join(cache_dir, f\"y_train_fold{fold_idx}_seq{seq_len}_3d.npy\"), y_train)\n",
        "            np.save(os.path.join(cache_dir, f\"X_val_fold{fold_idx}_seq{seq_len}_3d.npy\"), X_val_scaled)\n",
        "            np.save(os.path.join(cache_dir, f\"y_val_fold{fold_idx}_seq{seq_len}_3d.npy\"), y_val)\n",
        "\n",
        "            print(f\"✅ Fold {fold_idx} done | train: {len(train_idx)} | val: {len(val_idx)}\")\n",
        "\n",
        "    print(\"\\n🎉 所有 K-Fold 3D 前處理完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-x-GLAh00T2"
      },
      "source": [
        "定義快取函數(二維)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naR3mpLp0zSh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def generate_kfold_preprocessed_cache_2d(\n",
        "    seq_lens,\n",
        "    label_dirs,\n",
        "    k_folds=5,\n",
        "    cache_dir=\"/content/preprocessed_kfold_2d\"\n",
        "):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        all_features, all_labels = [], []\n",
        "\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    try:\n",
        "                        data = df[[\"current\", \"voltage\", \"power\", \"temp_C\"]].values.astype(np.float32)\n",
        "                    except KeyError as e:\n",
        "                        print(f\"❌ 缺少欄位 {e}：{path}\")\n",
        "                        continue\n",
        "\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "\n",
        "                    for chunk in chunks:\n",
        "                        features = []\n",
        "                        features.extend(np.mean(chunk, axis=0))  # 4\n",
        "                        features.extend(np.std(chunk, axis=0))   # 4\n",
        "                        features.extend(np.max(chunk, axis=0))   # 4\n",
        "                        features.extend(np.min(chunk, axis=0))   # 4\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(label)\n",
        "\n",
        "        X = np.array(all_features, dtype=np.float32)\n",
        "        y = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "        print(f\"📊 Total samples: {len(X)} | seq_len: {seq_len} | feature dim: {X.shape[1]}\")\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "            X_train, X_val = X[train_idx], X[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "            scaler = StandardScaler().fit(X_train)\n",
        "            X_train_scaled = scaler.transform(X_train)\n",
        "            X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "            np.save(os.path.join(cache_dir, f\"X_train_fold{fold_idx}_seq{seq_len}_2d.npy\"), X_train_scaled)\n",
        "            np.save(os.path.join(cache_dir, f\"y_train_fold{fold_idx}_seq{seq_len}_2d.npy\"), y_train)\n",
        "            np.save(os.path.join(cache_dir, f\"X_val_fold{fold_idx}_seq{seq_len}_2d.npy\"), X_val_scaled)\n",
        "            np.save(os.path.join(cache_dir, f\"y_val_fold{fold_idx}_seq{seq_len}_2d.npy\"), y_val)\n",
        "\n",
        "            print(f\"✅ Fold {fold_idx} done | train: {len(train_idx)} | val: {len(val_idx)}\")\n",
        "\n",
        "    print(\"\\n🎉 所有 K-Fold 2D 前處理完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBeY_OhIvQ6o"
      },
      "source": [
        "實際執行快取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wad3ku5VvRNo",
        "outputId": "37bfec68-9dab-4a56-af0f-0a10f8953552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Total sequences: 2994 | seq_len: 10\n",
            "✅ Fold 1 done | train: 2395 | val: 599\n",
            "✅ Fold 2 done | train: 2395 | val: 599\n",
            "✅ Fold 3 done | train: 2395 | val: 599\n",
            "✅ Fold 4 done | train: 2395 | val: 599\n",
            "✅ Fold 5 done | train: 2396 | val: 598\n",
            "📊 Total sequences: 1494 | seq_len: 20\n",
            "✅ Fold 1 done | train: 1195 | val: 299\n",
            "✅ Fold 2 done | train: 1195 | val: 299\n",
            "✅ Fold 3 done | train: 1195 | val: 299\n",
            "✅ Fold 4 done | train: 1195 | val: 299\n",
            "✅ Fold 5 done | train: 1196 | val: 298\n",
            "📊 Total sequences: 995 | seq_len: 30\n",
            "✅ Fold 1 done | train: 796 | val: 199\n",
            "✅ Fold 2 done | train: 796 | val: 199\n",
            "✅ Fold 3 done | train: 796 | val: 199\n",
            "✅ Fold 4 done | train: 796 | val: 199\n",
            "✅ Fold 5 done | train: 796 | val: 199\n",
            "📊 Total sequences: 743 | seq_len: 40\n",
            "✅ Fold 1 done | train: 594 | val: 149\n",
            "✅ Fold 2 done | train: 594 | val: 149\n",
            "✅ Fold 3 done | train: 594 | val: 149\n",
            "✅ Fold 4 done | train: 595 | val: 148\n",
            "✅ Fold 5 done | train: 595 | val: 148\n",
            "\n",
            "🎉 所有 K-Fold 3D 前處理完成！\n",
            "📊 Total samples: 2994 | seq_len: 10 | feature dim: 16\n",
            "✅ Fold 1 done | train: 2395 | val: 599\n",
            "✅ Fold 2 done | train: 2395 | val: 599\n",
            "✅ Fold 3 done | train: 2395 | val: 599\n",
            "✅ Fold 4 done | train: 2395 | val: 599\n",
            "✅ Fold 5 done | train: 2396 | val: 598\n",
            "📊 Total samples: 1494 | seq_len: 20 | feature dim: 16\n",
            "✅ Fold 1 done | train: 1195 | val: 299\n",
            "✅ Fold 2 done | train: 1195 | val: 299\n",
            "✅ Fold 3 done | train: 1195 | val: 299\n",
            "✅ Fold 4 done | train: 1195 | val: 299\n",
            "✅ Fold 5 done | train: 1196 | val: 298\n",
            "📊 Total samples: 995 | seq_len: 30 | feature dim: 16\n",
            "✅ Fold 1 done | train: 796 | val: 199\n",
            "✅ Fold 2 done | train: 796 | val: 199\n",
            "✅ Fold 3 done | train: 796 | val: 199\n",
            "✅ Fold 4 done | train: 796 | val: 199\n",
            "✅ Fold 5 done | train: 796 | val: 199\n",
            "📊 Total samples: 743 | seq_len: 40 | feature dim: 16\n",
            "✅ Fold 1 done | train: 594 | val: 149\n",
            "✅ Fold 2 done | train: 594 | val: 149\n",
            "✅ Fold 3 done | train: 594 | val: 149\n",
            "✅ Fold 4 done | train: 595 | val: 148\n",
            "✅ Fold 5 done | train: 595 | val: 148\n",
            "\n",
            "🎉 所有 K-Fold 2D 前處理完成！\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    LABEL_DIRS = {\n",
        "        0: \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4/hardware/normal\",\n",
        "        1: \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4/hardware/abnormal/wire_rust\",\n",
        "        2: \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4/hardware/abnormal/transformer_rust\",\n",
        "        3: \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4/hardware/abnormal/transformer_overheating\",\n",
        "    }\n",
        "\n",
        "    SEQ_LENS = [10, 20, 30, 40]\n",
        "\n",
        "    # 產生 3D K-Fold\n",
        "    generate_kfold_preprocessed_cache_3d(\n",
        "        SEQ_LENS, LABEL_DIRS, k_folds=5, cache_dir=\"/content/preprocessed_kfold_3d\"\n",
        "    )\n",
        "\n",
        "    # 產生 2D K-Fold\n",
        "    generate_kfold_preprocessed_cache_2d(\n",
        "        SEQ_LENS, LABEL_DIRS, k_folds=5, cache_dir=\"/content/preprocessed_kfold_2d\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0wQZBWz5Uht"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qepGDca0Yfpm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === 資料夾與儲存路徑設定 ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/LSTM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === 自訂 Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === LSTM 模型 ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === 主訓練與超參數搜尋函數（使用快取） ===\n",
        "def train_and_search_lstm(batch_sizes, learning_rates, seq_lens,\n",
        "                          num_epochs=100, hidden_dim=64, num_layers=1,\n",
        "                          num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n🧪 BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = LSTMClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                           num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, num_epochs, bs, lr, seq_len):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # 純線條，沒有 marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", num_epochs, bs, lr, seq_len)\n",
        "\n",
        "                # 混淆矩陣\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\n📊 統計結果 (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  🔹 Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  🔹 F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ⏱️  Training Time  : {final_result['training_time_s']} 秒\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"lstm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n📄 All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\n🏆 Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n❗ No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWK53YBRYxJY"
      },
      "outputs": [],
      "source": [
        "# 設定訓練參數\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# 顯示總組數\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"🔍 總共訓練組數：{total_combinations}，每組訓練 {num_epochs} epochs\")\n",
        "\n",
        "# 執行網格搜尋訓練\n",
        "results, best = train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# 輸出最佳參數與指標\n",
        "if best is not None:\n",
        "    print(\"\\n🎯 最佳參數組合：\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")#\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} 秒\")\n",
        "else:\n",
        "    print(\"❗ 沒有有效結果（準確率全部為 1.0）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q3inndRoyYY"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMxndCGUhHqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === 資料夾與儲存路徑設定 ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_4/hardware/MLP\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === 自訂 Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === MLP 模型 ===\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === 訓練與儲存 ===\n",
        "def train_and_search_mlp(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=128,\n",
        "                         num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n🧪 BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "                input_dim = X.shape[1]\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = MLPClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, num_epochs, bs, lr, seq_len):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # 純線條\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", num_epochs, bs, lr, seq_len)\n",
        "\n",
        "\n",
        "                # 混淆矩陣\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\n📊 統計結果 (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  🔹 Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  🔹 F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ⏱️  Training Time  : {final_result['training_time_s']} 秒\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"mlp_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n📄 All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\n🏆 Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n❗ No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA1ydN20jMiw"
      },
      "outputs": [],
      "source": [
        "# 設定訓練參數\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# 顯示總組數\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"🔍 總共訓練組數：{total_combinations}，每組訓練 {num_epochs} epochs\")\n",
        "\n",
        "# 執行網格搜尋訓練\n",
        "results, best = train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# 輸出最佳參數與指標\n",
        "if best is not None:\n",
        "    print(\"\\n🎯 最佳參數組合：\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} 秒\")\n",
        "else:\n",
        "    print(\"❗ 沒有有效結果（準確率全部為 1.0）\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmAFC98o2iM"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxnqos1Eo4UC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        ")\n",
        "\n",
        "# === 路徑設定 ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_4/hardware/SVM\"\n",
        "PREPROCESSED_DIR = \"/content/preprocessed_kfold_2d\"\n",
        "\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "]\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === 改寫後的 SVM 訓練流程 ===\n",
        "def train_svm_search(kernels, Cs, seq_lens, k_folds=5):\n",
        "    results = []\n",
        "    best = None\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for C_val in Cs:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n🔎 Kernel={kernel} | C={C_val} | SEQ={seq_len}\")\n",
        "\n",
        "                acc_list, f1_list, all_y_true, all_y_pred = [], [], [], []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold_idx in range(1, k_folds + 1):\n",
        "                    X_train = np.load(os.path.join(PREPROCESSED_DIR, f\"X_train_fold{fold_idx}_seq{seq_len}_2d.npy\"))\n",
        "                    y_train = np.load(os.path.join(PREPROCESSED_DIR, f\"y_train_fold{fold_idx}_seq{seq_len}_2d.npy\"))\n",
        "                    X_val = np.load(os.path.join(PREPROCESSED_DIR, f\"X_val_fold{fold_idx}_seq{seq_len}_2d.npy\"))\n",
        "                    y_val = np.load(os.path.join(PREPROCESSED_DIR, f\"y_val_fold{fold_idx}_seq{seq_len}_2d.npy\"))\n",
        "\n",
        "                    clf = SVC(kernel=kernel, C=C_val)\n",
        "                    clf.fit(X_train, y_train)\n",
        "                    y_pred = clf.predict(X_val)\n",
        "\n",
        "                    acc = accuracy_score(y_val, y_pred)\n",
        "                    f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                    acc_list.append(acc)\n",
        "                    f1_list.append(f1)\n",
        "                    all_y_true.extend(y_val)\n",
        "                    all_y_pred.extend(y_pred)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                final_acc = np.mean(acc_list)\n",
        "                final_f1 = np.mean(f1_list)\n",
        "\n",
        "                if final_acc >= 1.0:\n",
        "                    print(f\"⚠️ Skipped Kernel={kernel} C={C_val} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                precision = precision_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                # === K-Fold Accuracy 曲線 ===\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds + 1), acc_list, label='Validation Accuracy')\n",
        "                plt.title(f\"K-fold Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"accuracy_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # === K-Fold F1-score 曲線 ===\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds + 1), f1_list, label='Validation F1-score', color='orange')\n",
        "                plt.title(f\"K-fold F1-score\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"F1-score\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"f1_score_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # === 訓練集準確率 ===\n",
        "                X_all = np.concatenate([\n",
        "                    np.load(os.path.join(PREPROCESSED_DIR, f\"X_train_fold{fold}_seq{seq_len}_2d.npy\"))\n",
        "                    for fold in range(1, k_folds + 1)\n",
        "                ])\n",
        "                y_all = np.concatenate([\n",
        "                    np.load(os.path.join(PREPROCESSED_DIR, f\"y_train_fold{fold}_seq{seq_len}_2d.npy\"))\n",
        "                    for fold in range(1, k_folds + 1)\n",
        "                ])\n",
        "                clf_full = SVC(kernel=kernel, C=C_val)\n",
        "                clf_full.fit(X_all, y_all)\n",
        "                train_pred = clf_full.predict(X_all)\n",
        "                train_acc = accuracy_score(y_all, train_pred)\n",
        "                plt.figure()\n",
        "                plt.bar(['train'], [train_acc])\n",
        "                plt.title(f\"Train Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"train_accuracy\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # === 混淆矩陣 ===\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_all))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                # === 儲存結果 ===\n",
        "                result = {\n",
        "                    'kernel': kernel, 'C': C_val, 'seq_len': seq_len,\n",
        "                    'final_acc': final_acc,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1_score': final_f1,\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "                if best is None or result['final_acc'] > best['final_acc']:\n",
        "                    best = result\n",
        "\n",
        "                print(f\"\\n📊 統計結果 (Kernel={kernel}, C={C_val}, SEQ={seq_len}):\")\n",
        "                print(f\"  🔹 Final Accuracy : {final_acc:.4f}\")\n",
        "                print(f\"  🔹 Precision      : {precision:.4f}\")\n",
        "                print(f\"  🔹 Recall         : {recall:.4f}\")\n",
        "                print(f\"  🔹 F1-score       : {final_f1:.4f}\")\n",
        "                print(f\"  ⏱️  Training Time  : {round(t1 - t0, 2)} 秒\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"svm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n📄 Results saved to {csv_path}\")\n",
        "    return results, best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB0a2PFxo8TZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7169a3ee-94f5-4dd9-e43b-37eea305d2cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔎 Kernel=rbf | C=1.0 | SEQ=10\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=1.0, SEQ=10):\n",
            "  🔹 Final Accuracy : 0.9021\n",
            "  🔹 Precision      : 0.9143\n",
            "  🔹 Recall         : 0.8985\n",
            "  🔹 F1-score       : 0.8971\n",
            "  ⏱️  Training Time  : 0.43 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=1.0 | SEQ=20\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=1.0, SEQ=20):\n",
            "  🔹 Final Accuracy : 0.8989\n",
            "  🔹 Precision      : 0.9012\n",
            "  🔹 Recall         : 0.8958\n",
            "  🔹 F1-score       : 0.8947\n",
            "  ⏱️  Training Time  : 0.14 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=1.0 | SEQ=30\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=1.0, SEQ=30):\n",
            "  🔹 Final Accuracy : 0.9025\n",
            "  🔹 Precision      : 0.9022\n",
            "  🔹 Recall         : 0.8999\n",
            "  🔹 F1-score       : 0.9000\n",
            "  ⏱️  Training Time  : 0.09 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=1.0 | SEQ=40\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=1.0, SEQ=40):\n",
            "  🔹 Final Accuracy : 0.9125\n",
            "  🔹 Precision      : 0.9125\n",
            "  🔹 Recall         : 0.9100\n",
            "  🔹 F1-score       : 0.9101\n",
            "  ⏱️  Training Time  : 0.05 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=10.0 | SEQ=10\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=10.0, SEQ=10):\n",
            "  🔹 Final Accuracy : 0.9325\n",
            "  🔹 Precision      : 0.9351\n",
            "  🔹 Recall         : 0.9303\n",
            "  🔹 F1-score       : 0.9304\n",
            "  ⏱️  Training Time  : 0.36 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=10.0 | SEQ=20\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=10.0, SEQ=20):\n",
            "  🔹 Final Accuracy : 0.9358\n",
            "  🔹 Precision      : 0.9368\n",
            "  🔹 Recall         : 0.9338\n",
            "  🔹 F1-score       : 0.9338\n",
            "  ⏱️  Training Time  : 0.24 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=10.0 | SEQ=30\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=10.0, SEQ=30):\n",
            "  🔹 Final Accuracy : 0.9186\n",
            "  🔹 Precision      : 0.9173\n",
            "  🔹 Recall         : 0.9166\n",
            "  🔹 F1-score       : 0.9166\n",
            "  ⏱️  Training Time  : 0.12 秒\n",
            "\n",
            "🔎 Kernel=rbf | C=10.0 | SEQ=40\n",
            "\n",
            "📊 統計結果 (Kernel=rbf, C=10.0, SEQ=40):\n",
            "  🔹 Final Accuracy : 0.9273\n",
            "  🔹 Precision      : 0.9268\n",
            "  🔹 Recall         : 0.9254\n",
            "  🔹 F1-score       : 0.9256\n",
            "  ⏱️  Training Time  : 0.05 秒\n",
            "\n",
            "🔎 Kernel=linear | C=1.0 | SEQ=10\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=1.0, SEQ=10):\n",
            "  🔹 Final Accuracy : 0.8778\n",
            "  🔹 Precision      : 0.8775\n",
            "  🔹 Recall         : 0.8742\n",
            "  🔹 F1-score       : 0.8737\n",
            "  ⏱️  Training Time  : 0.37 秒\n",
            "\n",
            "🔎 Kernel=linear | C=1.0 | SEQ=20\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=1.0, SEQ=20):\n",
            "  🔹 Final Accuracy : 0.8735\n",
            "  🔹 Precision      : 0.8710\n",
            "  🔹 Recall         : 0.8703\n",
            "  🔹 F1-score       : 0.8696\n",
            "  ⏱️  Training Time  : 0.12 秒\n",
            "\n",
            "🔎 Kernel=linear | C=1.0 | SEQ=30\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=1.0, SEQ=30):\n",
            "  🔹 Final Accuracy : 0.8965\n",
            "  🔹 Precision      : 0.8937\n",
            "  🔹 Recall         : 0.8940\n",
            "  🔹 F1-score       : 0.8937\n",
            "  ⏱️  Training Time  : 0.07 秒\n",
            "\n",
            "🔎 Kernel=linear | C=1.0 | SEQ=40\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=1.0, SEQ=40):\n",
            "  🔹 Final Accuracy : 0.9044\n",
            "  🔹 Precision      : 0.9022\n",
            "  🔹 Recall         : 0.9022\n",
            "  🔹 F1-score       : 0.9021\n",
            "  ⏱️  Training Time  : 0.05 秒\n",
            "\n",
            "🔎 Kernel=linear | C=10.0 | SEQ=10\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=10.0, SEQ=10):\n",
            "  🔹 Final Accuracy : 0.8771\n",
            "  🔹 Precision      : 0.8769\n",
            "  🔹 Recall         : 0.8735\n",
            "  🔹 F1-score       : 0.8731\n",
            "  ⏱️  Training Time  : 0.73 秒\n",
            "\n",
            "🔎 Kernel=linear | C=10.0 | SEQ=20\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=10.0, SEQ=20):\n",
            "  🔹 Final Accuracy : 0.8762\n",
            "  🔹 Precision      : 0.8739\n",
            "  🔹 Recall         : 0.8729\n",
            "  🔹 F1-score       : 0.8724\n",
            "  ⏱️  Training Time  : 0.24 秒\n",
            "\n",
            "🔎 Kernel=linear | C=10.0 | SEQ=30\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=10.0, SEQ=30):\n",
            "  🔹 Final Accuracy : 0.9035\n",
            "  🔹 Precision      : 0.9009\n",
            "  🔹 Recall         : 0.9013\n",
            "  🔹 F1-score       : 0.9010\n",
            "  ⏱️  Training Time  : 0.12 秒\n",
            "\n",
            "🔎 Kernel=linear | C=10.0 | SEQ=40\n",
            "\n",
            "📊 統計結果 (Kernel=linear, C=10.0, SEQ=40):\n",
            "  🔹 Final Accuracy : 0.9058\n",
            "  🔹 Precision      : 0.9035\n",
            "  🔹 Recall         : 0.9035\n",
            "  🔹 F1-score       : 0.9034\n",
            "  ⏱️  Training Time  : 0.09 秒\n",
            "\n",
            "📄 Results saved to /content/drive/MyDrive/Colab Notebooks/test/feature dim_4/hardware/SVM/svm_experiment_results.csv\n",
            "\n",
            "🎯 最佳參數組合：\n",
            "Sequence Length = 20\n",
            "C = 10.0\n",
            "Kernel = rbf\n",
            "Accuracy = 0.9358\n",
            "Precision = 0.9368\n",
            "Recall = 0.9338\n",
            "F1-score = 0.9338\n",
            "Training Time = 0.24 秒\n"
          ]
        }
      ],
      "source": [
        "seq_lens = [10, 20, 30, 40]\n",
        "C_list = [1.0, 10.0]\n",
        "kernel_list = [\"rbf\", \"linear\"]\n",
        "\n",
        "results, best = train_svm_search(kernel_list, C_list, seq_lens)\n",
        "\n",
        "print(\"\\n🎯 最佳參數組合：\")\n",
        "print(f\"Sequence Length = {best['seq_len']}\")\n",
        "print(f\"C = {best['C']}\")\n",
        "print(f\"Kernel = {best['kernel']}\")\n",
        "print(f\"Accuracy = {best['final_acc']:.4f}\")\n",
        "print(f\"Precision = {best['precision']:.4f}\")\n",
        "print(f\"Recall = {best['recall']:.4f}\")\n",
        "print(f\"F1-score = {best['f1_score']:.4f}\")\n",
        "print(f\"Training Time = {best['training_time_s']} 秒\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVLlUCDrD_2"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVsy2pBvrFfh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === 資料夾與儲存路徑設定 ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/GRU\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === 自訂 Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === GRU 模型 ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === 訓練與儲存 ===\n",
        "def train_and_search_gru(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n🧪 BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X = np.load(os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\"))\n",
        "                y = np.load(os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\"))\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = GRUClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        correct_train, total_train, total_loss_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train_list.append(correct_train / total_train)\n",
        "                        loss_train_list.append(total_loss_train / total_train)\n",
        "\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc_list.append(correct / total)\n",
        "                        loss_list.append(total_loss / total)\n",
        "                        f1_list.append(f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0))\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc_list[-1]:.4f} | Loss: {loss_list[-1]:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # 純線條，無 marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\n📊 統計結果 (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  🔹 Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  🔹 F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ⏱️  Training Time  : {final_result['training_time_s']} 秒\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"gru_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n📄 All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\n🏆 Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n❗ No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVWAGumMrPmo"
      },
      "outputs": [],
      "source": [
        "# 設定訓練參數\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# 顯示總組數\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"🔍 總共訓練組數：{total_combinations}，每組訓練 {num_epochs} epochs\")\n",
        "\n",
        "# 執行網格搜尋訓練\n",
        "results, best = train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# 輸出最佳參數與指標\n",
        "if best is not None:\n",
        "    print(\"\\n🎯 最佳參數組合：\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} 秒\")\n",
        "else:\n",
        "    print(\"❗ 沒有有效結果（準確率全部為 1.0）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKn_cKbgu0O"
      },
      "source": [
        "1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWrgapHBgxcw"
      },
      "outputs": [],
      "source": [
        "# === CNN 模型完整訓練程式 ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === 資料夾與儲存路徑設定 ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/CNN\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === 自訂 Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === CNN 模型 ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === 主訓練與超參數搜尋函數（使用快取） ===\n",
        "def train_and_search_cnn(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n🧪 BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = CNNClassifier(num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # 無 marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\n📊 統計結果 (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  🔹 Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  🔹 F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ⏱️  Training Time  : {final_result['training_time_s']} 秒\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"cnn_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n📄 All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\n🏆 Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n❗ No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2P8TmkIijrl"
      },
      "outputs": [],
      "source": [
        "# 設定訓練參數\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# 顯示總組數\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"🔍 總共訓練組數：{total_combinations}，每組訓練 {num_epochs} epochs\")\n",
        "\n",
        "# 執行網格搜尋訓練\n",
        "results, best = train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# 輸出最佳參數與指標\n",
        "if best is not None:\n",
        "    print(\"\\n🎯 最佳參數組合：\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} 秒\")\n",
        "else:\n",
        "    print(\"❗ 沒有有效結果（準確率全部為 1.0）\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TimesNet"
      ],
      "metadata": {
        "id": "MIiGTNGfpQn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === 資料夾與儲存路徑設定 ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === 自訂 Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "# === TimesNet 模型 ===\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        # 殘差 skip connection 處理\n",
        "        if input_dim != hidden_dim:\n",
        "            self.skip_conv = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=1)\n",
        "        else:\n",
        "            self.skip_conv = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x.transpose(1, 2)  # (batch, input_dim, seq_len)\n",
        "\n",
        "        x = residual\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.skip_conv is not None:\n",
        "            residual = self.skip_conv(residual)\n",
        "\n",
        "        x = x + residual  # 殘差加法\n",
        "        x = x.transpose(1, 2)  # 回到 (batch, seq_len, hidden_dim)\n",
        "        return x\n",
        "\n",
        "class TimesNetClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([TimesBlock(input_dim if i==0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)  # 不要再加殘差了，block 內部已處理殘差\n",
        "        x = x.transpose(1, 2)  # (batch, hidden_dim, seq_len)\n",
        "        x = self.pool(x).squeeze(-1)  # (batch, hidden_dim)\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "# === 主訓練與超參數搜尋函數 ===\n",
        "def train_and_search_timesnet(batch_sizes, learning_rates, seq_lens,\n",
        "                              num_epochs=100, hidden_dim=64, num_layers=2,\n",
        "                              num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    RESULTS_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet\"\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    SUB_DIRS = [\n",
        "        \"accuracy_curves\", \"loss_curves\", \"f1_score_curves\",\n",
        "        \"confusion_matrices\", \"train_accuracy\", \"train_loss\"\n",
        "    ]\n",
        "    for sub in SUB_DIRS:\n",
        "        os.makedirs(os.path.join(RESULTS_DIR, sub), exist_ok=True)\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\n🧪 BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = TimesNetClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                               num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # 無 marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                # 混淆矩陣\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\n📊 統計結果 (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  🔹 Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  🔹 F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ⏱️  Training Time  : {final_result['training_time_s']} 秒\")\n",
        "\n",
        "    csv_path = os.path.join(RESULTS_DIR, \"timesnet_experiment_results.csv\")\n",
        "\n",
        "    # 如果原本有檔案，先讀進來舊的\n",
        "    if os.path.exists(csv_path):\n",
        "        df_old = pd.read_csv(csv_path)\n",
        "        df_new = pd.DataFrame(results)\n",
        "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
        "    else:\n",
        "        df_combined = pd.DataFrame(results)\n",
        "\n",
        "    # 儲存合併後的新 dataframe\n",
        "    df_combined.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n📄 All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df_combined.empty:\n",
        "        best = df_combined.loc[df_combined['final_acc'].idxmax()]\n",
        "        print(f\"\\n🏆 Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n❗ No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "QXdg-w6HpQTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定訓練參數\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# 顯示總組數\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"🔍 總共訓練組數：{total_combinations}，每組訓練 {num_epochs} epochs\")\n",
        "\n",
        "# 執行網格搜尋訓練\n",
        "results, best = train_and_search_timesnet(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# 輸出最佳參數與指標\n",
        "if best is not None:\n",
        "    print(\"\\n🎯 最佳參數組合：\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} 秒\")\n",
        "else:\n",
        "    print(\"❗ 沒有有效結果（準確率全部為 1.0）\")"
      ],
      "metadata": {
        "id": "KO5scvlppmiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJiKI8F3IIxzhCDMLZb2EH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}