{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xin-kai08/Machine-Learning-Models/blob/main/machine_learning_compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUdeWQTkaMyj",
        "outputId": "b0d5207f-b885-400b-9304-74aa1c9c35f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# ÊéõËºâ Google Èõ≤Á´ØÁ°¨Á¢ü\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ë≥áÊñôÈõÜÊ†πÁõÆÈåÑ\n",
        "BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset/feature dim_4\"\n",
        "\n",
        "# ÂêÑÂàÜÈ°ûË≥áÊñôÂ§æË®≠ÂÆö\n",
        "LABEL_DIRS = {\n",
        "    0: os.path.join(BASE_PATH, \"normal\"),\n",
        "    1: os.path.join(BASE_PATH, \"abnormal/transformer_rust\"),\n",
        "    2: os.path.join(BASE_PATH, \"abnormal/wire_rust\"),\n",
        "    3: os.path.join(BASE_PATH, \"abnormal/wire_peeling\"),\n",
        "    4: os.path.join(BASE_PATH, \"abnormal/wire_bending\"),\n",
        "    5: os.path.join(BASE_PATH, \"abnormal/phone_overheating\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBh_Hm3gvQA1"
      },
      "source": [
        "ÂÆöÁæ©Âø´ÂèñÂáΩÊï∏(‰∏âÁ∂≠)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oay5nopuvOx3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def generate_preprocessed_cache_3d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_3d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_3d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"üì• Â∑≤Â≠òÂú® 3D Âø´ÂèñÔºöseq_len={seq_len}ÔºåÁï•ÈÅé\")\n",
        "            continue\n",
        "\n",
        "        all_seq, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\", \"temp_C\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "                    all_seq.extend(chunks)\n",
        "                    all_labels.extend([label] * len(chunks))\n",
        "\n",
        "        seq_arr = np.array(all_seq, dtype=np.float32)\n",
        "        labels_arr = np.array(all_labels, dtype=np.int64)\n",
        "        B, T, F = seq_arr.shape\n",
        "        reshaped = seq_arr.reshape(-1, F)\n",
        "        scaled = StandardScaler().fit_transform(reshaped).reshape(B, T, F)\n",
        "\n",
        "        np.save(cache_X, scaled)\n",
        "        np.save(cache_y, labels_arr)\n",
        "        print(f\"‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len={seq_len}ÔºàÂÖ± {B} Á≠ÜÂ∫èÂàóÔºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-x-GLAh00T2"
      },
      "source": [
        "ÂÆöÁæ©Âø´ÂèñÂáΩÊï∏(‰∫åÁ∂≠)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naR3mpLp0zSh"
      },
      "outputs": [],
      "source": [
        "def generate_preprocessed_cache_2d(seq_lens, label_dirs, cache_dir=\"/content/preprocessed\"):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    for seq_len in seq_lens:\n",
        "        cache_X = os.path.join(cache_dir, f\"X_seq{seq_len}_2d.npy\")\n",
        "        cache_y = os.path.join(cache_dir, f\"y_seq{seq_len}_2d.npy\")\n",
        "\n",
        "        if os.path.exists(cache_X) and os.path.exists(cache_y):\n",
        "            print(f\"üì• Â∑≤Â≠òÂú® 2D Âø´ÂèñÔºöseq_len={seq_len}ÔºåÁï•ÈÅé\")\n",
        "            continue\n",
        "\n",
        "        all_features, all_labels = [], []\n",
        "        for label, folder in label_dirs.items():\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.endswith(\".csv\"):\n",
        "                    path = os.path.join(folder, fname)\n",
        "                    df = pd.read_csv(path)\n",
        "                    data = df[[\"voltage\", \"current\", \"power\", \"temp_C\"]].values.astype(np.float32)\n",
        "                    num_chunks = len(data) // seq_len\n",
        "                    chunks = [data[i * seq_len : (i + 1) * seq_len] for i in range(num_chunks)]\n",
        "\n",
        "                    for chunk in chunks:\n",
        "                        features = []\n",
        "                        features.extend(np.mean(chunk, axis=0))  # 3\n",
        "                        features.extend(np.std(chunk, axis=0))   # 3\n",
        "                        features.extend(np.max(chunk, axis=0))   # 3\n",
        "                        features.extend(np.min(chunk, axis=0))   # 3\n",
        "                        all_features.append(features)\n",
        "                        all_labels.append(label)\n",
        "\n",
        "        X = np.array(all_features, dtype=np.float32)\n",
        "        y = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        np.save(cache_X, X_scaled)\n",
        "        np.save(cache_y, y)\n",
        "        print(f\"‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len={seq_len}ÔºàÂÖ± {len(X_scaled)} Á≠ÜÔºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBeY_OhIvQ6o"
      },
      "source": [
        "ÂØ¶ÈöõÂü∑Ë°åÂø´Âèñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wad3ku5VvRNo",
        "outputId": "e877dd03-1f31-42e7-d054-d04d541e7d5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=4ÔºàÂÖ± 15458 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=8ÔºàÂÖ± 7723 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=10ÔºàÂÖ± 6172 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=20ÔºàÂÖ± 3077 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=30ÔºàÂÖ± 2047 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 3D Âø´ÂèñÔºöseq_len=40ÔºàÂÖ± 1530 Á≠ÜÂ∫èÂàóÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=4ÔºàÂÖ± 15458 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=8ÔºàÂÖ± 7723 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=10ÔºàÂÖ± 6172 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=20ÔºàÂÖ± 3077 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=30ÔºàÂÖ± 2047 Á≠ÜÔºâ\n",
            "‚úÖ ÂÆåÊàê 2D Âø´ÂèñÔºöseq_len=40ÔºàÂÖ± 1530 Á≠ÜÔºâ\n"
          ]
        }
      ],
      "source": [
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "generate_preprocessed_cache_3d(seq_lens, LABEL_DIRS)      # Áî¢Áîü 3D\n",
        "generate_preprocessed_cache_2d(seq_lens, LABEL_DIRS)   # Áî¢Áîü 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0wQZBWz5Uht"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qepGDca0Yfpm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/LSTM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === LSTM Ê®°Âûã ===\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === ‰∏ªË®ìÁ∑¥ËàáË∂ÖÂèÉÊï∏ÊêúÂ∞ãÂáΩÊï∏Ôºà‰ΩøÁî®Âø´ÂèñÔºâ ===\n",
        "def train_and_search_lstm(batch_sizes, learning_rates, seq_lens,\n",
        "                          num_epochs=100, hidden_dim=64, num_layers=1,\n",
        "                          num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = LSTMClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                           num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, num_epochs, bs, lr, seq_len):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # Á¥îÁ∑öÊ¢ùÔºåÊ≤íÊúâ marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", num_epochs, bs, lr, seq_len)\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"lstm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWK53YBRYxJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37055518-ae28-4103-be92-f1f7574a1ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö72ÔºåÊØèÁµÑË®ìÁ∑¥ 100 epochs\n",
            "\n",
            "üß™ BS=4 | LR=0.01 | SEQ=4\n",
            "    [Fold 1] Epoch 10/100 | Acc: 0.9812 | Loss: 0.0769\n",
            "    [Fold 1] Epoch 20/100 | Acc: 0.9867 | Loss: 0.0628\n",
            "    [Fold 1] Epoch 30/100 | Acc: 0.9891 | Loss: 0.0412\n",
            "    [Fold 1] Epoch 40/100 | Acc: 0.9901 | Loss: 0.0517\n",
            "    [Fold 1] Epoch 50/100 | Acc: 0.9881 | Loss: 0.0517\n",
            "    [Fold 1] Epoch 60/100 | Acc: 0.9876 | Loss: 0.0578\n",
            "    [Fold 1] Epoch 70/100 | Acc: 0.9852 | Loss: 0.0686\n",
            "    [Fold 1] Epoch 80/100 | Acc: 0.9906 | Loss: 0.0323\n",
            "    [Fold 1] Epoch 90/100 | Acc: 0.9896 | Loss: 0.0486\n",
            "    [Fold 1] Epoch 100/100 | Acc: 0.9876 | Loss: 0.0590\n",
            "    [Fold 2] Epoch 10/100 | Acc: 0.9916 | Loss: 0.0390\n",
            "    [Fold 2] Epoch 20/100 | Acc: 0.9921 | Loss: 0.0393\n",
            "    [Fold 2] Epoch 30/100 | Acc: 0.9886 | Loss: 0.0496\n",
            "    [Fold 2] Epoch 40/100 | Acc: 0.9886 | Loss: 0.0497\n",
            "    [Fold 2] Epoch 50/100 | Acc: 0.9941 | Loss: 0.0321\n",
            "    [Fold 2] Epoch 60/100 | Acc: 0.9822 | Loss: 0.0693\n",
            "    [Fold 2] Epoch 70/100 | Acc: 0.9916 | Loss: 0.0421\n",
            "    [Fold 2] Epoch 80/100 | Acc: 0.9916 | Loss: 0.0439\n",
            "    [Fold 2] Epoch 90/100 | Acc: 0.9901 | Loss: 0.0807\n",
            "    [Fold 2] Epoch 100/100 | Acc: 0.9876 | Loss: 0.0581\n",
            "    [Fold 3] Epoch 10/100 | Acc: 0.9852 | Loss: 0.0490\n",
            "    [Fold 3] Epoch 20/100 | Acc: 0.9886 | Loss: 0.0372\n",
            "    [Fold 3] Epoch 30/100 | Acc: 0.9901 | Loss: 0.0371\n",
            "    [Fold 3] Epoch 40/100 | Acc: 0.9896 | Loss: 0.0475\n",
            "    [Fold 3] Epoch 50/100 | Acc: 0.9911 | Loss: 0.0324\n",
            "    [Fold 3] Epoch 60/100 | Acc: 0.9886 | Loss: 0.0448\n",
            "    [Fold 3] Epoch 70/100 | Acc: 0.9911 | Loss: 0.0430\n",
            "    [Fold 3] Epoch 80/100 | Acc: 0.9891 | Loss: 0.0440\n",
            "    [Fold 3] Epoch 90/100 | Acc: 0.9906 | Loss: 0.0302\n",
            "    [Fold 3] Epoch 100/100 | Acc: 0.9886 | Loss: 0.0594\n",
            "    [Fold 4] Epoch 10/100 | Acc: 0.9827 | Loss: 0.0499\n",
            "    [Fold 4] Epoch 20/100 | Acc: 0.9896 | Loss: 0.0407\n",
            "    [Fold 4] Epoch 30/100 | Acc: 0.9921 | Loss: 0.0345\n",
            "    [Fold 4] Epoch 40/100 | Acc: 0.9921 | Loss: 0.0675\n",
            "    [Fold 4] Epoch 50/100 | Acc: 0.9862 | Loss: 0.0666\n",
            "    [Fold 4] Epoch 60/100 | Acc: 0.9911 | Loss: 0.0478\n",
            "    [Fold 4] Epoch 70/100 | Acc: 0.9921 | Loss: 0.0569\n",
            "    [Fold 4] Epoch 80/100 | Acc: 0.9911 | Loss: 0.0688\n",
            "    [Fold 4] Epoch 90/100 | Acc: 0.9911 | Loss: 0.0772\n",
            "    [Fold 4] Epoch 100/100 | Acc: 0.9921 | Loss: 0.0762\n",
            "    [Fold 5] Epoch 10/100 | Acc: 0.9852 | Loss: 0.0581\n",
            "    [Fold 5] Epoch 20/100 | Acc: 0.9857 | Loss: 0.0550\n",
            "    [Fold 5] Epoch 30/100 | Acc: 0.9867 | Loss: 0.0554\n",
            "    [Fold 5] Epoch 40/100 | Acc: 0.9876 | Loss: 0.0675\n",
            "    [Fold 5] Epoch 50/100 | Acc: 0.9862 | Loss: 0.0632\n",
            "    [Fold 5] Epoch 60/100 | Acc: 0.9862 | Loss: 0.0709\n",
            "    [Fold 5] Epoch 70/100 | Acc: 0.9847 | Loss: 0.1056\n",
            "    [Fold 5] Epoch 80/100 | Acc: 0.9832 | Loss: 0.0660\n",
            "    [Fold 5] Epoch 90/100 | Acc: 0.9857 | Loss: 0.0866\n",
            "    [Fold 5] Epoch 100/100 | Acc: 0.9867 | Loss: 0.1163\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=4, LR=0.01, SEQ=4):\n",
            "  üîπ Final Accuracy : 0.9867\n",
            "  üîπ F1-score       : 0.9853\n",
            "  ‚è±Ô∏è  Training Time  : 2839.97 Áßí\n",
            "\n",
            "üß™ BS=4 | LR=0.01 | SEQ=8\n",
            "    [Fold 1] Epoch 10/100 | Acc: 0.9881 | Loss: 0.0587\n",
            "    [Fold 1] Epoch 20/100 | Acc: 0.9861 | Loss: 0.0429\n",
            "    [Fold 1] Epoch 30/100 | Acc: 0.9842 | Loss: 0.0505\n",
            "    [Fold 1] Epoch 40/100 | Acc: 0.9871 | Loss: 0.0688\n",
            "    [Fold 1] Epoch 50/100 | Acc: 0.9891 | Loss: 0.0398\n",
            "    [Fold 1] Epoch 60/100 | Acc: 0.9911 | Loss: 0.0744\n",
            "    [Fold 1] Epoch 70/100 | Acc: 0.9911 | Loss: 0.0415\n",
            "    [Fold 1] Epoch 80/100 | Acc: 0.9891 | Loss: 0.0473\n",
            "    [Fold 1] Epoch 90/100 | Acc: 0.9901 | Loss: 0.0385\n",
            "    [Fold 1] Epoch 100/100 | Acc: 0.9901 | Loss: 0.0444\n",
            "    [Fold 2] Epoch 10/100 | Acc: 0.9950 | Loss: 0.0254\n",
            "    [Fold 2] Epoch 20/100 | Acc: 0.9941 | Loss: 0.0254\n",
            "    [Fold 2] Epoch 30/100 | Acc: 0.9911 | Loss: 0.0342\n",
            "    [Fold 2] Epoch 40/100 | Acc: 0.9921 | Loss: 0.0240\n",
            "    [Fold 2] Epoch 50/100 | Acc: 0.9950 | Loss: 0.0293\n",
            "    [Fold 2] Epoch 60/100 | Acc: 0.9941 | Loss: 0.0391\n",
            "    [Fold 2] Epoch 70/100 | Acc: 0.9931 | Loss: 0.0330\n",
            "    [Fold 2] Epoch 80/100 | Acc: 0.9950 | Loss: 0.0315\n",
            "    [Fold 2] Epoch 90/100 | Acc: 0.9931 | Loss: 0.0332\n",
            "    [Fold 2] Epoch 100/100 | Acc: 0.9950 | Loss: 0.0383\n",
            "    [Fold 3] Epoch 10/100 | Acc: 0.9663 | Loss: 0.1779\n",
            "    [Fold 3] Epoch 20/100 | Acc: 0.9812 | Loss: 0.0737\n",
            "    [Fold 3] Epoch 30/100 | Acc: 0.9822 | Loss: 0.0595\n",
            "    [Fold 3] Epoch 40/100 | Acc: 0.9841 | Loss: 0.0808\n",
            "    [Fold 3] Epoch 50/100 | Acc: 0.9871 | Loss: 0.0549\n",
            "    [Fold 3] Epoch 60/100 | Acc: 0.9841 | Loss: 0.0737\n",
            "    [Fold 3] Epoch 70/100 | Acc: 0.9871 | Loss: 0.0524\n",
            "    [Fold 3] Epoch 80/100 | Acc: 0.9792 | Loss: 0.1047\n",
            "    [Fold 3] Epoch 90/100 | Acc: 0.9871 | Loss: 0.0858\n",
            "    [Fold 3] Epoch 100/100 | Acc: 0.9832 | Loss: 0.1328\n",
            "    [Fold 4] Epoch 10/100 | Acc: 0.9822 | Loss: 0.0447\n",
            "    [Fold 4] Epoch 20/100 | Acc: 0.9960 | Loss: 0.0199\n",
            "    [Fold 4] Epoch 30/100 | Acc: 0.9980 | Loss: 0.0157\n",
            "    [Fold 4] Epoch 40/100 | Acc: 0.9941 | Loss: 0.0139\n",
            "    [Fold 4] Epoch 50/100 | Acc: 0.9980 | Loss: 0.0071\n",
            "    [Fold 4] Epoch 60/100 | Acc: 0.9931 | Loss: 0.0135\n",
            "    [Fold 4] Epoch 70/100 | Acc: 0.9901 | Loss: 0.0249\n",
            "    [Fold 4] Epoch 80/100 | Acc: 0.9960 | Loss: 0.0096\n",
            "    [Fold 4] Epoch 90/100 | Acc: 0.9970 | Loss: 0.0204\n",
            "    [Fold 4] Epoch 100/100 | Acc: 0.9980 | Loss: 0.0089\n",
            "    [Fold 5] Epoch 10/100 | Acc: 0.9822 | Loss: 0.0615\n",
            "    [Fold 5] Epoch 20/100 | Acc: 0.9891 | Loss: 0.0396\n",
            "    [Fold 5] Epoch 30/100 | Acc: 0.9911 | Loss: 0.0512\n",
            "    [Fold 5] Epoch 40/100 | Acc: 0.9901 | Loss: 0.0601\n",
            "    [Fold 5] Epoch 50/100 | Acc: 0.9911 | Loss: 0.0612\n",
            "    [Fold 5] Epoch 60/100 | Acc: 0.9881 | Loss: 0.0518\n",
            "    [Fold 5] Epoch 70/100 | Acc: 0.9911 | Loss: 0.0658\n",
            "    [Fold 5] Epoch 80/100 | Acc: 0.9901 | Loss: 0.0671\n",
            "    [Fold 5] Epoch 90/100 | Acc: 0.9861 | Loss: 0.0575\n",
            "    [Fold 5] Epoch 100/100 | Acc: 0.9891 | Loss: 0.0634\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=4, LR=0.01, SEQ=8):\n",
            "  üîπ Final Accuracy : 0.9891\n",
            "  üîπ F1-score       : 0.9883\n",
            "  ‚è±Ô∏è  Training Time  : 1474.31 Áßí\n",
            "\n",
            "üß™ BS=4 | LR=0.01 | SEQ=10\n",
            "    [Fold 1] Epoch 10/100 | Acc: 0.9593 | Loss: 0.1605\n",
            "    [Fold 1] Epoch 20/100 | Acc: 0.9901 | Loss: 0.0374\n",
            "    [Fold 1] Epoch 30/100 | Acc: 0.9889 | Loss: 0.0586\n",
            "    [Fold 1] Epoch 40/100 | Acc: 0.9889 | Loss: 0.0360\n",
            "    [Fold 1] Epoch 50/100 | Acc: 0.9914 | Loss: 0.0453\n",
            "    [Fold 1] Epoch 60/100 | Acc: 0.9901 | Loss: 0.0847\n",
            "    [Fold 1] Epoch 70/100 | Acc: 0.9889 | Loss: 0.0487\n",
            "    [Fold 1] Epoch 80/100 | Acc: 0.9914 | Loss: 0.0572\n",
            "    [Fold 1] Epoch 90/100 | Acc: 0.9901 | Loss: 0.0600\n",
            "    [Fold 1] Epoch 100/100 | Acc: 0.9901 | Loss: 0.0496\n",
            "    [Fold 2] Epoch 10/100 | Acc: 0.9778 | Loss: 0.0597\n",
            "    [Fold 2] Epoch 20/100 | Acc: 0.9840 | Loss: 0.0559\n",
            "    [Fold 2] Epoch 30/100 | Acc: 0.9852 | Loss: 0.0458\n",
            "    [Fold 2] Epoch 40/100 | Acc: 0.9864 | Loss: 0.0405\n",
            "    [Fold 2] Epoch 50/100 | Acc: 0.9790 | Loss: 0.0599\n",
            "    [Fold 2] Epoch 60/100 | Acc: 0.9901 | Loss: 0.0515\n",
            "    [Fold 2] Epoch 70/100 | Acc: 0.9889 | Loss: 0.0338\n",
            "    [Fold 2] Epoch 80/100 | Acc: 0.9877 | Loss: 0.0452\n",
            "    [Fold 2] Epoch 90/100 | Acc: 0.9864 | Loss: 0.0491\n",
            "    [Fold 2] Epoch 100/100 | Acc: 0.9790 | Loss: 0.0576\n",
            "    [Fold 3] Epoch 10/100 | Acc: 0.9765 | Loss: 0.0700\n",
            "    [Fold 3] Epoch 20/100 | Acc: 0.9852 | Loss: 0.0565\n",
            "    [Fold 3] Epoch 30/100 | Acc: 0.9802 | Loss: 0.1039\n",
            "    [Fold 3] Epoch 40/100 | Acc: 0.9864 | Loss: 0.0558\n",
            "    [Fold 3] Epoch 50/100 | Acc: 0.9901 | Loss: 0.0734\n",
            "    [Fold 3] Epoch 60/100 | Acc: 0.9901 | Loss: 0.0650\n",
            "    [Fold 3] Epoch 70/100 | Acc: 0.9889 | Loss: 0.0612\n",
            "    [Fold 3] Epoch 80/100 | Acc: 0.9876 | Loss: 0.0466\n",
            "    [Fold 3] Epoch 90/100 | Acc: 0.9901 | Loss: 0.0694\n",
            "    [Fold 3] Epoch 100/100 | Acc: 0.9889 | Loss: 0.0377\n",
            "    [Fold 4] Epoch 10/100 | Acc: 0.9889 | Loss: 0.0323\n",
            "    [Fold 4] Epoch 20/100 | Acc: 0.9938 | Loss: 0.0310\n",
            "    [Fold 4] Epoch 30/100 | Acc: 0.9901 | Loss: 0.0273\n",
            "    [Fold 4] Epoch 40/100 | Acc: 0.9938 | Loss: 0.0155\n",
            "    [Fold 4] Epoch 50/100 | Acc: 0.9802 | Loss: 0.0728\n",
            "    [Fold 4] Epoch 60/100 | Acc: 0.9938 | Loss: 0.0175\n",
            "    [Fold 4] Epoch 70/100 | Acc: 0.9864 | Loss: 0.0492\n",
            "    [Fold 4] Epoch 80/100 | Acc: 0.9926 | Loss: 0.0289\n",
            "    [Fold 4] Epoch 90/100 | Acc: 0.9839 | Loss: 0.0881\n",
            "    [Fold 4] Epoch 100/100 | Acc: 0.9901 | Loss: 0.0323\n",
            "    [Fold 5] Epoch 10/100 | Acc: 0.9716 | Loss: 0.0876\n",
            "    [Fold 5] Epoch 20/100 | Acc: 0.9876 | Loss: 0.0501\n",
            "    [Fold 5] Epoch 30/100 | Acc: 0.9913 | Loss: 0.0720\n",
            "    [Fold 5] Epoch 40/100 | Acc: 0.9913 | Loss: 0.0518\n",
            "    [Fold 5] Epoch 50/100 | Acc: 0.9629 | Loss: 0.1820\n",
            "    [Fold 5] Epoch 60/100 | Acc: 0.9926 | Loss: 0.0556\n",
            "    [Fold 5] Epoch 70/100 | Acc: 0.9926 | Loss: 0.0772\n",
            "    [Fold 5] Epoch 80/100 | Acc: 0.9926 | Loss: 0.0618\n",
            "    [Fold 5] Epoch 90/100 | Acc: 0.9901 | Loss: 0.0605\n",
            "    [Fold 5] Epoch 100/100 | Acc: 0.9864 | Loss: 0.1027\n",
            "\n",
            "üìä Áµ±Ë®àÁµêÊûú (BS=4, LR=0.01, SEQ=10):\n",
            "  üîπ Final Accuracy : 0.9864\n",
            "  üîπ F1-score       : 0.9809\n",
            "  ‚è±Ô∏è  Training Time  : 1193.16 Áßí\n",
            "\n",
            "üß™ BS=4 | LR=0.01 | SEQ=20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/preprocessed/X_seq20_3d.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1051554269>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_search_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1284608246>\u001b[0m in \u001b[0;36mtrain_and_search_lstm\u001b[0;34m(batch_sizes, learning_rates, seq_lens, num_epochs, hidden_dim, num_layers, num_classes, k_folds)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mx_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREPROCESSED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"X_seq{seq_len}_3d.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0my_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREPROCESSED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"y_seq{seq_len}_3d.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/preprocessed/X_seq20_3d.npy'"
          ]
        }
      ],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_lstm(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")#\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q3inndRoyYY"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMxndCGUhHqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/MLP\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === MLP Ê®°Âûã ===\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_mlp(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=128,\n",
        "                         num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "                input_dim = X.shape[1]\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = MLPClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, num_epochs, bs, lr, seq_len):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # Á¥îÁ∑öÊ¢ù\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", num_epochs, bs, lr, seq_len)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", num_epochs, bs, lr, seq_len)\n",
        "\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"mlp_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA1ydN20jMiw"
      },
      "outputs": [],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_mlp(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmAFC98o2iM"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxnqos1Eo4UC"
      },
      "outputs": [],
      "source": [
        "# === SVM Ë®ìÁ∑¥Á®ãÂºèÔºàÊîπÂØ´ÁÇ∫Ëàá LSTM Êû∂Êßã‰∏ÄËá¥Ôºâ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/SVM\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === ÊîπÂØ´ÂæåÁöÑ SVM Ë®ìÁ∑¥ÊµÅÁ®ã ===\n",
        "def train_svm_search(kernels, Cs, seq_lens, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "    results = []\n",
        "    best = None\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for C_val in Cs:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüîé Kernel={kernel} | C={C_val} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_2d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_2d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_list, f1_list, all_y_true, all_y_pred = [], [], [], []\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "                    clf = SVC(kernel=kernel, C=C_val)\n",
        "                    clf.fit(X_train, y_train)\n",
        "                    y_pred = clf.predict(X_val)\n",
        "                    acc = accuracy_score(y_val, y_pred)\n",
        "                    f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "                    acc_list.append(acc)\n",
        "                    f1_list.append(f1)\n",
        "                    all_y_true.extend(y_val)\n",
        "                    all_y_pred.extend(y_pred)\n",
        "                t1 = time.time()\n",
        "\n",
        "                final_acc = np.mean(acc_list)\n",
        "                final_f1 = np.mean(f1_list)\n",
        "                if final_acc >= 1.0:\n",
        "                    print(f\"‚ö†Ô∏è Skipped Kernel={kernel} C={C_val} SEQ={seq_len} due to acc=1.0\")\n",
        "                    continue\n",
        "\n",
        "                precision = precision_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(all_y_true, all_y_pred, average='macro', zero_division=0)\n",
        "\n",
        "                # Êñ∞Â¢û‰∏âÁ®ÆÂúñ\n",
        "                # 1. K-fold validation accuracy Êõ≤Á∑ö\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds+1), acc_list, label='Validation Accuracy')\n",
        "                plt.title(f\"K-fold Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"accuracy_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "                # 2. K-fold F1-score Êõ≤Á∑ö\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, k_folds+1), f1_list, label='Validation F1-score', color='orange')\n",
        "                plt.title(f\"K-fold F1-score\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.xlabel(\"Fold\")\n",
        "                plt.ylabel(\"F1-score\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"f1_score_curves\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "                # 3. Ë®ìÁ∑¥ÈõÜÊ∫ñÁ¢∫ÁéáÔºàÊï¥È´îÔºâ\n",
        "                clf_full = SVC(kernel=kernel, C=C_val)\n",
        "                clf_full.fit(X, y)\n",
        "                train_pred = clf_full.predict(X)\n",
        "                train_acc = accuracy_score(y, train_pred)\n",
        "                plt.figure()\n",
        "                plt.bar(['train'], [train_acc])\n",
        "                plt.title(f\"Train Accuracy\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.ylim(0, 1.01)\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"train_accuracy\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nKernel={kernel} C={C_val} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"k{kernel}_c{C_val}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                result = {\n",
        "                    'kernel': kernel, 'C': C_val, 'seq_len': seq_len,\n",
        "                    'final_acc': final_acc, 'precision': precision,\n",
        "                    'recall': recall, 'f1_score': final_f1,\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(result)\n",
        "                if best is None or result['final_acc'] > best['final_acc']:\n",
        "                    best = result\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (Kernel={kernel}, C={C_val}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_acc:.4f}\")\n",
        "                print(f\"  üîπ Precision      : {precision:.4f}\")\n",
        "                print(f\"  üîπ Recall         : {recall:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_f1:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {round(t1 - t0, 2)} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"svm_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ Results saved to {csv_path}\")\n",
        "    return results, best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB0a2PFxo8TZ"
      },
      "outputs": [],
      "source": [
        "seq_lens = [4, 5, 8, 10, 20, 30, 40]\n",
        "C_list = [1.0, 10.0]\n",
        "kernel_list = [\"rbf\", \"linear\"]\n",
        "\n",
        "results, best = train_svm_search(kernel_list, C_list, seq_lens)\n",
        "\n",
        "print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "print(f\"Sequence Length = {best['seq_len']}\")\n",
        "print(f\"C = {best['C']}\")\n",
        "print(f\"Kernel = {best['kernel']}\")\n",
        "print(f\"Accuracy = {best['final_acc']:.4f}\")\n",
        "print(f\"Precision = {best['precision']:.4f}\")\n",
        "print(f\"Recall = {best['recall']:.4f}\")\n",
        "print(f\"F1-score = {best['f1_score']:.4f}\")\n",
        "print(f\"Training Time = {best['training_time_s']} Áßí\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxVLlUCDrD_2"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVsy2pBvrFfh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/GRU\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === GRU Ê®°Âûã ===\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "# === Ë®ìÁ∑¥ËàáÂÑ≤Â≠ò ===\n",
        "def train_and_search_gru(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, hidden_dim=64, num_layers=1, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"./preprocessed/\"\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                X = np.load(os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\"))\n",
        "                y = np.load(os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\"))\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = GRUClassifier(input_dim=3, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        correct_train, total_train, total_loss_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train_list.append(correct_train / total_train)\n",
        "                        loss_train_list.append(total_loss_train / total_train)\n",
        "\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc_list.append(correct / total)\n",
        "                        loss_list.append(total_loss / total)\n",
        "                        f1_list.append(f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0))\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc_list[-1]:.4f} | Loss: {loss_list[-1]:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # Á¥îÁ∑öÊ¢ùÔºåÁÑ° marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"gru_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVWAGumMrPmo"
      },
      "outputs": [],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_gru(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKn_cKbgu0O"
      },
      "source": [
        "1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWrgapHBgxcw"
      },
      "outputs": [],
      "source": [
        "# === CNN Ê®°ÂûãÂÆåÊï¥Ë®ìÁ∑¥Á®ãÂºè ===\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/CNN\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# === CNN Ê®°Âûã ===\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === ‰∏ªË®ìÁ∑¥ËàáË∂ÖÂèÉÊï∏ÊêúÂ∞ãÂáΩÊï∏Ôºà‰ΩøÁî®Âø´ÂèñÔºâ ===\n",
        "def train_and_search_cnn(batch_sizes, learning_rates, seq_lens,\n",
        "                         num_epochs=100, num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = CNNClassifier(num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # ÁÑ° marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULT_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    csv_path = os.path.join(RESULT_DIR, \"cnn_experiment_results.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        best = df.loc[df['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2P8TmkIijrl"
      },
      "outputs": [],
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_cnn(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TimesNet"
      ],
      "metadata": {
        "id": "MIiGTNGfpQn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Ë≥áÊñôÂ§æËàáÂÑ≤Â≠òË∑ØÂæëË®≠ÂÆö ===\n",
        "RESULT_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "SUB_DIRS = [\n",
        "    \"accuracy_curves\",\n",
        "    \"loss_curves\",\n",
        "    \"f1_score_curves\",\n",
        "    \"confusion_matrices\",\n",
        "    \"train_accuracy\",\n",
        "    \"train_loss\",\n",
        "]\n",
        "\n",
        "for sub in SUB_DIRS:\n",
        "    os.makedirs(os.path.join(RESULT_DIR, sub), exist_ok=True)\n",
        "\n",
        "# === Ëá™Ë®Ç Dataset ===\n",
        "class ChargeSequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.X = torch.tensor(sequences, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "# === TimesNet Ê®°Âûã ===\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        # ÊÆòÂ∑Æ skip connection ËôïÁêÜ\n",
        "        if input_dim != hidden_dim:\n",
        "            self.skip_conv = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=1)\n",
        "        else:\n",
        "            self.skip_conv = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x.transpose(1, 2)  # (batch, input_dim, seq_len)\n",
        "\n",
        "        x = residual\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.skip_conv is not None:\n",
        "            residual = self.skip_conv(residual)\n",
        "\n",
        "        x = x + residual  # ÊÆòÂ∑ÆÂä†Ê≥ï\n",
        "        x = x.transpose(1, 2)  # ÂõûÂà∞ (batch, seq_len, hidden_dim)\n",
        "        return x\n",
        "\n",
        "class TimesNetClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([TimesBlock(input_dim if i==0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)  # ‰∏çË¶ÅÂÜçÂä†ÊÆòÂ∑Æ‰∫ÜÔºåblock ÂÖßÈÉ®Â∑≤ËôïÁêÜÊÆòÂ∑Æ\n",
        "        x = x.transpose(1, 2)  # (batch, hidden_dim, seq_len)\n",
        "        x = self.pool(x).squeeze(-1)  # (batch, hidden_dim)\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "# === ‰∏ªË®ìÁ∑¥ËàáË∂ÖÂèÉÊï∏ÊêúÂ∞ãÂáΩÊï∏ ===\n",
        "def train_and_search_timesnet(batch_sizes, learning_rates, seq_lens,\n",
        "                              num_epochs=100, hidden_dim=64, num_layers=2,\n",
        "                              num_classes=4, k_folds=5):\n",
        "\n",
        "    PREPROCESSED_DIR = \"/content/preprocessed/\"\n",
        "    RESULTS_DIR = \"/content/drive/MyDrive/Colab Notebooks/test/feature dim_3/TimesNet\"\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    SUB_DIRS = [\n",
        "        \"accuracy_curves\", \"loss_curves\", \"f1_score_curves\",\n",
        "        \"confusion_matrices\", \"train_accuracy\", \"train_loss\"\n",
        "    ]\n",
        "    for sub in SUB_DIRS:\n",
        "        os.makedirs(os.path.join(RESULTS_DIR, sub), exist_ok=True)\n",
        "\n",
        "    results = []\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            for seq_len in seq_lens:\n",
        "                print(f\"\\nüß™ BS={bs} | LR={lr} | SEQ={seq_len}\")\n",
        "                x_path = os.path.join(PREPROCESSED_DIR, f\"X_seq{seq_len}_3d.npy\")\n",
        "                y_path = os.path.join(PREPROCESSED_DIR, f\"y_seq{seq_len}_3d.npy\")\n",
        "                X = np.load(x_path)\n",
        "                y = np.load(y_path)\n",
        "\n",
        "                skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "                acc_curves, loss_curves, f1_curves = [], [], []\n",
        "                acc_train_curves, loss_train_curves = [], []\n",
        "                all_y_true, all_y_pred = [], []\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "                    X_train, X_val = X[train_idx], X[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                    train_loader = DataLoader(ChargeSequenceDataset(X_train, y_train), batch_size=bs, shuffle=True)\n",
        "                    val_loader = DataLoader(ChargeSequenceDataset(X_val, y_val), batch_size=bs)\n",
        "\n",
        "                    model = TimesNetClassifier(input_dim=3, hidden_dim=hidden_dim,\n",
        "                                               num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                    acc_list, loss_list, f1_list = [], [], []\n",
        "                    acc_train_list, loss_train_list = [], []\n",
        "\n",
        "                    for epoch in range(num_epochs):\n",
        "                        # === Training ===\n",
        "                        model.train()\n",
        "                        total_loss_train, correct_train, total_train = 0, 0, 0\n",
        "                        for xb, yb in train_loader:\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            optimizer.zero_grad()\n",
        "                            out = model(xb)\n",
        "                            loss = criterion(out, yb)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            _, pred = torch.max(out, 1)\n",
        "                            correct_train += (pred == yb).sum().item()\n",
        "                            total_train += yb.size(0)\n",
        "                            total_loss_train += loss.item() * xb.size(0)\n",
        "\n",
        "                        acc_train = correct_train / total_train\n",
        "                        loss_train = total_loss_train / total_train\n",
        "\n",
        "                        acc_train_list.append(acc_train)\n",
        "                        loss_train_list.append(loss_train)\n",
        "\n",
        "                        # === Validation ===\n",
        "                        model.eval()\n",
        "                        correct, total, total_loss = 0, 0, 0\n",
        "                        y_pred_epoch, y_true_epoch = [], []\n",
        "                        with torch.no_grad():\n",
        "                            for xb, yb in val_loader:\n",
        "                                xb, yb = xb.to(device), yb.to(device)\n",
        "                                out = model(xb)\n",
        "                                loss = criterion(out, yb)\n",
        "                                _, pred = torch.max(out, 1)\n",
        "                                correct += (pred == yb).sum().item()\n",
        "                                total += yb.size(0)\n",
        "                                total_loss += loss.item() * xb.size(0)\n",
        "                                y_pred_epoch.extend(pred.cpu().numpy())\n",
        "                                y_true_epoch.extend(yb.cpu().numpy())\n",
        "\n",
        "                        acc = correct / total\n",
        "                        avg_loss = total_loss / total\n",
        "                        f1 = f1_score(y_true_epoch, y_pred_epoch, average='macro', zero_division=0)\n",
        "\n",
        "                        acc_list.append(acc)\n",
        "                        loss_list.append(avg_loss)\n",
        "                        f1_list.append(f1)\n",
        "\n",
        "                        if (epoch + 1) % 10 == 0:\n",
        "                            print(f\"    [Fold {fold+1}] Epoch {epoch+1}/{num_epochs} | Acc: {acc:.4f} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                    acc_curves.append(acc_list)\n",
        "                    loss_curves.append(loss_list)\n",
        "                    f1_curves.append(f1_list)\n",
        "                    acc_train_curves.append(acc_train_list)\n",
        "                    loss_train_curves.append(loss_train_list)\n",
        "                    all_y_true.extend(y_true_epoch)\n",
        "                    all_y_pred.extend(y_pred_epoch)\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "                def plot_metric_per_fold(fold_lists, metric_name, folder, bs, lr, seq_len, num_epochs):\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    for fold_idx, fold_metric in enumerate(fold_lists):\n",
        "                        plt.plot(range(1, num_epochs + 1), fold_metric, label=f\"Fold {fold_idx + 1}\")  # ÁÑ° marker\n",
        "\n",
        "                    plt.title(f\"Combined {metric_name.capitalize()} (BS={bs}, LR={lr}, SEQ={seq_len})\")\n",
        "                    plt.xlabel(\"Epoch\")\n",
        "                    plt.ylabel(metric_name.capitalize())\n",
        "                    if metric_name != 'loss':\n",
        "                        plt.ylim(0, 1.0)\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    path = os.path.join(RESULT_DIR, folder, f\"bs{bs}_lr{lr}_seq{seq_len}_combined_{metric_name}.png\")\n",
        "                    plt.savefig(path, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                # === Plot ===\n",
        "                plot_metric_per_fold(acc_curves, \"accuracy\", \"accuracy_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_curves, \"loss\", \"loss_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(f1_curves, \"f1_score\", \"f1_score_curves\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(acc_train_curves, \"accuracy\", \"train_accuracy\", bs, lr, seq_len, num_epochs)\n",
        "                plot_metric_per_fold(loss_train_curves, \"loss\", \"train_loss\", bs, lr, seq_len, num_epochs)\n",
        "\n",
        "                # Ê∑∑Ê∑ÜÁü©Èô£\n",
        "                cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "                disp.plot(cmap='Blues', values_format='d')\n",
        "                plt.title(f\"Confusion Matrix\\nBS={bs} LR={lr} SEQ={seq_len}\")\n",
        "                plt.savefig(os.path.join(RESULTS_DIR, \"confusion_matrices\", f\"bs{bs}_lr{lr}_seq{seq_len}.png\"), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                final_result = {\n",
        "                    'batch_size': bs, 'learning_rate': lr, 'seq_len': seq_len,\n",
        "                    'final_acc': acc_list[-1], 'final_loss': loss_list[-1],\n",
        "                    'precision': precision_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'recall': recall_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'f1_score': f1_score(all_y_true, all_y_pred, average='macro', zero_division=0),\n",
        "                    'training_time_s': round(t1 - t0, 2)\n",
        "                }\n",
        "                results.append(final_result)\n",
        "\n",
        "                print(f\"\\nüìä Áµ±Ë®àÁµêÊûú (BS={bs}, LR={lr}, SEQ={seq_len}):\")\n",
        "                print(f\"  üîπ Final Accuracy : {final_result['final_acc']:.4f}\")\n",
        "                print(f\"  üîπ F1-score       : {final_result['f1_score']:.4f}\")\n",
        "                print(f\"  ‚è±Ô∏è  Training Time  : {final_result['training_time_s']} Áßí\")\n",
        "\n",
        "    csv_path = os.path.join(RESULTS_DIR, \"timesnet_experiment_results.csv\")\n",
        "\n",
        "    # Â¶ÇÊûúÂéüÊú¨ÊúâÊ™îÊ°àÔºåÂÖàËÆÄÈÄ≤‰æÜËàäÁöÑ\n",
        "    if os.path.exists(csv_path):\n",
        "        df_old = pd.read_csv(csv_path)\n",
        "        df_new = pd.DataFrame(results)\n",
        "        df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
        "    else:\n",
        "        df_combined = pd.DataFrame(results)\n",
        "\n",
        "    # ÂÑ≤Â≠òÂêà‰ΩµÂæåÁöÑÊñ∞ dataframe\n",
        "    df_combined.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ All experiment results saved to {csv_path}\")\n",
        "\n",
        "    if not df_combined.empty:\n",
        "        best = df_combined.loc[df_combined['final_acc'].idxmax()]\n",
        "        print(f\"\\nüèÜ Best: BS={best['batch_size']} | LR={best['learning_rate']} | SEQ={best['seq_len']} | ACC={best['final_acc']:.4f}\")\n",
        "        return results, best\n",
        "    else:\n",
        "        print(\"\\n‚ùó No valid results\")\n",
        "        return [], None"
      ],
      "metadata": {
        "id": "QXdg-w6HpQTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ë®≠ÂÆöË®ìÁ∑¥ÂèÉÊï∏\n",
        "batch_sizes = [4, 8, 16, 32]\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]\n",
        "seq_lens = [4, 8, 10, 20, 30, 40]\n",
        "num_epochs = 100\n",
        "\n",
        "# È°ØÁ§∫Á∏ΩÁµÑÊï∏\n",
        "total_combinations = len(batch_sizes) * len(learning_rates) * len(seq_lens)\n",
        "print(f\"üîç Á∏ΩÂÖ±Ë®ìÁ∑¥ÁµÑÊï∏Ôºö{total_combinations}ÔºåÊØèÁµÑË®ìÁ∑¥ {num_epochs} epochs\")\n",
        "\n",
        "# Âü∑Ë°åÁ∂≤Ê†ºÊêúÂ∞ãË®ìÁ∑¥\n",
        "results, best = train_and_search_timesnet(batch_sizes, learning_rates, seq_lens, num_epochs=num_epochs)\n",
        "\n",
        "# Ëº∏Âá∫ÊúÄ‰Ω≥ÂèÉÊï∏ËàáÊåáÊ®ô\n",
        "if best is not None:\n",
        "    print(\"\\nüéØ ÊúÄ‰Ω≥ÂèÉÊï∏ÁµÑÂêàÔºö\")\n",
        "    print(f\"Batch Size     = {best['batch_size']}\")\n",
        "    print(f\"Learning Rate  = {best['learning_rate']}\")\n",
        "    print(f\"Sequence Length= {best['seq_len']}\")\n",
        "    print(f\"Final Accuracy = {best['final_acc']:.4f}\")\n",
        "    print(f\"Final Loss     = {best['final_loss']:.4f}\")\n",
        "    print(f\"Precision      = {best['precision']:.4f}\")\n",
        "    print(f\"Recall         = {best['recall']:.4f}\")\n",
        "    print(f\"F1-score       = {best['f1_score']:.4f}\")\n",
        "    print(f\"Training Time  = {best['training_time_s']} Áßí\")\n",
        "else:\n",
        "    print(\"‚ùó Ê≤íÊúâÊúâÊïàÁµêÊûúÔºàÊ∫ñÁ¢∫ÁéáÂÖ®ÈÉ®ÁÇ∫ 1.0Ôºâ\")"
      ],
      "metadata": {
        "id": "KO5scvlppmiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyLkbkge/ZV+tMduxtSYD8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}